 In this video, I'm going to use generalized igan decomposition, which is often abbreviated Ghedi for source separation in simulated data. This is going to be the same simulated data set that we've been working with for the past several videos in this section of the course. And in particular, what we are going to do is create two covariance matrices based on different time windows and then see whether we can separate the two sources, which in this case means the projections from the two different dipoles by using generalized organic composition on those two covariance matrices. All right. So first, I'm going to generate the data. And if you have no idea what this script is doing, what kind of data this is producing, then I encourage you to go back and watch the one of the first videos in this course sorry, in this section in the course called simulating multi component EEG data. This is a reminder that it's always a good idea to inspect a new data set, simulated or empirical. However, we've been working with these simulated data for several videos now, so I'm not going to spend any time on this. All right. So here I'm going to create the two covariance matrices. I often call them S and R for signal and reference. And the way that I'm going to create these is by time intervals. So it's going to be minus one second to zero is for the R covariance matrix and then zero to one point five seconds is going to be the time window used for the s covariance matrix here. I initialize those two matrices. You can see they're going to be channel by channel, loop over trials and then I extract some data. So we isolate the data. We mean send to the data and then multiply the channel by time data, set our matrix by its transpose. So here you can see for the R matrix, I'm isolating the data from see from all channels from this time window so tied one to two idex two, which means minus one second to zero. And from this trial and then here is almost exactly the same code for the S matrix. But instead of time index going one to two, it's two to three. So that's going to be zero to one point five. And just as a reminder, if you are doing this copy paste, then you just need to be really careful that you are updating any variables that you need to know. In this case, I can see I actually didn't update this. So this still says our matrix when in fact that computing the matrix. But of course, this is just a comment. So that has no real negative impacts for the actual code itself. OK, here we go. So here we're selling the mattresses on top of themselves with each new covariance matrix, get some onto the S or R matrix. So then here I'm dividing by the number of trials in order to get the average covariance matrix. Let's see. And here I plot the two covariance matrices. So let's have a look and see what they look like. Actually, before discussing the covariance matrices, I want to just quickly point out how I'm setting up the color limit here. So I create this two element variable called caelum, and that's dynamically set, according to Sanat. And I'm using the same ceiling, the same color limit, same color access limit for both of these matrices. So the way that I do this is by creating this two element vector minus one one. So that just makes sure that it's going to be a symmetric color scaling. And then I multiply this by a single number. So all this works out to be a single number. And essentially what I'm doing is stringing out the S and R matrices. So first of all, I vectorized them. They so they become or they get transformed from a matrix into a column vector. So I'll show you what that looks like. So all sixty four by sixty four elements are just strung out and then here I'm concatenating these two. So this is going to give me one really, really long vector. So let's see how big that vector is going to be. So a little over 8000. That's 64 times 64 and then times two, and then I take the absolute value of that because I'm just interested in the extent of the distance away from zero, and then I find the largest value there, and then I scale it down by point seven. So at this point, seven is just a scaling factor. You can see what happens. If I don't include it, then the color is a little bit lighter, which is fine. So then there's going to be one pixel here that gets saturated. And you can see, for example, if you make it zero point one seven now, it's too much saturation. You don't really see a lot of the more subtle features. So anyway, that being explained, it's interesting to see these two covariance matrices. Now, when you look at these side by side, it looks like they're overall the same. They look really, really similar to each other. This for the Excavates matrix. This is a little bit darker in here. So stronger covariance, stronger linear interactions amongst these sets of channels relative to here. But overall, the qualitative pattern looks really, really similar. So what the generalized Argante composition is going to do is find vectors, find features that actually differentiate these two matrices, even though they are really similar. There are going to be some subtle features that differentiate them. So here is how that generalized organic composition is implemented. The function is Eigg. Any input? First the covariance matrix, then the AKA variance matrix. You'll recall from the video on PCA that the PCI actually looked like this. So it was just one matrix that you input for PCA. And here we're inputting two matrices. First, the signal and then the reference. Then I'm sorting the eigenvalues descending so that the largest eigenvalue, so the largest component, the most important components, the most discriminative components are in the first positions. Let's see. So let me already show you what this looks like. So here you see that. Sixty two components are basically flat. They're really close to one, which is in fact, the null hypothesis value in this case. And then you see two big components that are really strongly popping out of I can call this the noise spectrum. So if you call this the noise spectrum, then there's two components that are very clearly popping out of the noise spectrum. And that's, first of all, interesting to compare against the principal components analysis. And in that video on PC of simulated data, because the PC revealed quite a few components and it didn't seem to nap on really closely to how the data were simulated, which is to dipoles. And then everything else is noirish to say to dipoles of signal. And all the other dipoles just contained noise here. This looks like we're having a pretty good match onto the original sources. So then what I do is compute the component time series, and that's basically the eigenvectors, so the first two eigenvectors that corresponds to these two components and you multiply them by the EEG data. So that's all the channel data all the time points and all the trials. And you can do this over a loop and compute the separately for each trial. But I'm actually just reshaping this three dimensional data into two dimensions. So I'm concatenating all the trials on top of each other to create a two dimensional signal. So that's our two dimensional matrix. So it ends up being too well, actually, let me show you what this size is. So this is going to be 64 by 300000. So this is channels by and then it's time. And trials are both in this dimension. And then you multiply that by the two eigenvectors corresponding to these two components or these two eigenvalues, and that gives you a two by 300000 matrix, which is two components, and then three hundred thousand time points. And then you want to reshape that back to time by trial. So that gives you this three dimensional representation. And then this is pretty interesting. I put it at the end of the data and I've also shown this before, but I think this is a pretty convenient way to organize the data because then you get the sixty four channels and these other two source time courses, which you can kind of think of as like virtual channels. All right, let's see. And then we do some plotting and I think I'll just run this whole cell. So let's see what we got here, so it must start figure three. So here you see the ground truth, this is the projection of the dipole onto the scalp, and here you see what the component looks like and the component is generated by multiplying the eigenvector by the covariance matrix. So that's pretty parallel with the Time series. So to get the time series of the component, you multiply the eigenvector by the channel data and to get the topography of the component, you multiply the the same eigenvector by the covariance matrix. So this is the, um, the forward model of the spatial filter and this is what the ground truth data look like. And these are overall quite similar. And in fact, I think I will add num contour zero to these that will make them just look even more similar to each other. Let's see. So there you go. Here's the estimate that we got from the data and here's the ground truth. They map onto each other quite well and let's see what this one looks like. So I will also redraw this topography. This also looks really, really similar. And again, you can contrast this with the results from the principal components analysis in that previous video on PC of simulated data. And it's also interesting to look at the time frequency power plots for these two components. So if you go back and forth between these, you can see that component one is a little bit earlier and a little bit lower frequency and component two is a little bit later and a little bit higher frequency. So that maps onto the simulation quite well. So in conclusion, by generating a covariance matrix, let's see, what was the time window game? So by generating two covariance matrices, one for a pre stimulus time period and here and one for a post stimulus time period here. So zero to one point five seconds, we were able to quite successfully and pretty accurately discriminate these two, I should say, separate these two different sources, the two different dipole activities. Even though we didn't provide any spectral information, we didn't specify that there needs to be two components. Exactly. In fact, we asked to return sixty four components and it was the top two that gave us back these two simulated sources.