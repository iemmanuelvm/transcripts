 The goal of this video is to perform a principal components analysis of an abbreviated PC on the simulated data that we've been creating in the past, several videos in this section of the course. So let's start again by running that code, and that will just generate the data. And here I say it's always a good idea to inspect the data. Now, in this case, if you're following along from the past several videos, for example, on simulating multi component EEG data and on creating covariance matrices based on time features and frequency features, then you're already pretty familiar with these data. So you don't need to look at them in. But, you know, just a good reminder, it's always important to inspect your data. OK, so how do you perform a principal components analysis? Uh, PC is essentially just an eye candy composition of a covariance matrix, a channel covariance matrix. And then the rest of the PCA is basically just working with the eigenvectors and the eigenvalues resulting from the Igan the composition of that covariance matrix. If you're interested in learning more about the hows and whys and the mechanisms and the math of PCA, you can consider taking my course either on linear algebra or on dimension reduction and source separation. For now, suffice it to say that a PCA is a covariance matrix and then again, the composition. All right. So I am computing the covariance matrix based on the time window from zero to it's really should be 800 milliseconds. So it actually should be point eight like this, not 800. OK, so I run this cell and it goes quite quickly. You can see here isolating the data. So from this particular trial in this particular time window and then I'm means entering the data and then I add to the data, the data set times, it's transpose and this is really the covariance matrix. So the convergence matrix of this trial and then this is just the normalization factor. This is just the normalization factor. All right. Then we compute the Igen decomposition that returns the eigenvectors and the eigenvalues. Now, here I'm the Eigenvectors and the eigenvalues matrix. According to the eigenvalues themselves, in descending order here, I'm converting to percent change. This is optional. This is just another way to help you interpret the igan spectrum. So here you see the eigenvectors. The eigenvectors are all in the columns. So you always want to look down the columns, not the rows, the columns containing eigenvectors. And the idea is that each eigenvector has a corresponding eigenvalue. Now, of course, these are all going down because I've sort of them descending. So this is a way of looking at the eigenvalues values as the diagonal of a full square matrix. And if you would just plot these values, just the diagonal values, and that would look like this. So here is the raw eigenvalue. This is in the scale of the data, the scale, the covariance of the data. And here is the eigenvalues converted into percent change. So generally, the way to interpret this, these are often called scre plots. I mean, write that down so it's clear subtitle scre plot. So here's how it's spelled. Generally, the way to interpret these plots is that you want to look for the number of components that are popping out of the noise spectrum. So the idea is that this is some noise here and then you have some number of components that are kind of clearly differentiable from the noise spectrum. Now, sometimes this is done visually, qualitatively, like what I'm doing here and sometimes this is done statistically. And I discuss how to do that more in my course on dimensioned reduction. OK, but here it looks like there's definitely one huge component that's clearly different from the others and maybe there's a second one in here. And in fact, I think I'll even look at these numerically so we can see what they look like. So let's see. So the top component, the largest principal component accounts for 66, almost 66 percent of the variance of the entire segment. And then the second component, which you see down here is twelve point seven. So that's a bit under 13 percent of the total variance explained. Now, maybe this corresponds to the first two components that were the first to dipoles that we simulated in this data set. So that's kind of what I want to do with the rest of the script, do some analyses on the first principle component and see if that maps onto one of the components, one of the dipoles that we simulated in the simulated data. OK, so what I'm going to do now is compute the component time series, which in these statistics literature is also sometimes, sometimes called the score is the principal component scores. So the way that you do that is by multiplying the eigenvectors by the data and you're multiplying the first two and the data here for simplicity and reshaping the data to go from. Actually what I want to do is start like this. So you reshape the data from three dimensions to two dimensions. I'm concatenating all the trials. It's going to give us a channels by time trials matrix. So sixty four by three hundred thousand points. And then here I multiply the top to eigenvectors by the those data and then here I'm reshaping that back into a two by time by trials matrix. And of course it's two because we have two components that we are extracting. So now what I do here is add two new extra rows, two extra channels to the data set. And that's going to be the movie channel, which is the number of channels 64 plus one to sixty four plus two. So I'll show you what this looks like now when I type. We can look at the data and see. So there are sixty four channels of course, but all the data set actually has sixty six channels, so one through sixty four are the SPG channels and sixty five and sixty six are the two principal component time series. So I like doing it like this because it's pretty convenient. You have all the data stored directly in this data matrix. OK, and here I'm just showing you that all three of these lines can be implemented in one line of code. It's a really long line of code. It's confusing because there's a lot of stuff happening there. There's to reshape commands. So one reshape function is embedded inside a second reshape function. Now, there's nothing wrong with this line of code, but on the other hand, scientific programming in MATLAB is not a competition to see how few lines of code you can possibly use. So if three lines of code is more clear and easier to read than one line of code, then of course that's what you should do. Right? OK, so now we're going to plot the two components. I'm using this function plot, Meg. Notice I'm inputting the data structure here and now I'm plotting channel sixty five. And then here is basically the same channel 66 troops are right. OK, so here was the reason why I deleted this thing over here. So now I run this code again and I'm saying reshape the data to channels by empty. So by whatever, which ends up being time times trials. But now the data are sixty four channels. So that actually doesn't fit like this. It certainly doesn't fit because there's now more channels than what we had initially. So that's why you would need to have some code like this to specify that it should be one to the number of skalp channels and then it's going to work. Right. So here you see the results. So this is principle component one and this is the ground truth for one of the two dipoles. Now, this one seems to match kind of it looks like it's about in the right spot, but this is much smaller. So this is more spatially constrained. And the principal component ended up being huge. It ended up being really smooth. And things are not looking much better when you get to component two because the principal component looks like some frontal lateral distribution, whereas the ground truth is much tighter. So I guess if you would flip the sign, it would kind of look a little bit like this, at least in the sense that this is centered around where this is blue. So, of course, the sign here is arbitrary. This line flipping is arbitrary. But then, of course, there's this strong feature here which doesn't really seem to be reflected over here. In fact, if anything, this feature here, these lines here are about orthogonal to these lines here. So overall, I would say this is quite disappointing. PCA does not seem to be able to accurately reconstruct or I should say separate the two sources that we generated in the Dipole Time series data. Last thing I want to illustrate in this video is using the Matlab function PCA. So to use the Matlab function PCA, you would input the data. You don't have to compute the covariance matrix PCA. This PCA function will compute the covariance matrix for you. You just input the data and it's going to give a bunch of outputs. Note that this PCA function is in the statistics toolbox, so if you don't have the stats toolbox installed, then this line will not work for you. But that's totally, totally fine. You don't actually need the PCA function and the reason is because of what I'm showing you here. So the outputs of the PCA function match what we computed up here in earlier in the script. So this is showing the eigenvalues of the screen plot. So the black line is the code that I wrote and the red line is the output of the function. I intentionally shifted them a little bit on the x axis here. You see that here I do one through NP plus point to five. So it's just shifted just for visual inspection so you can see that they're the same. Now, this is a pretty interesting example here. I'm going to run this again for reasons that I discuss in the linear algebra course and also in the dimension reduction course. The sign of the eigenvector is arbitrary. So these signs are not really interpretable. So I'm going to flip the sign here just to make it more a bit easier to interpret. And then you see that the eigenvectors are the same for what I did so manually, my home written code and the Matlab function PC. Now I had to normalize them here using the Z score function, and that's because the PCA function actually is applying a normalization to put the eigenvector into the same scale as the original data. But that's an arbitrary scaling that doesn't actually affect the component itself. And you can see those are the same between the two methods.