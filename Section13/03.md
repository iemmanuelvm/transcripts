 In this video, we're going to create covariance matrices based on different features of the data. So either the entire broadband signal or the broadband signal is selecting a different time windows or the entire signal, but selecting different frequencies. And I will also use this as an opportunity to give you a reminder of the two ways to create covariance matrices. One, either by writing out all the math yourself or by using the Matlab function code for covariance. All right. So we start by generating the data. Now, this looks like I'm calling a function here, but remember, this is just a script. This is the script that I showed you in the previous video, the video on simulating multi component data. So now I'm just running that script and it's going to run the entire function or sorry, the script, except for the very last couple of lines, which does the plotting. Now, you might actually remember that there's some plotting in the beginning, so we're still going to get one figure coming out of running the script. But that's OK. I think I'll just close it anyway. Now, here I say it's always a good idea to inspect the data. If you are watching this video immediately after that video on simulating the multi component e.g data, you probably don't need to spend too much time looking through this data set. But this is just a reminder. Whenever you're creating new data or loading a new data, it's always a good idea. Just to have a quick look at the basic characteristics of the data, how many channels, trials, time points, what sampling rate, blah, blah, blah, all this kind of stuff. All right. So let's see. So now I'm going to compute the average covariance from all of the individual trials separately. So I have a loop that goes over trials and then I compute the covariance matrix from that individual trial. And then I add that all the covariance or to this kind of summed covariance matrix so that in the end I'm going to get one covariance matrix that reflects the average of the single trial covariance matrices. So here I call this the manual way. This is basically writing out all the math to do it. So isolate center transpose. That's one way to remember how to compute a covariance matrix. So here I'm isolating the data. And in this particular case, it's all the channels, all the time points and only this one trial. So let's see, initialize this. Initialize this to trial one. So here I isolate just the data. This gives me a slice of a three D cube of data. Then you have to mean center and I mean centering over the second dimension because I want to remove the average of all the time points or of the voltage activity across all the time points. And then here I compute the covariance matrix. It's the matrix times, it's transpose. And whether you transpose the first matrix or the second matrix is really just dependent on the orientation of the matrix. But you want this thing to be channels by channels or in this case, sixty four by sixty four. So that means the second matrix needs to be transposed. If you really unclear about what any of this stuff means or exactly how to interpret a covariance matrix, these are topics I cover in much more depth in two of my other courses, linear algebra course and the course on dimension reduction and source separation anyway. So you can see over all the trials, I'm summing up all these covariance matrices, so I'm continually adding to this. So that means in the end I need to divide by NP to complete the averaging. So here's a bunch of sums and then divide by and that gives us the average. Now this line here replaces all of these three lines here. This is using the Matlab code function, which will also mean center as part of the function. Now with the code function, you have to be really careful about the orientation of inputting the data. So if you would run it like this, you see error using plus matrix dimensions must agree. And here the issue is that Matlab is actually computing, in this case the covariance matrix over time. So we get a time by time matrix. Now this is OK is still a valid covariance matrix. However, in this case, what we want is a channel by channel covariance matrix. And I run the cell again and we get no errors because this is the correct orientation. So that's for computing the covariance matrix of each trial separately and then averaging across all the individual trials. There's another way to do this, which is to compute one covariance from all the concatenated trials. So here what I'm doing is concatenating all the trials into one matrix. And that gives me instead of having a three D cube of data, I'm reshaping this to be. Channels by whatever, which ends up being timed by trials, so now this variable tempo that is sixty four channels by three hundred thousand time points, which means all the individual trials have been concatenated on top of each other. And now I can compute one covariance matrix across all of those trial and time points. And if you like, these two lines of code are the manual way of implementing what the code function is also implementing. So people often ask whether they should compute the covariance matrix this way or this way. And unfortunately, it's a little difficult to know. So you'll see in a minute that these work out to be basically identical here, these two, but they work out to be identical here because these are simulated data and the spatial, temporal, spectral features of the data are basically the same on every single trial, you know, with some a little bit of stochastic city in there. And also the center is fairly high in these data. In practice, in real data, you might find that you get slightly different results if you compute the covariance matrix this way versus this way. And there's no right or wrong way to do it. It's just different ways of averaging data together. I find in my personal experience, this method tends to be a little bit better. The results tend to be a little bit more sensible and I think a little bit more stable. So when I do real data analysis, I tend to use this method where I compute the covariance matrix of each trial separately, add them all together, divide by and if you want, it turns out for some kinds of components analysis. Depending on what you're doing at the covariance matrix, it might not matter if you divide by N, but anyway, that's a separate issue. OK, so now I'm going to compare the two of these covariance matrices and that's why I didn't run this code yet. OK, so now you see the covariance matrix from each trial separately and the covariance matrix from the concatenated trials. Now it looks like these are different from each other, but this is actually just a bit of a scaling thing here. You see all the individual elements of the two covariance matrices plotted against each other. So this is let's see, so the trial base versus the concatenated base and you can see that they are nearly perfectly correlated. In fact, they might even be perfectly correlated. Let me see. Kurkov Um, that T and Co. Matt C. And they're basically perfectly correlated with each other. So, again, in this case, with simulated data, you get exactly the same result in real data. You will find that these two approaches will be strongly related to each other. But depending on the kind of data and how much non stationary you have in the data, how the data change over different trials, these two methods might give slightly different results. All right. So now what I'm going to do is compute the covariance matrix again, but using different features of the data. So there's nothing terribly new that I'm doing here compared to what you already saw in this video. Just illustrating to you how to compute covariance matrices from different features of the data. So in this case, it's going to be based on a time window or based on two time windows. So I'm going to compute to covariance matrices one from the stimulus data and one from the post stimulus data. So these are the window times in milliseconds, and I believe this is not correct. Yeah. So this let me see this. Yeah. So this, uh, so that time vector here is actually given in seconds. So this technically we're telling Matlab to look for the time point closest to minus 800 seconds. So really this should be divided by a thousand. So then here I'm initializing the covariance matrices. I'm calling them covariance for the pre stimulus period and covariance for the post stimulus period. Again, loop over trials. But now here you can see what I'm doing is selecting the data from the first time window and from the second time window. And then let's make a plot of these two. So now it looks like these two are basically the same. You can see some differences between them here. You see most strong differences, but it turns out that these are automatically color scale. So if we were to set these to force these to be in the same color scale, which is mostly a good idea because that allows you to compare them visually, run this code again. Oops. And now we see that. Let's see if that looks better times 100. So now you see the differences between these two covariance matrices is a little bit more apparent when they're on exactly the same color scale. And it's also interesting to just eyeball these two covariance matrices and get a bit of a feel for where there are differences and where there aren't differences in the variances. So you look it looks like most of the differences are around channels 20 to 30, you know, so here this is brighter here. On the other hand, they look qualitatively really similar. So it's brighter here. The colors are more saturated here compared to here. So the covariance is the inter channel. Correlations are stronger here compared to here. However, it looks like it's more an issue of intensity rather than a qualitatively distinct pattern of covariance is before versus after time zero. And you can find other channel groups like around, you know, say 10 to 15 electrodes, 10 to 15, where it doesn't really first of all, there isn't very strong covariance at all relative to the rest of the map, but also these covariance. So this block of variances doesn't seem to be a whole lot different between these two time windows. OK, so that's all qualitative. We're going to do more quantitative comparisons between covariance matrices later on in this section in particular, basically any video that has the title with generalised organic composition. But still, I'm a big believer in the importance of visual qualitative inspection of data. All right. So this was selecting data to create covariance matrices based on time windows. Now, what I'm going to do is a similar thing, except it's going to be covariance is based on filtering, based on frequency content. So I'm going to create a narrowband filtered version of the signal. And I'm going to call that I'm going to put it into a different field inside this structure called F data for a filtered data. So you can see I'm calling this function filter FGF, which is a frequency domain Gaussian filter. So I input the data, the sampling rate, the center frequency and the width of the frequency. And you've seen this function before? Several times before. So here I'm filtering the data centered at nine hertz with a full with Raef maximum of five hertz. Now I loop over trials and compute covariance matrices separately for the filtered data and the non filter data or the broadband data. And now I'm using the Matlab code function. So you can see I'm extracting all the channels, only this one time window. So this was a time window. I think it's zero to fifteen hundred milliseconds and then only this trial and then this transpose here to make sure we're getting channel by channel covariance matrices. So you can see I'm, I'm switching around sometimes using matlab code function. Sometimes I write out all the code myself. The point is for you to gain some experience and have some flexibility so you can set up the analysis and set up your code however you like. OK, and then the plotting is basically the same as before. This will go into figure four. So here you see now this is pretty interesting. It looks like there's not only differences in scale, but also differences in qualitative patterns of variances. So let me first talk about the scale difference so you can see I have now two variables for the color limits for these data. So if I set Caelum F for the filter to be caelum B, then when I run this, here's what the two Corrine's matrices look like when they're at the same color scale. So this is also pretty interesting to to consider and to think about. Now, this actually isn't so surprising when we filter the data. We're just getting a very narrow section of the entire spectrum of the data. So we're only taking a little bit of energy from the entire signal from each channel. So therefore, it's really not surprising that the unskilled covariance matrices are the covariance matrices that's in the same scale as the original data. So micro volts in this case, this is much smaller because we are extracting a tiny sliver of the total variance of the signal. So this is interesting to look at, but to make more qualitative comparisons, it's also useful to have the color scales be different. And that you can see here, it's not comparing these two. It does look like there's also some qualitative differences in addition to scale differences, for example, here around channels like eleven to fifteen or so, this is weakly orange here, which means weak, positive covariance is here between channels. The. Say like 18 to 30 to and 12 to 15 in this general range, whereas here those same channel pairs have a negative correlation between them. And here you also see some qualitative differences here. So this is pretty interesting that when we filter the data, you see very different patterns emerging compared to the broadband signal. It's also interesting because everything that you see in this covariance matrix is also part of this covariance matrix because this is the broadband signal. And so, of course, the broadband signal also contains the energy at around nine hertz. But of course, the broadband signal has a lot more information packed in and it's a lot less specific. So this is very specific to one frequency and this is very non-specific to all frequencies. So in later videos in this course, you will see how we can use these differences in the covariance structures as well as these differences in these covariance structures in order to try to separate the two different sources that we simulated.