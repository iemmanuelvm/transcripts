The goal of this video is to see how the inverse Fourier transform gets implemented in code using the

painstakingly slow for loop that you see here, similar to with the discrete time for you transform.

So the Forward 48 transform.

It's useful to see how the inverse Fourier transform is implemented in a loop like this for educational

purposes and in practice.

In reality, you never actually use this loop.

You just want to use the AI function.

Anyway, let us begin.

We are going to start by creating a signal here.

So let's see.

We define a sampling rate, a time vector.

So our signal is going to be around two seconds long.

It goes from minus one to plus one here.

I'm defining a bunch of frequencies and then here we have a loop.

So we initialize this variable signal to be zeros and then in a loop we're saying this signal equals

itself, plus a sine wave at this particular frequency.

So each of these frequencies times the face.

I'm treating this indexing looping variable here also as an amplitude scaling parameter.

So there's no particularly amazing there's no particularly interesting or important reason why I set

it up like this.

Sometimes it's just useful to add a little bit of variety.

So let's see let's see what the signal looks like.

So plot time by signal.

And this looks very similar to these kind of very simplistic artificial signals that we've created earlier

in this section.

So the goal here is going to be to create another signal, another time domain signal based on the inverse

Fourier transform using the procedure that I described in the previous video in the slides.

And you will discover that that signal gives us the same result as what you're looking at here.

All right.

So let's begin, I suppose actually this is the important next line here.

We take the 50 of the signal and then I'm dividing by PN, which actually isn't really necessary here

because this is a normalization factor.

We're going to have to actually undo this normalization factor later.

So but anyway, so here we take the forward 48 transform of the signal, and that gives us this vector

of Fourier coefficients.

And then let's see.

So here a lot of this stuff looks really similar to what you saw in the loop for the forward, for a

transform.

So we have this normalized time vector that goes from zero to one or one tiny point, less than one.

And this is the reconstructed signal.

So let's see what we do in this loop.

So we loop over frequencies.

And just like in the forward 48 transform, there are PN frequencies.

So what we do here is create a coefficient modulated complex sine wave for this frequency.

So let's see.

Let me close this.

So let's begin by just creating the complex sine wave.

So that is E to the and then we need I times K where K gets replaced by two times pi times the frequency

which is the looping index minus one times time.

And now here you have to be really careful.

We're not using the time vector that we created for the signal we are using the normalized time vector

here called for a time.

OK, now this is not yet correct.

What we've done here is create this template complex sine wave for each frequency.

What we need to do is modulate this according to the Fourier coefficient from each corresponding frequency.

So I'm going to write for a CofS the F if coefficient times this complex sine wave.

So then I call this variable Fourier sign and then all we do is set the reconstructed signal to be equal

to itself.

Plus this for a sign that we've just created up here.

All right.

So let's run this code.

And so we don't get any matlab errors.

That's already a good sign.

And then we plot them here.

So let's see.

Oops.

Just this part, so then what you can see here is the original signal in blue and the red dots correspond

to the or the red circles correspond to the oh sorry, I got that backwards.

The red circles correspond to the original signal and the blue line with the dots corresponds to the

reconstructed signal.

And you can see that they match quite well.

Well, quite well.

They are in fact perfect.

One is a perfect representation of the other.

Now, you might have noticed that I have this function real here that's sometimes necessary and sometimes

unnecessary.

So now when I plotted this with out taking the real part of this function, we only see the red dots.

In fact, all the blue stuff is still, in fact, in here somewhere.

So what is going on here?

What's actually happening?

Well, let's start by looking at this recons signal.

So whose recon signal?

You can see that this is actually a complex valued signal.

Now, that might seem a little bit strange, because I said in the video that the reconstruction of

the inverse for a transform should be perfect.

It should be exactly our original signal.

So let's see what the real parts and the imaginary parts of this reconstructed signal looks like.

So I'm going to plot time by the real part of the recon signal.

And actually, well, this is what we already saw before.

So I'm going to do now is plot on top of this time by the imaginary part of the recon signal.

And that is just a flat line.

It's it's really just a flat line.

We can zoom all the way in.

Let's see.

I'll set the the Y limits, to be honest, do it like this.

So minus one to plus one.

And actually let's just leave it like that.

So now you can see we still don't see any variance on the red line here.

So now I'm going to say times point oh one like this.

So I'm zooming in.

So now this is ten to the minus five here, the scale on the Y axis.

Let's add a few more zeros in here.

OK, so now we get ten to the minus nine and I don't know if you can see it on your screen.

We're starting to get some tiny, tiny, very OK.

Now we're at ten to the minus ten and now we can finally start seeing a little bit of fluctuations in

the imaginary part of this signal.

Now, notice that these fluctuations are about 12 or 11 or 12 orders of magnitude smaller than the real

part of this signal.

So these are tiny, tiny, tiny little things.

Essentially, what's happening is that there are some very, very small computer rounding errors that

happen in in particular in this equation.

And actually, it's a lot of it is coming from here because this thing, you know, we can write PI

on the computer.

It says three point one four one six.

This is just four digits being displayed here.

But in fact, that computer is representing it out to, you know, whatever the computer precision is.

But this is actually not pi.

This is an approximation of pi.

PI is kind of like a theoretical number, right?

It's an irrational number.

It goes on forever without any repeating patterns or without end, as far as we know.

And this thing that the computer is representing is not actually PI.

It's an approximation of PI that is generally more than good enough for nearly all cases.

But you do see that it introduces these tiny, tiny computer rounding errors that eventually can start

getting, you know, large enough to be observable.

And I'm making air quotes with my fingers here, which you can't see but large enough means, you know,

11 or 12 orders of magnitude smaller than the actual signal.

So these are tiny computer rounding errors and they can simply be ignored.

And I wonder we might not even get them if we're using the function I left, which should be more accurate.

So let's see who's recons signal.

Yeah.

So in fact, this the imaginary part of this goes away.

You can see when I use the F of T function, this is no longer a complex valued result.

It is just a real valued result.

So now we can generate this plot again and we get the same result.

But the reason why I wanted to go through all of this and explain about the imaginary part being really

tiny is that this is the sort of thing that you see often in practical data analysis.

You expect something to be zero or nonexistent and it ends up being a present.

But really, really, really, really, really, really small.

So those are computer rounding errors, and that is essentially the difference between theoretical math

and doing math on a digital computer.

OK, but the main point of this video was for you to see the inverse Fourier transform written out in

a loop like this.

So it is a little bit more concrete in the context of what I discussed in the previous video.

And you saw a demonstration of how we can perfectly reconstruct a time domain signal by going into the

for the frequency domain, using the forward for a transform and then getting back to the time domain,

using the inverse Fourier transform.

