Now, I'm going to walk you through my solutions to this project.

You're probably tired of hearing me say this, but I will repeat it because I think it's really important

to understand it doesn't matter if your code looks exactly the same as my code.

It doesn't matter if you use more loops than I do.

What matters is that you are getting the concepts and gaining expertise in neural time series analysis

in Matlab.

So here we load in the data.

I convert the data from single precision to double precision.

I don't think that's necessary here, but it does in theory increase the accuracy of the F of T results.

So here is the difference with these timing indices.

So typically I would set this up using D search engine or whatever, so that this is just two numbers

that gives me the boundary.

In this case.

You can see this is an entire vector.

It's a logical vector.

It's 640 elements long, which corresponds to six hundred and forty time points.

And it's all zeros, which is actually falsies everywhere except for this range here where it's one

which means true.

So that tells us that these are the time points that satisfy these criteria.

So these are the time points where the time series vector is both greater than minus eight hundred and

at the same time less than zero and the same thing here.

So you'll see in a minute how this ends up working out when we index the data.

OK, so here are the FFE parameters I specified in the instructions that you have to use and NFTE of

a thousand.

So you're going to be upsampling or also called zero padding in order to spectral subsample.

And this is a tricky point here.

When you generate the vector of frequencies, you're still always going from zero up to Nyqvist.

And here I'm using a little programming trick to actually go up to the sampling rate, although there

are only valid up to Nykvist.

But it doesn't matter.

What does matter is that you define the vector of Hertz and you define the frequency resolution.

So the number of frequency points between Zero and Nykvist corresponding to the end of the 50, not

corresponding to the number of time points.

So if you did something like this, then this would be wrong, because here you're suggesting that there

are 640 total frequency bands, but that's actually not the case.

We're going to have an FFE frequency bins.

OK, and then here I specify the boundaries for the frequencies and I'm using the same approach that

I did here for the time points.

And let's see down here is where I extract the power.

So here I'm taking the 15 of the data from all of the channels, all of the trials and just these time

points.

Now, here is where you really see the difference between implementing the time index this way versus

how I typically do it, because typically I would just have the boundary.

So then I would need to do something like this to IDEX Preto.

Now, the thing is, you have to be really careful about which way you are defining these indices,

because if you do it this way, then.

OK, so first of all, you get an error, but if you would convert this into numbers, then it wouldn't

even make sense.

So either way you do it is fine, but you have to be careful that you're doing it in a way that's consistent.

OK, and now we're using the optional second input into the fine function.

So we specify explicitly the end of the fee.

And then, of course, here I'm also explicitly specifying to compute the 50 over the second dimension,

which is time points.

And then all in one line, I'm extracting the magnitude of the forty eight coefficients and then squaring

them.

Now this.

So data ex pre and data ex post.

This is actually the entire power spectrum.

So sixty four channels, one thousand frequencies and ninety nine trials.

But of course we are only interested in theta and alpha and so that is where I extract them here.

So here I say theta and this theta power from the Pristina's interval and now I need to mean functions

here and that's because we need to mean over frequencies.

That's the second dimension.

And then we need to average over trials, which is the third dimension.

And I would like you to think about to generate an hypothesis and then test empirically in matlab,

whether it matters if you if you say two, three or three, two.

So in other words, if you first average frequencies and then average trials or if you first average

trials and then averaged.

Of frequencies.

All right, I will set that back and now here we run this, and this gives us a vector of sixty four

elements that corresponds to the theta power.

So average from between three and eight hurts over ninety nine trials and then we get one value per

electrode.

So then this gets repeated for Alpha and that's the same code.

But replace Theta with Alpha here and then we can compute the ratios.

So Alpha pre to theta pre and alpha post to theta post and that gives us rat ratio pre and rat post.

All right, so run all that code.

Now we get to the plotting.

So the plotting business itself isn't so tricky.

That's not so interesting.

Here is the way that I get the Greek characters into the title of the figures.

So using Slash Alpha and slash Theta.

This comes from latex coding.

So you can always use latex code inside graphical objects.

If you are familiar with programming and latex or if you're coming from python programming, then you

might notice that we don't have to include any dollar sign in in here too.

In case the latest code, you can just type the latest coding the Mathlouthi coding directly.

OK, so I would like now to discuss actually let me plot this first.

I would like to discuss how I selected these color limits for the raw power ratio and the log of the

ratio.

So what I've done here in both cases is specify color limits go from minus one, plus one, and then

here I'm scaling them by some factor.

So this part is not super interesting.

This still gives us symmetric numbers around zero.

So minus point seven five and plus point seven five.

The main difference is that here I'm adding a one.

So that actually gives me the values of point to five plus point to five and plus one point seventy

five, which means that the average to the middle point in here is going to be one.

Not to understand why that needs to be the case, consider what would be the null hypothesis.

So the null hypothesis occurs when theta power exactly equals alpha power.

So then you have the same values in the numerator and the denominator.

And that means that the expected value, the null hypothesis value is exactly one.

So therefore I want these plots to be centered around one.

I want the color values to have a middle range of one.

Now, in contrast, when I compute the logarithm down here, so it's the log of these ratios.

The log works a bit differently, so in particular, the log of the log base 10 are also the natural

log of one is zero.

So therefore, when you convert this ratio into log units, the null hypothesis value is no longer one.

It's zero.

So that's why I don't need to add any additional offset factor here.

Otherwise, you can see these two sets of plots are more or less the same, they're certainly qualitatively

very similar.

They differ a little bit.

Now, part of the difference between these you could fix by adjusting the color limit so you can change

these color limits a bit to get these plots to look a little bit more similar.

But part of it is just that they are going to differ naturally and that's just the result of taking

the logarithm.

And let me show you this really quick.

So I'm going to specify X goes from, let's say, linearly spaced numbers from point zero one up to

two and say one hundred of them.

Now I'm going to plot X by block 10 X.

And so here you see that function.

So this is X and this is the log of X.

So you do see that when X equals one, the log of X is zero and then it goes down very steeply, down

to minus infinity here and on this side, on the right side, it goes up to plus infinity, but it climbs

up to plus infinity much more slowly than it does climbing down to minus infinity here.

So it is a non-linear relationship, and that's why these two sets of plots are going to be really similar

to each other, but not identical.

And of course, they're similar because any non-linear function is well approximated by linear function

when you have a small enough range.

So you can imagine that if all the data values are kind of in this range, then a linear function is

going to match this logarithm pretty closely.

OK, so that's about it for this project, I think that's the end in the cognitive neuroscience literature.

You don't often see data shown in this way as a ratio of two different frequency bands.

This is more common in resting state EEG data, and sometimes they use this and D.C.I applications of

brain computer interface applications.

From a practical perspective, like in BCI, that's fine.

From a scientific perspective, it's sometimes a little bit awkward to plot a ratio like this, and

that's because it's not really clear what is driving these effects here.

So if you see a change in the alpha to theta ratio like what you see here for stimulus versus stimulus,

then this effect could be due either to an increase in alpha or a decrease in theta or the other way

around, of course, depending on what what the result looks like.

Or it could be both.

So it could be, for example, that this ratio is changing, but the ratio is changing because both

Alpha and Theta are increasing.

But let's say Alpha is increasing more and Theta is only increasing a little bit.

So basically these ratios can be a little bit difficult to interpret.

Nonetheless, it's useful to know about these frequency band ratios.

And I hope that you gain some more expertise in programming, spectral time series analysis.