 So I assume that by now you have completed or at least gave your 100 percent effort to project one. From the section, and now I'm going to show you my solutions. So first of all, we need to just run all of this code here. This is all stuff that was copied from this previous script. So let's see. There's that one and that one. That one. This will take a minute because particularly to generate all these plots. All right, now for the project, so the first thing to do was to compute the FFE over all the channels. Now, you could have done this in a loop. That's totally fine. You can initialize some. Matrix that stores all the four year coefficients and then in a loop over channels, run the 50 or maybe even did it in a loop over channels and the loop over trials. So a double loop, that's all fine. It turns out that the FFE function will accept matrices of any size. So here I'm inputting the three dimensional matrix corresponding to the EEG data. You just have to be careful when you're inputting a matrix into the FFE function that the FFP is going to be computed along the correct dimensions. So you don't want to compute the four, you transform along the channels dimension, nor do you want to compute the eight transform along the trials dimension. So whenever you are inputting a matrix into the FFE function, I recommend always adding this optional third input to specify exactly which dimension you are computing the FFE over. Now, sometimes you don't even need this because the default dimension is to compute over columns and times in the columns here in the second dimension. Still, I think it's good to do this. Good practice also helps the readability of the code. OK, so then we want to extract power or I guess amplitude technically is extracting amplitude. Now, if you're familiar with the details of the Fourier transform, for example, if you took my course on forty eight transform, you will probably recognize that this is not going to give power and it's also not going to give amplitude in the correct units in the original units of the data. Now, in this particular case, that's totally fine because what we care about here is the shape of the power spectrum. We don't care about the actual values themselves. We just care about the shape. And if that's really all you care about, then you don't need to include those two normalization factors. So that would be dividing by the number of time points and multiplying the positive coefficients or so the coefficients for the positive frequencies by two. And then here I'm averaging over the third dimension, which corresponds to trials. And you'll notice that I'm first extracting the magnitude of the 40 coefficients and then averaging over trials, I'm not averaging the four year coefficients first and then extracting the magnitude. That's a subtle but really important distinction, and I went over that a few videos ago. OK, here is the vector of frequencies. And then here I define the frequency cutoffs, these are in Hertz, so for 11 and 20 and I just guessed that by I guess it, but I just came up with those numbers by inspecting these spectral plots. So, for example, you can do data cursor mode on and now I can click around here. So I just kind of guessed that four hertz or somewhere around four hertz is a good lower frequency curve. And for this I picked 11 hertz because that's right in the middle of these two peaks. And then over here, 20 hertz is around this point here. OK, again, you might have done this differently, maybe you selected some points that were a little higher up, you would have been a little bit more selective, and then maybe you would have had to list four frequencies instead of just three because of this, of course, this 11 here. Is both the upper boundary of the lower frequency and the lower boundary of the higher frequency. So then here I convert that into frequency indices, so that ends up being five, 13 and 23 and we can see that hurts if I do the actual frequencies. So I requested here for 11 and 20, the actual frequencies are around three point six, ten point seventy six and just under 20 hertz. OK, so now we need to extract the power in the special windows, actually, that says first, but it really should say both spectral windows. So here is the first spectral window. So we got the data panel, which is channels by frequency, and then we do from the first frequency next to the second frequency index and then the second frequency index to the third frequency index. So this gives us a single vector corresponding to the average power at each channel from inside these spectral windows. OK, and then some plotting. This stuff is pretty straightforward. I won't go into detail about that. So here's how these graphs look now, visually, qualitatively, it's pretty obvious that this lower frequency peak maps better onto this type of one compared to this type of two and vice versa for the upper frequency peak. So then the next step is to quantify that, and that's one of the useful. Advantages of simulating data that you're able to quantify the mapping of the data onto the ground truth. So the way that I do that is by comparing the correlation coefficient between the vector of power values that each channel and the, uh. Leitchfield forward model. So this topography over channels. So now when you use the core COHA function, you actually are getting back for values. So this is the full correlation matrix. And what we care about is actually just one number. And it turns out that this is a symmetric matrix. And that's not so surprising. The correlation of variable one with variable two is the same thing as the correlation of variable to with variable one. Now, this correlation is negative. That might look weird when considering this plot. However, you'll notice that I actually put a minus sign in front of all these. Dipole projections, and that's really just about the orientation of the dipole and it's pretty arbitrary, we don't actually care about the sign. What we care about is the strength of the relationship. Fortunately, when you square it, it all becomes positive. Anyway, that's another advantage of computing R-squared as opposed to just, ah, just the correlation. Now, if you have the statistics toolbox, you might have used the function core instead of core instead of Kirchhoff. And this is going to give you the same result, except it doesn't output a full matrix. It only returns the one value that you are interested in. So if you use the core function, then this would actually just be that value squared. And in fact then, you know, you could even just put this square term here and you wouldn't even need to worry about that. OK, there is another thing that I would like to point out, which is that the core and core functions require columns as inputs, not rows. So if you would input these as rows like this, some transposing both these vectors so that they are rows, but you're going to get out is something that looks really weird. It's a huge matrix, all of Enan. And so it's a sixty four by 64 matrix of Nans. This is definitely not what you want. Now there is actually a very good reason why it's giving out a sixty four by sixty four matrix of Anan's. That has to do with some concepts in linear algebra and how. Yeah, basically these end up being outr products instead of inner products. That part doesn't matter so much, the point is, if you were ever trying to correlate to variables and you get a huge matrix of opinions, the first thing that you should do is check whether you have row vectors or column vectors. And I'll give you also a little tip, a little matlab tip. You can always create a column vector by an operation called vector zation, and that basically just looks like this. So you just say parentheses and then a colon and that's going to force a column vector. Even if this variable is a row vector or a matrix, if this is a matrix, you will still get a column vector. Now, that's not going to work if you're indexing part of a matrix because it's not legal in Matlab to do something like this. So if you have a matrix that you were indexing or slicing would be the python terminology like this. And you want to make sure that this is a column vector, what you can do is use the reshape function so you can say reshape and then the sizes that you want to reshape into are empty comma one. So now Matlab is going to reshape whatever is in here, whether it's a vector or a matrix into one row, and however many columns are necessary based on the number of elements in that matrix. Now, in this case, that doesn't do anything because it's already was a column vector. So this is just a little bit of FII some matlab programming. All right. So all the rest of these lines are all identical. It's just correlation coefficients. You just have to be really careful that you're running the correlation between the right variable. So these two are low frequency power, these two are the higher frequency power. And then the first one is dipole one. And then the second one is dipole, too. OK, once you get that, then you can make this bar plot and this shows the fit of the data to the ground truth. Now, this is pretty interesting to look at. First of all, you see that it's going in the right direction. So all the numbers are going in the right direction. The fit to the ground is highest for the lower frequency and Dipo one and the higher frequency and dipole two and the fit is lower to the lower frequency in Dyball two and higher frequency and dipole one. So that part looks kind of good. However, a fit to the ground truth of between points. So an R squared of between point six two point seven. It's actually not very good for simulated data with very low noise. That's really not a great R squared. And it's also not really encouraging that here for these kind of, quote unquote, wrong conditions, the R-squared is still relatively high around point five point four here and whatever this is, point five, two or something. So this isn't really that great. You will see later in this course on the section on multivariate decompositions and components based analysis that we can do quite a bit better than this. However, this isn't so bad. I didn't mean to be so dismissive, but we can do better anyway. These are all my solutions. If you have any remaining questions, please feel free to post them to the Q&A forum.