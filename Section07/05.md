 If you are not already familiar with the convolution theorem, then this video is going to be really illuminating. This video will give you a whole new perspective on convolution, a different way of thinking about convolution in a different way of implementing convolution. And importantly, you will learn in this video why Morleigh Wavelet Convolution is exactly the right thing to do for time frequency analysis. So let's jump right in with the convolution theorem. Simply put, the convolution theorem says that convolution in the time domain and to be clear, convolution in the time domain refers to a sliding time series of dot products between the kernel and the signal. That's what I explained in one or two videos ago. So convolution in the time domain is the same thing as multiplication in the frequency domain. So let's look at this diagram to get a sense of what this means. So here is some signal and here is a kernel. So this is not a Morleigh wavelet. This is just a little Gaussian kernel. And here is the signal. Now, what you've learned so far is time domain convolution. So you take the kernel, you line it up with a signal, you compute the product, you do some zero padding and blah, blah, blah, and then you slide the kernel along some more zero padding here. And then the result of convolution is longer than the signal you cut off the wings and so on and you get a result that looks something like this. So this would be the result of convolution. And then what you can do is take the Fourier transform of this signal. This result of convolution and plot the power spectrum. And for example, that looks something like this. OK, so this is time domain convolution what you already know. Now, the convolution theorem tells us that we can do this procedure in another way so we can take the FFT separately of the signal and of the kernel. So we are not involving them. We're not computing any products. We're not lining them up or doing any anything unusual. All we do is take the 50 of the signal and the 50 of the kernel and their power spectra might look something like this. And then once we have these two Fourier spectra, we multiply them together. So not the product or not something just element wise or frequency wise multiplication. So this four year coefficient times, this Fourier coefficient all the way down to this Fourier coefficient times this four year coefficient. And that is going to give us one Fourier coefficient series. So one power spectrum in this case. And then we can take the inverse Fourier transform of this multiplied spectrum and get back into the time domain. And so the convolution theorem tells us that this Fourier spectrum and this Fourier spectrum are exactly identical. And therefore this Time series and this time series are also exactly identical and I really mean identical. It's not that this is like a sort of kind of reasonably good enough approximation of this. No, these are absolutely numerically identical. Now, why is this so important? You might look at this and think, actually, you know, this involves more steps here. We have to do a Ph.D. and multiplication and then inverse that fees, whereas here we only do two steps. So isn't this the better way to go? It turns out that this is not the better way to go. And the reason is that the FFE, the F the first F is for fast. It's really, really, really fast. So even though this looks like more steps, this is actually computationally a lot faster than doing convolution where we have to do lots of multiplications and additions. So in other words, the total number of flops, floating point operations is actually larger here than here, even though conceptually this seems like it's less work than this whole business here. OK, but expressing convolution as spectral multiplication is not just about decreasing the computation time. It also provides us a whole new way of thinking about convolution, a new perspective on convolution. And I want to spend a few minutes now to tell you about that new perspective. OK, so what we have here is to say, well, it's one signal. I'm just repeating it. It's a sine wave at some frequency, pure sine wave. These two signals are exactly the same. And here we have two kind of different kernels. They're both Gaussian. They only differ in their widths. So this Gaussian is a bit wider. This Gaussian is a bit narrower, and here is the result of convolution. But. I mean, this signal and this kernel, so here we get essentially a flat line, it turns out it's not totally flat. There's some edge effects, which I will talk about in a moment. But let's just imagine this is a flat line. And here, even though the signal is exactly the same, the kernel is nearly the same, just a little narrower. We get a result that is basically the same as the original signal. So we still get a sine wave at the same frequency, but it's dampened. It's attenuated. Now, the question is, why does that happen? Why do we get this? Why do we get a flat line when we use a wider Gaussian and and dampened sine wave at the same frequency when we use the narrower Gaussian? Now, if you're thinking about convolution as a sliding time series of DOT products, this can make sense. You can make this result make sense. And it kind of has to do with, you know, like the window of integration. So this Gaussian is wider. So you get more cycles from this frequency inside the Gaussian. So the negative bits and the positive bits will cancel each other out. That's all true. And here, you know, it's a little bit harder to think about, but you still get some window of integration where the negative bits and the positive bits are canceling each other out, but they don't cancel out quite as much as they do here. So I'm not saying that it's totally incomprehensible to get this result when thinking about convolution in the time domain. But we can make this result much more sensible. We can make this much easier to understand when we think about convolution from the perspective of the frequency domain and from the perspective of spectral multiplication. So what you see here is the power spectra of the signal and the kernel. So the signal is a pure sine wave. So we just got a bar at whatever frequency. The numbers, of course, don't matter here. So here we just get a bar and it turns out that the power spectrum or the spectral representation of a Gaussian is a negative exponential. So it's going down like this. So now what we do is frequency wise, multiply the spectrum of the signal by the spectrum of the colonel. Now, notice that at these frequencies down here, the signal has no representation. So it's whatever these values are, times zero. And then we get down here and now we have the the signal has a positive representation in the frequency domain, but the kernel has already tapered down to zero. So then we get this thing times zero from the kernel. And so essentially we get basically zeros everywhere in the spectrum because the power spectrum of the signal and the power spectrum of the kernel do not overlap at any point. They don't share any spectral features with each other, so therefore the result ends up being all zeros in the frequency domain. And of course, when you take the inverse Fourier transform of a flat power or an empty, you know, a zero power spectrum, the result is going to be zero. OK, now it turns out that the Gaussian still has a little bit of energy here. And so that's why there are these tiny edge effects here. So it's not a perfect it's not exactly flat line, but these are some details that don't really matter here. OK, so now what's going to happen here? Well, the frequency representation of the sine wave is exactly the same, of course, but the frequency representation of a narrower Gaussian is a more gentle, exponential decay in the frequency domain. So you can see the general shape is the same for the fat Gaussian and the skinny Gaussian. But the skinny Gaussian is dampening. It's decreasing in power more slowly. So now what happens when we frequency was multiply this spectrum by this spectrum? You can still you can see we still get all zeros here and we get all zeros here because the signal has no energy here. The signal is no energy here. But these two features, these two time series have overlapping spectral representations here exactly at the frequency of the sine wave. And you can also see that it's attenuated, it's dampened relative to the total signal. And that is why we got this result. When you take the inverse Fourier transform of the multiplication, the product of this spectrum and this spectrum. So I think this is really beautiful. This is a great way to conceptualize convolution as a spectral filter. And what this shows you is that the result of convolution is going to be in the frequency domain. It's going to be the features of the signal that share characteristics with the features of the kernel. And if those two features don't. Share anything, if they have nothing in common, no common spectral features, then there's no overlap and you're just going to get a whole spectrum of zeros and the inverse Fourier transform will be a flat line. OK, so this is for a kernel that is a Gaussian. Let's go back to our more late wavelet. So remember, this was the time domain representation of the Morleigh wavelet. And in the frequency domain, the more they wavelet is a Gaussian with a peak corresponding to the frequency that was used to create the sine wave. So now let's go back to this example. So I showed this in the previous video or two videos ago where this is the signal. This is the kernel. And this is the result of convolution between this kernel and this signal. So now let's think about this again as spectral multiplication. So we take the 48 transform on the signal, the Fourier transform of the kernel, overlay them in the frequency domain, and then we multiply them frequency wise. So elegant Y's multiplication. So you can see is that all of the low frequency features of the signal get obliterated because the kernel has zero energy at these frequencies. Likewise, at all these higher frequencies, the kernel has zero energy. So it doesn't matter what's happening in the signal at these higher frequencies, they are going to get obliterated because the kernel has zero energy or at least very, very close to zero energy. And when you do this frequency Y's multiplication, the only features that will survive are the features in the signal that have energy at the frequencies where the kernel also has energy. And in fact, this ends up being basically just a narrowband filter. We are attenuating or obliterating lower frequencies and attenuating to obliterating higher frequencies, and we are only preserving we are only allowing to pass through the frequencies in this narrow range here. Now, you actually get to define how wide this Gaussian is in the frequency domain, that is a key parameter of Morleigh wavelet convolution. It's related to the width of this Gaussian, the Gaussian that creates this wavelet in the time domain. And I'm going to have a whole video just on this parameter. So for now, just be aware that that is possible to change this shape. But we're going to be ignoring that parameter and any discussions about that parameter for the moment. OK, so then the thing is, if we would just stop convolution as spectral multiplication and then we would just say, you know, here's this part of the blue spectrum where the science spectrum has non-zero energy. If we just stop there, there would really be no advantage of this approach. Wavelet convolution over the static Fourier transform because still, when we are here in the frequency domain, we still don't have any information about temporal dynamics or non-state personalities. So once you get here, you multiply all the spectra. You have to take the inverse Fourier transform to get back into the time domain. And that's why this works really well. For a time frequency analysis. We get these spectral information isolated from the frequency domain multiplication and then we take the inverse Fourier transform to get back into the time domain. And that gives us the temporal dynamics, but only the temporal dynamics that are in the frequency range specified by or constrained by this kernel. And again, this is all given to us by the convolution theorem. Now, there's one more thing I want to say about the convolution theorem, which is that it's a little bit about something about terminology. So in the time domain, you would say that we are performing convolution and that is a sliding time series of DOT products between the kernel and the signal. Now, when you go into the frequency domain, you can no longer call this convolution in the frequency domain. This is multiplication in the frequency domain. It's not convolution in the frequency domain. The terminology gets tricky. What we are implementing here is convolution in the time domain implemented as multiplication in the frequency domain. The reason why this is important is that convolute so if you would use the term convolution in the frequency domain, what that actually refers to, what that term really means is taking the kernel and computing a time series or a frequency series of sliding dot products between the kernel in the frequency domain and the signal in the frequency domain. So whenever you use the term convolution, it refers to a sliding series of DOT products, whether it's in the time domain or in the frequency domain. So you just have to be a little bit careful with the terminology. And by the way, the convolution theorem works the other way around as well, so if you would implement convolution in the frequency domain, that's actually equivalent to multiplication in the time domain. So this thing works both ways. But in general, certainly for time frequency analysis, we are only interested in convolution in the time domain implemented through spectral multiplication. So here are the five steps of performing convolution. The first step is to compute the ends and by the ends I'm referring to the number of time points in the signal, the number of time points in the kernel, the wavelet, and the length of the result of convolution. So n plus and minus one. The reason why this is important is that even though you're doing this through spectral multiplication, we still have to make sure that the result of convolution is N plus and minus one. We still need that parameter. We need that length for a valid result of convolution. So then once you've computed the ends, you have to take the efforts of the signal and the kernel. And remember, these are separate fees. So here at step two, the signal and the kernel are totally separate from each other. They do not meet. They do not intermingle. It's only at step three where you multiply the spectra element by element. So frequency by frequency. And for this multiplication to be valid, what has to be true about the signal or the 50 of the signal and the 50 of the kernel, I'm sure you've guessed it, to multiply their spectra, we need the safety of the signal and the fifty the kernel to be exactly the same length and that length is N plus and minus one. So that's why we have to compute the ends up here. So if the signal is longer than the kernel, then it doesn't actually matter for the fees because they both get zero padded. So we have to do frequency domain zero padding or that is say 50 zero padding. Again, you don't do that manually. You let the FFT function do it for you by specifying the second input into the FFT function. So then you multiply the spectra and then you take the inverse Fourier transform of one spectra. Right after step three, you only have one spectrum. Then you take the inverse Fourier transform to get back into the time domain. And then the length of this result is going to be N plus and minus one. So this is actually going to be longer. The result of step four is going to be longer than the original signal, just like in the time domain. So then step five, which is optional, but I don't see why you wouldn't do it. There's no reason not to do step five is to cut off the wings of convolution. And that is half the length of the wavelet and the beginning and half the length of the wavelet at the end. And after all of this procedure, you will end up with a result of convolution that is ready for time frequency information extraction. So we are almost there. What do you know now is you know about Morleigh Wavelets, you know about convolution, you know about the Fourier transform. There's still a few more things that you need to learn before you are ready to do a complete full on logit time frequency analysis. So you need to learn about Complex Morleigh Wavelets. You need to learn about extracting information from complex wavelets, and you need to learn about multi wavelet convolution. Now, these all sound like really fancy terms, but you basically already know all of this stuff. I just haven't really put it together for you. For example, extracting all the information from complex wavelet coefficients. This is really the same thing that you were doing with the Fourier coefficients. We just think about them in a slightly different way. Anyway, I hope you enjoyed this video. I hope you found it useful and inspiring. And Eye-Opening and don't go too far because you have just a little bit more to learn and then you are ready to do your first time frequency analysis.