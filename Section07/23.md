A few videos ago, I discussed the one over f temporal structure of spectral and time frequency brain

activity.

And in that video I said that there are two approaches for dealing with this one over F structure.

One approach is to normalize it out.

And there I introduce you to two methods, mainly decibel.

That's what I focused most on.

And then you can also use percent change.

So some kind of divisive normalization.

And then I also mentioned that another option for dealing with the one over F structure is to embrace

it, to analyze it and study it, because this one over F structure reflects the fact that the brain

is in a scale free state or a state that approaches criticality.

And so that's what we are going to focus on in this video.

In particular, I'm going to tell you about an analysis method called Defriended Fluctuation Analysis,

sometimes abbreviated as DFA.

And that is probably the main method that people use for analyzing temporal structure in scalfari dynamics.

So I want to start by calling up a slide that I used in a previous video.

So there I mentioned that when you have scale free networks, you will find that these statistical descriptive

statistical measures of central tendency are generally not very informative.

So that's things like the mean or the median or the mode.

If you look at this distribution of earthquake magnitudes as a function of earthquake frequency, you

can see that these measures that there is no real central tendency here.

So you technically could compute the average of all these numbers, but the average is not very informative

about what's happening here.

And so systems that exhibit this kind of scale, free or fractal characteristic, are often said to

be in a critical state or a state that is approaching criticality.

And that means that these systems are maximally flexible for adapting to different kinds of environments

or different kinds of inputs.

So in this video, I'm not really going to say much more about the theory of scalfari dynamics.

Instead, I want to focus more on the analysis method of DfE determined and fluctuation analysis.

However, I do want to highlight just two findings out of many, many, many studies on temporal dynamics

and one of our characteristics in brain activity.

But this is just two examples to show you that these kinds of one over dynamics are actually related

to various characteristics that people might be interested in, such as age.

So you can see comparing the EEG or the temporal structure of EEG signals from young adults versus older

adults, and they primarily differ in the rate at which the energy decays over different frequencies,

with older adults having a more shallow one over.

And here you see another example I actually forget what was the point of this study.

But you do see comparing the red line and the blue line that there are differences.

So there are peaks in the spectrum.

But if you look across the peaks, you can also see that there's a difference in the one of Raef characteristics.

Now you can quantify one of Raef in the frequency domain like this, but it's also common to measure

this in the time domain.

So to look for scale free dynamics using the time domain version of the signal.

And that is the idea of the trended fluctuation analysis.

So there's a couple of steps to computing the DfE and I'm going to walk you through them.

So you start with your signal and then step one is to convert that signal into a mean centered cumulative

sum.

So that means your signal might start off looking something like this and then you start by means centering

the signal.

So that's pretty easy.

You just subtract the mean and then you compute the cumulative sum.

The cumulative sum means that you take each time point and add it to the previous time point, and that

gives you quite a different looking characteristic signal.

So this is the mean centered cumulative, some of this signal here.

And what you see in this means in this cumulative some version of this signal is these longer trends.

So what you don't see so much here.

So here you see the signal is going up and down, of course.

But what you can see here in this version and the cumulative sum which you don't really see here, is

that in this part of the signal here, there are more jumps going downwards and there are jumps going

upward.

So, of course, there are increases in the signal energy, but there's more steps going down than there

are and fewer steps going up.

And here it's the opposite for this area of the signal here and now for electrophysiology.

It's actually common not to do this analysis on the time domain signal itself, the original time domain

signal, but on the power spectrum where the amplitude spectrum.

So this signal would actually come from first can.

Using the ample two time series from, for example, Wavelet Convolution, and then you work with the

Amplitude Time series.

OK, so this is step one.

Then we have step two, which is to define a number of scales that are logarithmically spaced.

So a scale here just refers to a width of time.

So what you're seeing here is the plot of these scales.

In this case, I chose 20 scales and each scale has a different duration.

So you can see the first scale is one second.

It's a window of one second and then up to scale 20, which in this case is I guess it's around twenty

three seconds or so.

So you can see that these scales actually get really long.

This is not an analysis that you can apply to brief task related epics.

You need a really, really long time series to do.

This analysis should be, you know, it's typically done during spontaneous activities or resting state

activity where you might have, for example, 10 minutes of the the research participant not really

doing anything, just sitting in a chair, relaxing.

OK, so then so now we have step two.

We have all of these scales.

So 20 scales ranging from one second to twenty three seconds or whatever.

And then what you do with these scales is you use them to cut the data into epochs.

So we have this really long time series and let's say this is ten minutes long or something.

And then what we do is apoc the data.

So we segment the data according to these different scales.

So the longest scale that I showed here was twenty three seconds.

So each one of these windows might be twenty three seconds.

Sometimes you got a little bit at the end here you get some, some little bit that doesn't fit into

one of these segments, which is fine, it's generally just a small amount of data.

OK, so then each segment gets trended and then you compute the root mean square.

So that would look something like this.

And I've converted all of these segments in here into these segments, these Time series.

Now, this looks like it might be short, but each one of these is, you know, the time scale here

is twenty three seconds.

So this could actually be pretty long.

So this first signal here, this first segment here would come from this one and then it's just trended.

So you remove this trend line and then for each one of these you compute the root, mean square root,

mean square is a pretty simple calculation.

And to interpret this, you basically just read this title backwards.

So first you square all of the individual elements.

So this would be a time point.

So any time points in this segment here.

So you square each individual element and then you compute the mean, which is summing up all the squared

terms and dividing by NP and then you compute the square root.

So root mean squared is computed as the square and then the mean and then the root.

This is actually very closely related concept to variance.

This is a measure of the total energy in the signal.

Now I have this little subscript s here because you get a different route means square for each scale.

So this would be the root mean square just for this one scale.

Twenty three seconds.

Now you compute this root mean square for each one of these times segments individually and then you

average all these together.

So that gives you at the end of step three, that gives you one value of root, mean square average

across all these different segments for this one particular scale.

OK, and then we repeat this step for all of the different scales.

So previously I said that that was twenty three seconds.

Maybe this one is 18 seconds or something.

So this is now 18 seconds instead of twenty three seconds.

So when you finish going through this procedure, you will have a vector of scales that I showed a moment

ago and a vector of root mean square values.

And then you plot them on a line in a plot where where the axes are both log scale.

So this is the log of the root mean square and the log of the data scale or the time scale.

And then you just compute the linear fit between these.

And what you will always find is that this relationship is positive and it's generally linear in this

log log scale, which means it's actually a log relationship.

But we look for a linear fit and the theoretical value of the DfE for pure white noise is point five.

So you can see this is empirical noise.

So I simulated random numbers and then I got an empirical DfE of point five 06.

So it's really, really close to the theoretical value of point five.

And then you see in your data, this is the real data.

You will get a DfE values often called Hayhurst Exponent.

This this slope here, the slope of this line is called the first exponent.

And in this case it ended up being between eight seven.

And the interpretation of this value is that systems that have long range memory so strong, positive.

Autocorrelation in time have Hearst exponents that are higher than point five, generally somewhere

close to one or between point five and one.

So this is indicative of a system that is in a critical state.

And this is actually an interesting value here.

Point eight seven, because when you look in the literature, in the neuroscience literature, you will

find that Hearst exponents from exactly computed based on exactly this analysis description here, they

tend to be somewhere around point seven point eight point nine.

So this is a pretty typical value that you see across a large range of data sets and individuals.

All right.

So this is an overview of the analysis procedure for computing the trend and fluctuation analysis.

As I mentioned, it's often used to study the criticality of systems in neuroscience.

People use DfE analysis to study various patient groups, for example, comparing this first exponent

between individuals with Alzheimer's versus controls or individuals with schizophrenia versus controls

or healthy aging and so on.

This is generally something that fluctuates over slower timescales like minutes, hours to decades.

So DfE analysis can be interesting, but this is not an analysis that you can apply to a kind of brief

task related design where your data Epic's are only a few seconds long.