Let's talk about temporal resolution versus temporal precision before and after wavelet convolution,

I will start by defining what these terms mean and how they differ from each other.

And then essentially, I'm going to argue that it's a good idea to temporarily downsampled your data

after you've already done the time frequency decomposition.

So let's start with some definitions and differentiations temporal resolution.

You already know it is the number of measurement points per unit time, so per second typically.

So that's really just a sampling rate.

The temporal resolution is the sampling rate.

No big shocker there.

Temporal precision, on the other hand, can be a little bit more difficult to quantify.

The temporal precision is essentially the ability to distinguish neighboring points.

And this is related to the information content in the signal.

It's related to how quickly the dynamics of the signal are changing.

So let's take an example.

A typical EEG temporal resolution or sampling rate is around a kilohertz or a thousand hertz or a thousand

time points per second thereabout.

Sometimes people record at half a kilohertz or sometimes two kilohertz.

But let's just a typical sampling resolution.

Temporal resolution is one thousand hertz.

Now, imagine if I recorded data at one million hertz.

So now, instead of recording one thousand data points per second, I'm recording one million data points

per second.

So the question is, do I really need that extra temporal resolution?

Am I getting anything more out of the signal that I wouldn't get from sampling at a kilohertz?

And the answer is no.

It is unnecessary to sample EEG data at a million hertz because the data just don't change that fast

EEG data change or the information that we're looking for, an EEG data change on the timescale of generally

around tens of milliseconds.

So if you would get a million data points per second, then essentially the the resolution is much higher

than the precision.

So then you should then you just have an excess amount of data.

So it's useful when the precision matches the resolution.

OK, so enough of that.

Let's talk about time frequency analysis.

And here I'm referring specifically to Wavelet Convolution and Filter Hilbert, because this is a bit

different for this statement is actually not true for short time, for a transform and multi taper method.

But for Wavelet Convolution and Filter Hilbert, the time frequency analysis does not change the temporal

resolution.

So if you have your data recorded at a kilohertz and then you do a wavelike convolution for time frequency

analysis, the results of the time frequency analysis are still at one kilohertz.

You haven't lost any time points.

However, what you have done is reduce the temporal precision.

And why is that the case?

Why is the temporal precision lower when we've performed wavelet convolution or filtering?

The answer is that it has to do with the amount of smoothing that gets imposed by the wavelet or the

filtering.

So when you do your time frequency analysis, you are necessarily imposing some filtering, some temporal

smearing.

Now that's necessary, of course, because it allows us to get the frequency resolution, but it does

mean that we have reduced temporal precision and that means that the temporal resolution ends up being

higher than the temporal precision.

And when the temporal resolution is higher than the temporal precision, then it means that you have

an excess amount of data.

You could reduce the amount of data without losing any information.

Let me show you a picture so you can see what this looks like.

So here in the blue dots, this is power in decibels and this is at the native resolution.

So each dot corresponds to a time point.

And now each red circle is my proposal for the time points that we should actually save.

So what I'm suggesting here is that we Temperley down sample, not the original data.

We're not down sampling the raw data.

We are down sampling the results, the power time series, the results of wavelet convolution.

And my question for you is, are we going to come to a different conclusion about what's happening in

the data?

Based on this sub sampling, based on this downsampling of the data from this time window, and of course,

the answer is no, we are not losing any information.

Therefore, we can get rid of a lot of these excess blue dots, which allows us to save data, safe

space without losing any information.

Now, if you only have one time series like this and basically like the little toy examples that we've

been working with so far, then it doesn't really matter.

You might as well just keep all of the full temporal resolution data.

However, imagine you are running a big data set where you have, let's say, ninety six electrodes,

three seconds for the trial.

The trial lasts for three seconds.

You have four experiment conditions and 50 frequencies and you're saving results for power.

And it's those data matrices.

Those results matrices will be huge.

They will be ginormous.

They will be difficult and slow to work with.

They will take up a lot of disk space.

They'll be slow to save the data from the Matlab buffer into matte files.

And they will also take a long time to load from the disk back into Matlab when you need to work with

them more.

So therefore, it's a good idea to downsampled your results after wavelet convolution.

OK, what I want to show you here is two plots.

One of these is that the full temporal resolution and one of them has reduced temporal resolution.

So the data, the results were downsampled after the time frequency analysis was done.

And I would like you to pause the video and figure out which one is the full original resolution and

which one is downsampled.

OK, so I'm assuming that you pause the video and had a guess and maybe you got the correct answer.

Maybe you guessed it right.

But I bet that it took you many seconds to come to to gain any confidence about your results.

The answer is this is the original temporal resolution.

This is the Temperley downsampled version of this same plot.

So you really have to look hard.

You really have to search around to find time points where our time frequency regions where you can

see any difference between these two.

So, again, the idea is you would not come to a different conclusion about brain dynamics based on

this plot versus this plot.

However, this dataset here takes up a lot less space, so it's more convenient to work with.

OK, so I want to show you an example where this can fail.

This is like a failure scenario.

So here what I've done is downsampled the data after time frequency analysis to 20 hertz.

So there's one data point every fifty milliseconds.

Now, this is really excessive downsampling here.

We actually are losing information.

So if you look in this low frequency range, this is not so bad.

You know, we're maybe losing a little bit here, but this doesn't look so bad.

But at this higher frequency in 30 hertz, it's not really such a high frequency.

But even up at this high frequency of thirty two hertz, we are already losing a lot of information.

You know, you're missing this whole double peak here and it's just a line going straight through.

So this is excessive.

So you do have to be mindful of what you are downsampling to.

I typically downsampled depending on the frequencies that I'm looking at and the of how much data I

have and the kind of data it is, I would typically downsampled to somewhere around 50 hertz, I think

50 hertz.

So that's one data point every twenty milliseconds.

That's generally a pretty reasonable range in most cases for a level of downsampling where you are reducing

the file sizes without losing any information.

So this sort of thing temporally downsampling after you've completed your time frequency analysis.

It's something that I generally recommend doing, but it's not something that's really necessary.

It's just can be useful for working, making, working with files a bit faster.

