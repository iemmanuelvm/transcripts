 This video is all about time domain convolution implemented in Matlab. So let's start by building some intuition with a simple example. This is really similar to what I showed in the beginning of the slides in the previous video. So let's see. We are going to start by making a kernel. This is like a little Galson. You can see it's E to the minus and then this is T squared or X squared, some variable squared. And then this is a this is just normalizing it so that it integrates to one. And that's not strictly necessary. That just kind of helps with the plotting and the visualization. So here you see clearly looks like a Gaussian and the values are smaller. It doesn't peek at one. And that's because we have divided this, normalize this by the sum. And the effect of this is that the kernel, the Gaussian now has unit energy. So if you would sum up all of the values in the kernel, so you just sum up all of these points here, then you end up with one, OK, and then we have some other options. I'm going to get back to this in a moment. So here we are creating a signal and let's see what these look like. So this is the signal you'll recognize this from the previous video. This is the colonel here and let's convolve them. So for convenience, right now, I'm using the Matlab con function. I'm going to show you in the next cell, I think, or actually well, sometime later in this video, I'm going to show you how what convolution looks like when you implement it in code. But this is just, you know, something simple for now so that we can focus a bit more on building intuition. So we're plotting the result of convolution in a red line and we see what the result of convolution looks like. And we get a really informative legend with very informative labels here. Let's see. So this is not thing. What is the first thing that is being plotted? The first thing that gets plotted is the kernel. And then the second object that gets plotted is the signal. And then the last thing is not the Rocinante that is the result of convolution. Let's try this now. All right. This looks better. OK, so now we see, as I mentioned in the previous video, that the effect of involving this signal with this kernel is to smooth out the signal. Now, that is not a general feature of a convolution convolution, is not something that smooths out the signal. This is a smoothing kernel because this is a the result is a smooth version of the signal because the kernel is a smoothing kernel of some of those. Complicated to say. So now let's try this. I'm going to set this kernel just to be the negative version of itself. So that's a pretty interesting thing. So now I'm going to put this in figure three so we can compare. So here the kernel flipped. So it's the same Gaussian, but now it's upside down. And what's interesting is that it flipped the signal. So this is like, you know, the upside down world where what goes up here goes down here. Maybe it's like, well, it's not like a mirror reflection because this would be a really strange mirror. Anyway, this is too science fiction for me to think about. OK, but you see the idea. Let's try this other kernel and maybe I'll put this in. I guess I can overwrite this figure. We don't really need it. OK, so this is a pretty interesting colonel, look what we've done, we've changed the colonel so that it's all zeros and then it suddenly jumps up to one and then it suddenly jumps down to minus one. I'm adding you have to notice that I'm adding one here. That's just for the visualization in this plot so we can see it without interfering with the signal. So this is actually zero in the kernel. And then it jumps up to one, it jumps down to minus one, and then it jumps back up to zero. And what's interesting is that this turns out to be this kernel turns out to be a signed edge detector. So it's an edge detector because it's flat. When there's no edges, it's flat here. But the result of convolution is also flat here, even though the signal has a value of one. So it's an edge detector, but it's also a signed edge detector. So it's detecting whether we have a up going edge or a downward going edge. So this is going up. This is going up. And here this goes down. It goes down very steeply. And then we go down and then we go up. So that's pretty interesting. And in fact, if you ever study machine learning or deep learning or computer vision, you will learn that these kinds of filters turn out to be really useful for low level decoding of of visual features. So you have edge detectors for images. That's also the way that like X-rays at airports work, they're doing edge detection to allow the the screeners to see sharp boundaries, for example, of like guns or whatever they're looking for. OK, let's look at this question. What does the third input in the second function mean and what happens if you remove it? OK, so there's a couple of ways of answering this question. And I'm going to do the method of looking at the whole file so you can. So so I just write clicked on this function and then, uh, sorry, double left click and then right clicked. And now you can do help. You can type help now will open up the help document and sometimes this is useful. You get the description, the syntax and you see some examples. That's pretty nice. Sometimes there are pictures in here. So sometimes I do this, sometimes I also just open up the file. So then I get the help text right here in in text. So in here is the answer that we are looking for. So this is, uh, kind of a bee shape, so returns a subsection of the Constitution was specified by shape, so full returns, the full convolution remember that the full convolution corresponds to the size and plus minus one. That's the default if you don't add any input. And then same returns the central part of the convolution that is the same size as a.. So this option, same does what I referred to in the videos as clipping the wings. So cutting off the wings of the result of convolution. So let's see what happens if we return full, which actually means just getting rid of this. So let's see, we make this plot again. And I think I'm going to go back to this original kernel here. Uh, let's generate this plot again, so now it looks like it's actually shifted the reason why it's shifted. So it's not actually a problem with convolution. You have to keep in mind that one half of the length of the wavelet is added here in the beginning. So in fact, because we're not specifying any time vectors here that the beginning of the signal in the result of convolution is actually somewhere around here. So this is actually a phase shifted version of the right result. So we should really be centering this. And the fact that we're not doing that here is the reason why it looks like we're getting the wrong result. OK, so that's just something to keep in mind. If you are using the con function, it's always good to specify this to this third input to be same. So you can see now it's the same result, but it's shifted into the right orientation. Very nice. OK, so now we are going to look at another example of convolution. So let's see. We start by creating a signal. It's just all zeros and it's just like a boxcar function. So it's going to be ones in the middle and zeros in the edges. And then we create our convolution kernel. You can see this is just a linear decreasing function. So I also actually showed this signal and this colonel in the slides in the previous video, so now we compute the convolution sizes so we have the length of the signal, the length of the kernel, the length of the result of convolution is in plus minus one. I don't know what all this business is. That should be minus one like that. OK, let's do some plotting here. So we plot the signal plot the colonel and again, I'm using the matlab con function just for convenience, just so you can already see what this result looks like. So you can kind of imagine taking this, taking this kernel and sliding it along the signal and then this is going to be the result. OK, so then we see convolution in action and I will run this code multiple times so we can look at it and talk about it. But essentially this is going to implement convolution one time step at a time and allow us to visualize what this thing looks like as a movie. So let's see. So here you see the signal and the colonel and here is the result of convolution appear in these black dots, so it goes all the way up and then it comes all the way back down. Now, that's actually pretty interesting because you don't really get that sense when we looked at this result. But actually, if you look at the y axis, you do see this goes up to one and this goes up to three. So why is that happening? Why is the result of convolution so much larger than the original signal? Well, the answer comes from thinking about how Convolution works as a time series of sliding DOT products. So when this convolution is here in the middle, we are taking the DOT product between ones. So this is all ones. And this kernel, which is like starts off at point two. So the kernel is just multiplying by one and then summing up together. And if you sum all these together, this is one plus point eight and then of course this ends up being three. OK, so in other words, the kernel has a mean offset. What happens if you mean center the kernel? Why don't we try it and find out? So let's see. We are going to means into the kernel. So I'm just going to actually delete this comment here, which means adding this extra part. Notice here that I'm flipping the kernel. So here I a kernel or k flip equals kernel and then this is just spinning it around backwards and then it's actually k flip that gets implemented in this loop here. And so here inside this loop, what I'm doing is grabbing a chunk of data. So this is the data for convolution. I'm grabbing a piece of the the, the data, the time series signal and then I'm computing the DOT product. So some after Element Y's multiplication and that ends up being the result of convolution at this particular time point. And then this is just updating the plot and pausing four point six seconds at each iteration. You can also see that the way that I'm actually implementing this convolution in Matlab is slightly different from how I explained it in theory in the slides. So in the slides I explained that the signal stays fixed and it's the kernel that's moving along the signal. But here I'm actually not changing the kernel itself. I'm just grabbing different windows of data. And this is also that for conv. So we can see that's a zero potted version of the signal. So the signal here in the middle and then zeroes before and zeros after. And how many zeros are there. Well, half the length of the kernel. OK, so we were going to test this again with subtracting the or mean centering the kernel. So let's see, we need to rerun from here. And. Let's see what this does now, so it's certainly going to be different and it's going to be different because now when the colonel is here, we still are multiplying by old ones. But now, because the colonel is mean centered, it has an average value of zero. So when we multiply by ones and add together, then we are actually just getting some products of zero. And then this ends up being negative here because the colonel gets multiplied by zero. So the first couple of points in the colonel are multiplied by zeros. And these last few points are these last few points. And the colonel are multiplying these values of one here. So this gets negative. So were you surprised that this result of convolution looks so much different? It looks qualitatively different just by means centering the colonel. Maybe you expected that the result would be essentially this shape, but it would be the height of the signal instead of being a height of three. So what I challenge you to do, if you would like a little code challenge, but you can try to do now, is do something to the colonel. So change the colonel somehow so that you still get this result. But the height of this is the same as the original signal. So we end up with a result of convolution that as a height of one rather than a height of three. So that'll be a nice little challenge for you to do.