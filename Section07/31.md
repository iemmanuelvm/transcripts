 Many analyses that you do with EEG data involve averaging the data over all of the trials, and that, of course, makes sense because we assume that single trial data can be a bit noisy. And that's why it's beneficial to collect data from a lot of trials and then average over all of the trials. That gives us a cleaner estimate. However, there are often interesting dynamics that are happening within each individual across different trials. And so then it's also useful to know how to do within subject's analyses and in particular, cross trial analyses. And in this video, I'm going to introduce you to cross trial regression. Sometimes these kinds of analyses are called single trial analyses, but I actually think that term is a little bit of a misnomer because generally when you're doing what's commonly called single trial analyses, you're not actually analyzing a single trial. You're still doing the analyses across all the trials. It's just you're looking at variance within a subject instead of variability over different subjects averaged in each subject. But anyway, that's just a matter of linguistics. So what I'm going to show you is how to do cross trial regression using the least squares solution. So let me start by explaining the problem and why we need an approach like this. Well, it's not a problem per say, but you get what I mean. OK, so let's imagine you're doing some task related experiment. You have the research participant doing some kind of, you know, engaging in some kind of behavior, maybe a trial start. There's a stimulus that appears on the screen and they have to make a decision about stimulus and then press the button. And simultaneously, of course, you are measuring brain activity. Now, people don't behave exactly the same way on every single trial, every single stimulus repetition. Sometimes they're going to be slower, sometimes they're going to be faster. So let's say we want to try and correlate the variability in behavior over different trials with variability in the brain responses so we can imagine building up a plot that looks something like this. So for one trial, this is trial one. We have one measure of behavior. And in this case, I'm illustrating reaction time. But this could be anything else. It could be it doesn't even have to be behavior. It could be some stimulus quality. Maybe it's the brightness of the stimulus or the size of the stimulus. It could be some other physiological measure, maybe like pupil diameter or EMG or anything you want. So you have one variable, one measure for the behavior on this trial. And you also extract some feature from the brain signal. Maybe that is time, frequency, power from some time, frequency window. So for trial one, we get a dot in this space that tells us that this was the behavioral index and this was the brain index on trial one. Then we go to trial two and we have, you know, the same basic setup. But now the subject was a little bit faster to respond. He made his decision a little bit sooner. So we get a different behavior measure and a different brain measure. And then we go through this for all the trials and then you can build up a plot that looks like this. And so each dot here, each circle corresponds to the pair of data values. So behavior and brain from each particular trial. So this is one trial, another trial and so on. For all the trials now, it's visually clear that there is some relationship between this measure of brain signal and this measure of behavior. So then you can imagine what we want to do next. We want to fit a line that goes through this cloud, that goes to this population of data, and then we are interested in the slope of this line. And the null hypothesis is that this slope would be zero. So if there's no relationship between this behavior and this feature of the brain, then we would expect this to be basically just, you know, a random cloud of dots. And then the best fit line would be flat. And so the null hypothesis would be that this slope is zero. Now, you already know that there are many, many features that we can extract from EEG signals. And in particular in this section, of course, we are focusing on the time, frequency, power dynamics. So imagine extracting this slope for one time, frequency point one pixel in a time frequency plane. Then you repeat this procedure for all of the time frequency pixels and that would give you a map that looks something like this. Now this looks like a time frequency power plot, but this is not time, frequency power. This is time on this axis and frequency on the Y axis. But the color here actually corresponds to a correlation coefficient. So what you see in this map is not how much energy there is in the signal at different times frequency regions, but instead. Now, the energy over trials is related to the different reaction times over the different trials, so it's literally this slope here, the slope of this line plotted for every single time frequency pixel. OK, so that is the concept. That's the goal. Then the question is, how do we go about implementing this in a in an intelligent way? So we're going to do that using statistics and using the framework of the general linear model, often abbreviated Galam. This is the basic equation that a GM is trying to solve. So let me walk you through what these different terms mean. We have this matrix called X, and this is called a design matrix. And the idea of the design matrix is that the columns contain what are called independent variables. Sometimes they're called the predictors are the aggressors or the explanatory variables. There's a couple of different terms, but essentially the design Matrix X contains columns where you are trying to predict variability in the outcome measure and the outcome measure is vector y. So it's the outcome measure or sometimes called the dependent variable, sometimes just called the data. And then the goal of the general linear model is to identify this vector of regression coefficients, which are also sometimes called beta parameters. These are really similar to correlation coefficients. These are in fact basically correlation coefficients that are in the same scale as the data. Or you can turn that around and think of a correlation coefficient as being a normalized beta regression parameter. So let's go back to this example here. So we have two variables here, the behavior and the brain. And now when you only have two variables, it's up to you which one you want to call the dependent measure and which one you want to call the independent measure. You know, are you trying to predict the behavior based on brain activity or are you trying to predict the brain activity based on the behavior? Either one is fine. Mathematically, the procedure is going to be the same. It's more just a matter of interpretation. Now, if you're having larger and more complicated regression models, general linear models with more predictors, so more than two variables, then it does start to matter which variable you call it, the dependent measure in which variable you call the independent variable. So in general, what you call the dependent variable is the thing you are trying to explain. And the independent variables are the variables that you are using to try to explain variance in the dependent measure. OK, and so the beta is basically the slope over here. So that gets us back to this equation. So the design matrix times, the beta coefficients is equal to the data or is an approximation of the data? You know, sometimes there's also this error term or variance term written here. Now, before telling you how to solve this equation, I want to talk a little bit more about how to set up the design matrix. So to construct a design matrix for the glim. You start by writing out a model equation that looks something like this. So we say the dependent variable Y equals beta one times the explanatory variable or the independent variable one plus beta two times explanatory variable two plus beta three times explanatory variable three. Now in the picture that I showed a few slides ago, I was just showing you basically two variables. There was the dependent variable. Let's call that reaction time and then the independent variable, which was the time frequency power. So I'm showing three terms here, partly because this is the intercept. This is called the intercept term. I'm going to talk about this in a moment and then I'm just kind of generalizing it a bit here to show you with more possibilities. So let's say our model is that we are trying to explain the reaction time using a weighted combination of the intercept, which is some scalar times, the intercept plus some other weight. So another beta value times the time, frequency power plus another beta value. And I don't know, maybe this is the reaction time from the previous trial. So we have these three variables that we are trying to use. We are combining them a linear weighted combination of the three explanatory variables or independent variables to try to predict the dependent variable, which is reaction time now to be totally mathematically correct, you would actually need to have an error term here. So this would have to be plus Epsilon or something. However, for our purposes here, for constructing the design matrix, we don't need to worry about that. We are just using this as a stepping stone for conceptualizing what this design matrix should look like. OK, so then we have to build up these matrices and they look something like this, so. This is the design matrix that's Matrix X that I showed in the previous site, and then we have a vector of the beta coefficients and then we have the vector of a dependent variable. So why now this design matrix? It has the number of columns corresponding to the number of explanatory variables. And the first one is always the intercept term and the number of rows corresponds to the number of trials. So this is trial one, trial two and so on up to trial. And so this value here coming from this term here, this would be the ETG power, time, frequency, power from trial, one time frequency, power from trial two and so on. And then this is the reaction time from trial one reaction time from trial to and so on. And then you have a number of beta coefficients here corresponding to the number of explanatory variables that you have in your design matrix. So I hope this is sensible. I hope this is clear. The design matrix has one wrote one column for every explanatory variable or independent variable and one row for every trial in the experiment. OK, now let me start talking about this intercept a little bit. So this intercept is just a vector of ones. It doesn't vary. It doesn't change. It's always won. Every single trial has an intercept of one. And why do we need this term? Where does this term come from? Well, this term is basically an offset term. The intercept is an offset term that is literally where the Y axis is, where the line goes through the Y axis, when the x axis equals zero. So in this drawing, the way I've drawn this here, then, you know, the intercept would be around here. Now, the thing is that if you don't have an intercept term in the model, if you leave out the intercept term in the model, this line becomes constrained to go through the origin. So zero and zero. Now, you can imagine that if you constrain this model so that the line, the best fit line must go through the origin, you're not necessarily going to get a great fit to these data. In fact, an even better example of this would be if there was a negative correlation. So imagine if all these dots were going down this way. Then the best fit line goes like this. But a line that goes like this is never going to go through the origin. So the best fit line would still go look, be positive, even though the data are actually negatively correlated. So that's why you need an intercept term that essentially allows this best fit line to leave the origin of the graph. Now it turns out that you can actually avoid having an intercept term by means centering both the both of your data value. So you force the the dependent variable and all the independent variables to have an average of exactly zero. And then the intercept is going to be necessarily zero. So then you don't need the intercept term. Nonetheless, that is more like theoretical than practical in practice. Always have an intercept term. So in the simple case that I showed in the beginning of this video with trying to explain reaction time from time, frequency power, you still have two terms in the model. So two columns, the intercept and the Time Frequency Power. OK, so now that I've explained the the motivation for doing within subject's cross trial analyses and I've shown you how to construct a design matrix and how to set up this equation, I know I want to show you how to solve this equation and how to get these beta coefficients. Remember, these beta coefficients are the slopes of the lines. They're telling us how important each of these variables is for explaining variability in the does not in the dependent variable. So in our example, the reaction time. So here we have our equation X equals Y. Now, if these were you know, so we're trying to solve for BITA, if these were just three individual numbers, it would be really easy to solve for Beita. You would just divide both sides by X and then you would get that Beita equals Y. Divided by X, however, is not so simple in this case because these are matrices and vectors and we cannot simply divide matrices the way you can divide individual numbers. OK, so what we do is expand this term a little bit and then we do something called left multiplying by the left inverse. This is a collection of matrices that are all derived from this original design matrix. And these are constructed in such a way that when you add the left inverse to the left side, you run through this matrix multiplication. This left inverse will cancel out this design matrix X, and that will isolate Beita on the left hand side of the equation. Now, of course, whatever you do to one side of the equation, the. You have to do to the other side of the equation, that's why we have this huge space over here. So you also have to multiply the left inversed by Y. OK, so that leads to this final equation for solving for beta. And it's essentially beta equals the left inverse of X Matrix, X design, matrix X times vector Y. Now, if all of this looks really confusing and you have no idea what's going on, you don't know how to interpret this, then don't worry. This is a bit of linear algebra. The proof that this is the right equation. This is really the best way to solve the general linear model is not so difficult to work through. However, it does require a bit of background knowledge in linear algebra. It's beyond the scope of this course. So if this doesn't make sense and don't worry about it, that doesn't matter. In practice, software programs can very easily implement this equation. The important point here is the concept, the conceptual idea that we are trying to correlate the some measure of behavior with some measure of brain activity. And we can do that for a single time frequency point and we can expand that to an entire time frequency plot.