 We're going to start off here by exploring inter trial phase clustering in simulated data, and then we will get to real data in a few moments. So the goal here is to develop some visual intuition for the correspondence between it and phase angle distributions. So let's get started. What we are going to do here is simulate data with random face value. So you can see these are uniformly distributed numbers. So this goes from zero to one and then we multiply by two pi. So that's going to give us random phase angles. And then what I'm doing here is scaling that back down. So here I'm scaling it up and then here I'm scaling it back down by some fraction. That's between zero and one. So here it's at the point five. And, you know, you can do a bit of advanced calculus in your head and see that this actually just ends up being number scale from zero to PI, because the point five here will cancel out the two. OK, and then what we are going to do is compute it. So here we want the magnitude of the vector. That is the average vector. So let's walk through this. So we want the angles, that same data, and then we want to oilrigs them so e to the I times, same data and then we want to take the average of all of those vectors and then we want to take the magnitude of the average vector. So you can also see it's written right here as well. But it's good to walk through this from the inside going out. So what we're doing here is computing the preferred angle. Now, this is generally not used in neuroscience. It is occasionally it can be relevant if you want to look at the specific angle of the phase clustering. But most of the time this isn't used. Most of the time people just focus on the length of the average vector. So let's see. So the ETPs is zero point six and the preferred phase angle is one point six seven, which I guess is around PI over to. OK, so let's do some plotting here. I'm making a histogram of these data and here you can see we are going from zero to PI and then here is to PI. And all of these phase angles are just filling up one half of this space. That's not surprising. That's exactly what we defined up here. And then here is a polar plot. So we are making a polar distribution of all the phase angles. And indeed, you can see that the average phase angle or the average vector, sorry, is pointing roughly up. So the angle is close to PI over to now another part of this exercise and another reason why I'm simulating the data like this is for you to appreciate that the length of the average vector is not the same thing as the proportion of the circle that is filled. So it has to do here with the clustering. So, for example, this is still a point five. I'm going to run this code again, this number. In fact, I'm not changing anything here. The only thing that's changing are these random numbers. So now it's point six three and point six, seven, six got even higher. I'm curious to plot that one. Now, let's see. This is so this is with point six one. And now I ran it again, just some different random numbers. And now we got a phase clustering that is stronger. And you can see. So the overall distribution, the width of this distribution is approximately the same. And it just happened by chance that we get some more vectors here. You can see, you know, there's a bunch of vectors around here and here and here. These are some type clustering. And here there's very few vectors. It's purely by chance. OK, so you can continue playing around with this number. For example, let's try setting this to point one and then running all of this code again. And we can see that with a really tight distribution. We end up with a ITC that is really close to one, actually its point nine eight. OK, so I hope that helps with the intuition about the average vector. And actually I want to do one more. Let's make this very large. Let's make it how about point nine? So now we are going to see very little clustering because these phase angles are distributed all almost the whole way around the circle except for 10 percent here. All right. And now we are going to compute ITC for one electrode and we are going to do this over the entire time frequency plane. In fact, I believe this is the data that I showed in the slides and the previous video. So let's see. We load in the data set. We define our frequencies and we are going to have 40 frequencies spread from. Two hertz to thirty hertz, so let's see. And we're going to be looking at Channel P Z. OK, so here we set the range of cycles. This is the number of cycles parameter for the wavelets, for the Morleigh Wavelets. This parameter right here is the main topic of the next video, the next several videos, actually. So let's see. So but essentially what this is doing is defining the tradeoff between temporal precision and spectral precision for the time frequency plot. So then here we define frequencies a number of cycles for the Gaussian that creates the wavelet, the time vector for the wavelet. And the rest of these are the parameters for the convolution, which you are already familiar with. So then here we take the F 50 of the data. And of course, you've seen code like this before. So we are taking the data from this channel. So this line of code here is String Compar. So we are searching for the string P, Z and all of these channel labels and that returns a boolean vector, which is false everywhere, except for the one cell, the one channel for which the label matches the letters P Z. OK, and then we are reshaping that into a one by NP data. So sometimes I leave this empty and sometimes I explicitly write in and data and it's an end point fifty. OK, so then we are initializing the matrix, the time frequency matrix and let's see what's happening in here. So we are looping over frequencies and we create the wavelet, take its FFE, normalize the wavelet amplitude, normalize the wavelet. And then here's a question. Is this next line necessary? Is it necessary to normalize the wavelet? I'm going to get back. You can already start thinking about this. I'm going to get back to this in a moment. And then here we run convolution here. We have to reshape the data back to time by trials and then we compute ETPs. So notice we are computing it for all time points, one frequency at a time. OK, so I'm going to set this looping index variable to one, and that's going to allow us to basically walk through this for loop one piece at a time. Let's see. So I would like to look at these things. It's always a good idea to check what these things look like. So time by the real part of the wavelets. So there you go. And now maybe I'll also plot the time by the imaginary part of the wavelet just to make sure that this really is a complex wavelet. And we can see that the cosine part and the are the real part corresponds to a cosine and the imaginary part corresponds to a wobbly sine wave. OK, so now here is the question. Is this next line necessary? And just to remind you what this next line is doing, I'm going to plot the amplitude spectrum of the wavelets. And so this I discussed in a previous video. And what you see is that the wavelet goes up. So in the frequency domain, it goes above 150. So now that means that when we multiply it by the data with this wavelet, we are actually boosting the energy, artificially enhancing the energy in the signal at these frequencies, because the spectrum of the data is being multiplied by up to one hundred and fifty something, whatever this number happens to be. So the answer to this question is no, we do not need this next line. We do not need to normalize the wavelet in this particular case. And the question is, why is that the case? Why do we not need to normalize the wavelet? Well, the answer comes from the fact that in this code here, we are only interested in ITC. That means we only care about the phase values. We don't care about the amplitude values. We don't care about the power values or the projection onto the real axis. All we care about is the phase and the phase. Values are independent of power, so it doesn't matter if we multiply power by a billion or, you know, point zero zero zero zero zero zero one, we are only extracting the phase angles. OK, so you know, you can leave this line in. It's certainly not hurting anything, but it's totally unnecessary. OK, let's see. And then we run convolution here and that gives us back a vector as analytic signal. That is sixty three thousand points long and sixty three three sixty happens to correspond to the number of time points times the number of trials. So what we need. To do is reshape that vector back into a matrix, and I'm going to do that here by saying reshape as to ETG dot points by ETG dot trials. There you go. Let's run this code. And that looks good. Six hundred and forty by ninety nine. Now here we want to compute it. So again, I'm going to start from the innermost part of this formula and slowly work my way outwards, methodically and carefully and mathematically. So we start with this variable X and then we want the angles angle and then what do we do with these phase angles? Well, we put them into Euler's formula, and the reason why we put them into Euler's formula is that these are all just phase angles. What we want our unit vectors on the complex plane that have unit amplitude. So an amplitude, a magnitude, a length of one and an angle that's defined by the angle that we get from the result of complex Morleigh wavelet convolution. So this gives us all the angles and this is still two dimensional. So this is time by trial. So then what we want to do is compute the average and we want to compute the average over trials, which is the second dimension. Now, at this point, what we have is one single even run this. So we have one single number per time point. We can see what is the size of this thing. So that becomes a six hundred and forty factor. So this is a time vector of complex numbers. And what these complex numbers actually represent is the average vector in the complex plane. And what we are interested in is the magnitude. So then we take. So there you go. That was IDP's and it's pretty cool. You look at this line, it looks you know, it's a really deeply embedded line of code. If someone looks at this code, they think, oh, man, this is really impressive. Whoever wrote this code is really, really smart. Must be a really great programmer. But you can see that we worked through it from the middle and we went slowly and methodically and we were thinking carefully about every step. And then we ended up with this impressive looking line of code. So if you saw this before you watched me solve it in the video, then give yourself a pat on the back. All right. Anyway, what I'm going to do now is run this loop for all of the frequencies. It's quite fast and then plot the result. Very nice. So here you see the autopsy result that I talked about in the previous slide, in the next video, I'm going to start talking about the key parameter of Morleigh wavelets that defines the trade off between time and frequency precisions. So take a break, go get a get a cup of coffee or tea or water and come back and keep watching. 
