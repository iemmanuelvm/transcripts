 In this video, I'm going to tell you about convolution and how convolution is implemented in the time domain, you will see over the next several videos that there are two ways of thinking about convolution. There's convolution in the time domain, which is a sliding time series of DOT products. I will explain that in a few moments. And then there's also convolution implemented in the frequency domain, which is actually just a different way of thinking about the same procedure. It's a different implementation, but it's fundamentally the same that's given by something called the convolution theorem. Now, the thing is that in practice, when you actually sit down and analyze data, you will never, ever do convolution in the time domain. The way that I'm going to explain it in this video, it's a little bit like the Fourier transform and how you saw how to implement the Fourier transform in a loop. In practice, you never implement a free transform in a loop. You always use the FFT function. Similarly, analogously in convolution, it's important to understand how convolution works in the time domain, but in practice you actually should avoid using time domain convolution whenever possible because it's slow and it requires a lot of extra overhead extra code, which means more opportunity for confusion or mistakes. OK, but as I mentioned, it is really important to understand how convolution works in the time domain for conceptual purposes. So let's begin. What is the goal of convolution? The goal of convolution is to take two time series. Convolution can also be done with images. But here I'm just going to be talking about Time series. You start with two time series and you somehow mix them together to create a third signal, a third time series. Now, traditionally, the two signals that you start with in convolution are called the kernel and the signal now it's a little bit arbitrary, which one you give, which label, which time series gets, which label. However, in general, the signal, what you call the signal is like the thing that you are interested in. It's probably the thing that you measured from the outside world. So this might be your EEG signal and the kernel is like a filter that you are applying to the signal. So and then you do something to combine these two, you combine the signal and the kernel and that gives you the result of convolution. So here you see the three time series that are involved in convolution to signal the kernel and the result. So what does it look like here? What is the result of convolution between this particular signal and this kernel? So in this case, you can see that the result is basically just a smoothed version of the signal. So it looks a lot like the signal, but it's smoother. Now, to be clear, this is not a trivial result of convolution. It's not that the point of convolution is to smooth out the edges in a signal. This particular result happens because of the shape of this kernel. So if you have a kernel that has a different shape, the result of convolution will look different. So what's happening in convolution is that we combine the properties, the features, the characteristics of the kernel and the signal. And the result is somehow like a blend of the features of the signal and the features of the kernel. And I think that's pretty clear from this example here. So this kernel is this little Gaussian smoothing kernel and the signal has a lot of sharp edges. And so the result combines the these sort of ups and downs for the signal with the smoothness of the kernel. Again, if we would change this kernel, we would identify different features of the signal and then the result would look different. So this just gives you a bit of a sense of what convolution involves. Now let's talk about how Convolution is implemented. Mechanistically It is not so complicated, actually, and you already know everything you need to know to understand time domain convolution. So what you do is you take your. So now the signal is this FFP, ERP that we've been working with and the kernel is going to be a wavelet in this case. It's a real valued Morleigh wavelet. So what you do is line up the kernel with part of the signal so you can see they're aligned here and then we do Element Y's multiplication. So you multiply each point in the signal by the corresponding point in the kernel where they are lined up here and then. So Element Y's multiplication, and then you sum all of those element wise multiplications, and that operation is called the you remember correctly, it's the DOT product. So we're computing the DOT product between the kernel and the signal, not the entire signal, but just where it's aligned with the kernel here. And that gives us one single number and that is part of the result. That's not the entire result. That's a DOT product, but it is one time point in the result of convolution. And then what do you think the next step is going to be in convolution? I'm sure you guessed it correctly. What we do is don't touch the signal. You leave the signal as is and you take the kernel and you just slide it over by one time point so you can see I'm just sliding it over. Well, OK, this is more than one time point, but it's, you know, slid over by some reasonable small amount. In practice, you slide the kernel by the smallest time that you possibly can for a discrete signal that's represented on a computer. You would slide this across by one time point. Now, if you're doing this in, you know, if you're learning about convolution in a more abstract mathematical sense than the kernel is slid along by D.T. or decks, you know, an infinitesimal. But here we slide along by the smallest democratization of time that we have. So one sample point and then you just repeat this procedure. So element wise multiplication and some and that gives you another DOT product and you plot that correspond at a location here corresponding to the center of the wavelet. So wherever the wavelet has its center point, that is where the result of convolution gets plotted for this step, this DOT product here. And so for this reason, it's actually best practice. It's easiest to implement convolution using a kernel that has an odd number of points, because if the kernel has an odd number of points, there is an exact center point. Fortunately, it's fairly easy to construct a kernel with an odd number of points because you want zero time equals zero to be in the center. So if you go from minus, let's say, minus one second to plus one second, then that's going to guarantee an odd number of points because you have zero in the middle. OK, so you can see where this is going. We repeat this over and over and over again, sliding the kernel all the way along. And in the end you're going to get a result that looks something like this. So this is the actual result of convolution between this signal and this particular kernel. And so what you can already see is that the result of convolution shares, the characteristics of the signal and the kernel. So the kernel has the characteristic that it's a sine wave or cosine wave that is tapered by the Gaussian. So it has a narrow frequency representation. And what this kernel has done is essentially acted as a band pass filter. So it's isolated the features in the signal that the dynamics in the signal that are in the same frequency as this kernel. You can also see that there's a phase relationship. So the DOT product is going positive, negative, positive, negative, positive, negative and so on. It's ringing up and down. It's oscillating. In other words, there is a sign dependency of convolution. This is actually one of the reasons why we are going to start using complex more like wavelets instead of real valued wavelets. But that's not something you need to be concerned about now. But I'm just giving a little bit of foreshadowing for one of the topics later on in this section. OK, so I hope this procedure makes sense. You can see that convolution is fairly straightforward. It's really just dot products and then sliding the kernel along the Time series. Now, one thing you notice when you look at this result is that the result of convolution is shorter than the original signal. And that happens because when the kernel is aligned here, so the end of the kernel is aligned with the end of the time series, the DOT product gets plotted at a location corresponding to the center of the wavelet where it's aligned with the signal. So that means that convolution ends here. So the signal is shorter than our sorry. The result of convolution is shorter than the original signal. I would like to spend a moment talking about this because this is actually a problem that we need to fix somehow. So let's think about this in another way. Here's another example of convolutions. We have a very simple signal. It's just this plateau shape and then the kernel. In this case, it's just this slope here. It's just this line. And then so you can see that the result of. Convolution, the first point of the results of convolution gets plotted here corresponding to the center of the kernel where it's aligned with the signal and so on, and then the result is shorter than the original signal. Now, when you look at this example and the example with the FP data that I showed in the previous slide, you might be thinking, well, you know, it's not so bad. We lose a couple of points in the beginning and at the end and maybe, you know, we don't have to care about that because there's nothing interesting happening in the signal at these edges. The problem is not that we are losing a small number of points. The problem is that the number of points we lose depends on the size of the kernel relative to the size of the signal. So imagine we would use a kernel that was longer instead of the kernel being only five points. Imagine the kernel was the same length as the signal. Now, for convolution that shouldn't matter, we should be able to run convolution regardless of the lengths of the signal and the kernel. However, if we just followed this procedure of aligning the kernel and the signal here in the beginning and aligning them at the end, then if the kernel is the same length as the signal, then all of convolution just boils down to one single dot product, which is not very, you know, it's not a very useful result. OK, so what do we do instead? What we do instead is modify or I should say extend this procedure a little bit such that we start convolution with the rightmost point of the kernel aligned with the leftmost point of the data like this. So it's not aligned like this. The leftmost point to the kernel with the leftmost point of the signal, it's the rightmost point of the kernel with the leftmost point of the signal. Now, the problem is that this is no longer a valid product computation. Remember, from the beginning of this course when I introduced you to products that the DOT product is a valid operation only when the two signals have the same number of numbers, when they have the same dimensionality. But here, that's no longer the case. In this case, we have a five element kernel trying to compute the product with a one element signal. So that's not a valid operation. So what do we do? Well, you can see it written right here. We have two zero pad, so we add a bunch of zeros. In this particular case, we add four zeros for is not a magical number, but we have a five element kernel. So if the kernel matches with the data for one point, then we add four more zeros. And the idea of adding zeros is that these this is not adding any new information. In fact, we're just zeroing out all the rest of the kernel. But the zero padding here does make this a valid product. So then we get a valid result and you can see that the result is now going to be longer than the signal. So it's going to start in this case, two points before the signal actually starts. OK, and then we follow the same procedure at the end. So at the end of convolution, we have the leftmost point of the kernel aligned with the rightmost point of the signal. And then again, that is not a valid product on its own. So we have to add some zeros so you can see that zero padding in convolution is very different from zero putting in the 48 transform. Zero padding in the for a transform involves putting all of the zeros to the right of the signal and the purpose in of zero padding with the four transform is to increase the frequency resolution. So we have a totally different procedure, a totally different motivation for zero having here in convolution. It gets confusing because the term is the same. We call it zero padding in both cases, but the motivation is different and the mechanism is different and the goal is different. All right. So anyway, let's get back to this example here. So, in fact, we start convolutions such that the rightmost point of the kernel is aligned with the leftmost point of the signal. That means that the result of convolutions starts at half of the wavelet, which means one half of the wavelet too early or before the signal starts. And of course, again, as I mentioned in the previous slide, this is not a valid product here. So we have to add a bunch of zeros. We have to zero pad the signal in order for this operation to be valid. Now, you can see that that also means that these this part of the result of convolution is really difficult to interpret. It's mostly uninterpretable because we are just. Artificially adding all of these zeroes here and the reason why we're adding these zeros is to make this mathematical operation be valid. So this is not really interpretable. Same thing at this site at the end of convolution. This stuff is also not interpretable. So formally convolution the result of convolution is longer than the result of the signal. But in practice, what is basically always done, I guess, is that after convolution finishes, you would cut off this section and cut off this section. I call these the wings of convolution so that in the end the final result of convolution does indeed have the same length as the original signal. So this gets cut off and this part gets cut off. So the next thing we need to discuss about convolution is how much longer is the result of convolution? So not when cutting it off, cutting off the wings, but the default result of convolution linear convolution. How much what is the length of the result of convolution? That turns out to be a simple formula, but it's really important to know. You have to know that formula. It's easy to memorize. So that's the good thing. So the result of convolution is obviously going to be related to the length of the signal and is also going to be obviously related to the length of the kernel because the result of convolution is one half the length of the kernel too long here at the end and one half of the length of the kernel too long here in the beginning. Now it turns out that that's almost the right answer. We also have to account for a minus one. So the formula for the length of the result of convolution is N plus minus one. So NP would be the length of the signal, the number of time points in the signal. M is the number of time points in the kernel. And minus one is this weird little factor that I'm going to discuss in the next slide. So this is an important formula. You have to memorize this formula any time you do time domain convolution or whenever you implement convolution through frequency domain multiplication. So please commit this to memory. It's really important. OK, so the N and the M are intuitive. Why is there a minus one? The reason why there's a minus one GraphicLy. It has to do with the fact that the the kernel and the signal overlap one point over here and one point over here. So that needs to be accounted for. But I think it's easier just to see this in a visual example. So let's start with an imaginary signal. It has length for so four elements, four time points. And here's our kernel, which has like three. So here we are going to show the result of convolution between this kernel and this signal. So we start such that the rightmost point to the kernel is aligned with the leftmost point of the signal. And now we want to compute the product. And of course, the DOT product here is not valid. This is not a valid DOT product situation. So we need to zero pad here and here. All right. And then the result of convolution gets plotted here corresponding to the center of the kernel. So that's the first result of convolution. And then what we do in convolution is keep the signal fixed and then take the kernel and slide it one time point so that we move the kernel down here and then the next result of convolution is going to be here and we can speed this up and watch convolution go through. So here is now the last point of convolution, the last step of convolution, where the leftmost point of the kernel is aligned with the rightmost point of the signal and then the result of convolution gets plotted here according to the center location of the kernel. So here we have a kernel of length for a start, a signal of length for a kernel of length three. And what is the length of the result of convolution? You're a great mathematician. It's six. I'm sure you got that right. And I'm sure you see that the relationship between four and three and six is four plus three. So N plus M minus one gives us six. So that's a graphical representation showing that this is just the way that it works out. All right. So there is one final point about doing convolution in the time domain that I need to tell you about, and that is this bizarre feature of convolution called kernel flipping. And I'm not going to spend too much time talking about it because in practice, we are not going to be implementing convolution in the time domain. We are going to be implementing common. Pollution through frequency domain operations, and when you do convolution through the frequency domain, you don't actually have to worry about Colonel flipping. So Colonel flipping is just this weird thing that only happens in the time domain. So if the actual colonel looks like this now, this is not a Morleigh wavelet, it's not even a wavelet. It's just some random little wiggle that I drew. But I drew it like this just so that it's obvious that it's asymmetric. So if the actual colonel looks like this, then when you go through convolution this time series of sliding DOT products between the colonel and the corresponding segment in the signal, you actually have to flip the colonel backwards. So you have to turn the colonel backwards in time so that you're actually implementing convolution with a flipped version of the colonel. So that would look like this. Now, there's a few different ways to explain why the colonel has to be flipped. One way is mathematical, and that comes from the convolution theorem. And essentially just the math has to work out that you flip the kernel. But there's another way to think about it, which is more conceptual. And that's what I'm going to explain now. So here is a signal and here is the kernel. So let's imagine what convolution looks like from two different perspectives. So this is you imagine you are a God. You are up in heaven looking down at planet earth and you see this signal and you see the kernel. So you are looking into the situation from outside. You are this omniscient being who looks down at everything. And from your perspective, you see the signal and you see the colonel forwards in time. So that is your perspective. But now imagine your perspective is, you know, one of us lowly mortals down here suffering on planet Earth and you are inside the signal. So now imagine you are standing here in the signal. Now, when you are implementing convolution, you're actually looking backwards onto previous values of the signal. So when you look at the signal from, ah, the kernel, from your perspective, it looks like it's forwards from your perspective, but it's actually backwards from the omniscient gods perspective up here. So the fact that the kernel gets flipped has to do with the fact that from the signals perspective, the kernel is still forwards, but the signals perspective is flipped relative to our perspective. OK, wait, I forget which one of us is supposed to be the God and which of us is the lowly serf in this explanation. But anyway, I hope this makes sense. So the colonel is kind of always looking forwards. But the perspective is different from when you're standing outside of evolution versus when you are inside. Convolution looking backwards at the signal, at previous values of the signal. So this business of colonel flipping is something that you will see in code when you see how convolution gets implemented in the time domain. As I mentioned several times already, you don't have to get stressed. Don't worry about Colonel flipping, because in practice, when we do convolution through the frequency domain, you don't have to worry about flipping the colonel.