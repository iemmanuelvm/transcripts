 Continuing on the discussion of normalizing power, time, frequency power in particular, for getting rid of this pesky one over business in this video, I'm going to talk more about the decibel formula. I'm actually going to start by talking about various motivations for baseline normalizing your data. And then I will spend most of the video talking about the importance of selecting an appropriate baseline time window, because that can be a pretty tricky issue. All right. So let me start with some of these motivations. So baseline normalization has several advantages. It allows you to compare better the data that you record, the results across different individuals. That's because the data quality, the specific data values are going to change for different individuals, depending on factors that you're probably not really interested in, like skull thickness and data quality or the preparation of the electrodes, the type of equipment and so on. So if you get an effect that is one microworld in one person, but five micro volts in another person, it might not mean that the the second person has, you know, five times as large an effect as the first person. It could just mean that the the skin preparation was much better. So the electrodes were put on better. So therefore the values, the data values have larger fluctuations. In other words, there is a variety of reasons why the microbial values can differ across different individuals that have nothing to do with the aspects of brain function that you are probably interested in studying. And so normalization actually gets rid of all of those factors. Relatedly, imagine if you have data from EEG and your colleague has data from M.G. and you want to compare your results to your colleagues results. Now, the thing is, Volts and Teslas are just different scales. You can't compare some number of micro volts with some number of Peko Teslas and say that, you know, that the results are different or the same. However, if you both independently normalize your data so you normalize your EEG data to decibels and your colleague normalizes their M.G. data to decibels, then you can directly compare effect sizes between the EEG and the Meji data. OK, secondly, baseline normalization allows you to compare across frequencies, and that's because the normalization is done per frequency and so therefore it gets rid of the one over Heff. So that's what I explained in the previous video. Another advantage of baseline normalization is that it allows you to separate the task related. So these specific aspects of the signal that you were interested in that are elicited by the stimulus from the ongoing or background brain activity. And you will see why, if that's not already clear, you will see why in a moment. Finally, when you baseline normalize the data, the resulting power values are normally distributed or approach a normal distribution and normally distributed data values are good for statistics, in particular parametric statistics. OK, so here you see two formulas. This is showing the two main normalizations that are used in the field. So we have DuSable, which I talk about more and percent change. So I think DuSable is used more often than percent change. So in general, I recommend sticking with decibels. But of course there's nothing wrong with looking at percent change as well. You can see the important feature is the same in both of these formulas and that is having some measure of activity and dividing by a baseline. And this is a frequency specific baseline division. So that part is here as well as the activity divided by the baseline. So these are both fairly comparable and they will both address all four points that I discussed in the previous slide. OK, so what is this baseline window here and what is the implication of selecting a particular baseline for the resulting decibel or percent change time series? So what you see here is five lines. Imagine this is your data at five different time points. And now what we are going to do is normalize these data using two different baseline windows. So here I selected time window one to be the baseline and here I've selected time, window two to be the baseline. And then the rest of the time series was normalized with respect to this this baseline or this baseline. Now, the first thing I would like to notice. To point out to you is that this baseline normalization doesn't actually change the shape of the Time series, so the Times series shape doesn't go any differently. So we're not changing baseline normalization doesn't change the overall characteristics, the temporal characteristics or the temporal dynamics. The only thing that baseline is doing is shifting on the Y axis and maybe also compressing over different frequencies. But really, the way to think about baseline is that we are shifting on the Y axis. So you can see this time series is exactly the same as this Time series. All we're doing is changing what we call time zero. OK, so now the question is, how do we interpret the signal in time window three? Under this situation, we interpret the signal at the power a time window three to be zero change relative to the baseline. So you look at this time window and you say, well, in time Windows three, there is zero power. It's not really zero absolute power, but at zero relative power, whereas here time window three gets interpreted as negative. So here you say there is a suppression of power, there's a decrease in power. Now, both of these interpretations are equally correct. They just depend on a different baseline time window. So this is a suppression of power relative to time window, too, but it's a zero change in power relative to time window one. So this is a bit of a toy model. I'm going to now show you three plots that show exactly the same time frequency data. So these are exactly the same data, exactly the same wavelets, exactly the same wavelet parameters. All this stuff is the same except for the time window used for baseline normalization. So here there is a baseline normalization of minus five to minus two hundred. So it's basically this time window here and here. This is a baseline time window of minus three hundred up to exactly zero, which is when the stimulus came on the screen. And here the baseline is minus one hundred. Minus one hundred. Plus two hundred. So I'd like to start by discussing this panel now. I think we can all agree that this is not a good baseline at a time window. And the reason why this is not a good baseline time window is because the baseline goes well into the stimulus processing period. So you have things that are happening in relation to the stimulus onset of time. Zero are actually getting into the baseline. And that's why we have this funny looking blue stuff down here. This is like one of these situations where you would interpret this as a relative suppression of power, which is technically true. However, it's awkward to to make that interpretation because what it's relative to is actually this big increase in upper alpha power after the stimulus. So so it's a little awkward to call this a an actual suppression, although that's technically what it is based on how we define this baseline. OK, so this is pretty obviously wrong. This is not a good choice of baseline. Now let's discuss panel B and panel C for a bit. I will make the claim or I do make the claim that of these two choices for baseline activity, this panel B is better. This is a better choice for a baseline time window compared to this panel C so minus five hundred to minus two is a better baseline compared to minus 300 to minus to zero. Now I would like you to pause the video and meditate on this for a moment and think about why that is the case. Why is this a better time window than this? So the reason why this is better is because at time zero, we have some temporal leakage from the post stimulus activity here. So remember that whenever you do time frequency analysis through wavelet convolution or any of the other methods that I will start talking about in the coming videos, you always have some temporal smearing. So when you have a wavelet that is centered at time zero. So thinking about convolution in the time domain, when you have a wavelet that is centered at time zero, it's going to include some activity from the post stimulus time window into the estimate of the baseline. So because of that temporal smearing, it's good it's generally good practice to have the baseline time window stop a little bit before time zero. So here we have the baseline goes up to minus two hundred and there's going to be very, very little. Maybe at the lowest frequency, there still be a little bit of of of influence from stimulus activity into the baseline, but very little. It's not something that I would be concerned about. You can also see this here. So you see there's this, you know, at six hertz or whatever this is, it looks like there's a little bit of activity that's starting before time zero. Now, I do not believe that this is reflecting some, you know, precognitive ability. It's not that this person's brain was able to know when the stimulus was going to appear and start processing instead. I think this is just some smearing from some early activity here. It just gets smeared out in time due to the natural smoothing, the symmetric smoothing from the wavelet convolution. But that little blip is gone here. And you can also see that because this little bit of smearing got into the baseline activity here, that actually decreases the estimate of power over here. So here you can see there's stronger power than there is here. And that's really because of this little blip here getting into the estimate of the baseline. Now, exactly how to pick the baseline is a tricky issue. It's really non-trivial. And whenever I teach this to an audience of more than like three people, there's always going to be at least one person who raises their hand and says, well, in my experiment we have this blah, blah, blah, this weird situation that's unusual and annoying and what should I do? And it's always difficult to answer these kinds of questions because any time when do you select for based learning is necessarily a little bit weird because of this issue of relative interpretation. There really is no such thing as a pure, absolutely perfect baseline that time window. So you just have to try to think about this issue and pick a baseline time window that you think is going to be best for your situation. This is also an important aspect of the analyses that you should think about before you start running your experiment. So when you are designing your research project, you can already think about what time window you want to use for the baseline and build that baseline into your experiments. So then you know that you have a good baseline. For example, is useful to have entire trial intervals of around one second. You can imagine that if the the time window between the previous trial and this trial was 400 milliseconds. So let's say the last trial, the previous trial ended right here, then. Yeah, that's very difficult to come up with a clean baseline period because the brain responds to the end of the trial is going to continue already for several hundred milliseconds. So you don't really have any time window for a baseline. OK, so again, there's no simple answers to these questions, but it is important to think about another thing I would like to discuss. Another issue related to baseline normalization is how to deal with normalization of multiple conditions. So let's imagine you have three different experiment conditions in your research project. Maybe, you know, let's say let's imagine you have a research experiment where you're doing some visual processing tasks. So there's a visual stimulus that appears at times zero. And you give your your participants different levels of motivation. So here they are not very motivated to do the task. Maybe here in this green condition, you tell them that if they if they get the right answer here, they'll get a dollar for every right answer. And here you say, well, you get ten dollars for every right answer. So here they're going to be much more motivated here. They're going to be a little bit motivated. And here you don't offer them anything. So they are not terribly motivated in this condition. So then we can imagine that this is what the power time courses look like for these three different conditions. So now the question is, when coming up with a baseline for normalizing these Power Times series, do you use a condition average baseline? So you take the baseline window averaging across all these conditions, or do you come up with or set each condition to have its own baseline time window? These are actually going to give you quite different results. You might come to very different interpretations. So, for example, if you use a condition specific baseline, you would end up with plots that look like this. They would actually be totally overlapping. I'm just separating them here for visibility. And now with conditions specific baseline, you would come to the conclusion you would interpret your results that there is no difference in the activity across these. Three different conditions, so the amount of motivation you give your subjects doesn't affect the brain response in this electrode, in this frequency. Now, again, that's that's technically true. The way that we normalize the data, that is a valid interpretation. But you can also see from looking at the raw power here that there's something else going on. So it looks like this Phasuk response, you know, the change from baseline to the peak, that is the same. But there is this kind of tonic response is ongoing difference between these different conditions. So then what you could do, alternatively, is compute a condition, average baseline where you take the one single baseline that gets applied to all conditions equally, and that one single baseline comes from the average of all three of these conditions. So then you would get a plot that looks like this, which preserves the differences, and then this horizontal line here would correspond to zero relative power. So then you can also see or you would interpret from your results that there is this difference even in the baseline time period. Maybe this was you know, this was motivation was manipulated block y. So the research participants knew that all the trials in this ten you year, ten minute window of time was going to be a possibility for getting ten dollars for a correct answer. I'm making up a lot of these details as I go along just to try to contextualize that a little bit. But I think you got the idea. The idea is that there are more decisions to be made about how you based on your data, these are important decisions. They affect what the results will look like. They affect how you will interpret your results. And it's difficult for me to give you specific concrete advice other than these are important issues that you need to think about and discuss with your colleagues. OK, so but that said, this is a rather unusual situation where you have such strong tonic differences between conditions. It is much more common that the differences between conditions is in post stimulus period and not in the pre stimulus periods of pre-trial periods. So therefore, my normal recommendation would be to use condition average baselines. And I don't think you are often going to encounter situations like this. So in this video, I discussed issues of baseline normalization and how to select the baseline. I will leave you with my two recommendations, which are to use a condition average baseline. So you derive the baseline from all of your conditions combined. That's advantageous because it will increase the amount of data that's going into the baseline. So that will give you a maximally clean, stable, high signal to noise characteristics estimate of the baseline. And my second recommendation is to have a baseline that starts and stops before the trial onset. So something like minus five hundred to minus 200 is a pretty good time window to use for the baseline. And then also keep in mind that you should feel free to disregard these two recommendations when you have some kind of, you know, different or special or unusual task design that is not amenable to these recommendations. 
