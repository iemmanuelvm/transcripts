You know, I think I mentioned in an earlier video that I used to give out an assignment where I would

tell students to create the same or get the same result using complex wavelet convolution and filter

Hilbert.

And then I believe I mentioned that I stopped giving out that assignment because I didn't think it was

super interesting.

Apparently, when I said that, I forgot that it is an assignment in this problem set and that is the

goal of number four here.

So we want to do is illustrate that complex, more like wavelet convolution and Filter Hilbert can give

either the same results qualitatively the same or different results depending on the parameters.

OK, so let's start.

So we are going to compute the ERP here and then essentially we're going to do wavelike convolution

on this ERP and then filter Hilbert on the ERP and see if we can get them to match.

So let's see here.

We define the time vector for the wavelet.

And I discussed this actually in the previous video, creating this time vector like this and convolution

parameters.

And let's see that this all looks good.

And then here's the wavelets.

We have E to the I to pi F.T. So we are hard coding the frequency here to be forty two hertz and then

for log to wave.

OK, so this all looks fine and then we want to normalize.

So that is a complex wavelet spectrum divided by max of complex wavelet spectrum.

OK, so that looks good.

Let's see.

There we go.

And then here is the rest of convolution, which is fine, although actually we do need to scale this

by two here.

So let's see.

Oops.

Hmm, OK, Matrix Dimensions must agree.

Let's see what we're doing here.

So we take the inverse for a transform of the wavelet spectrum times, frequency wise times, the spectrum

of the data, not the data itself.

Let's see.

So we need the FFE of IRP and this has to be the end.

Carns point fifty.

Let's see if that works.

All right, very nice.

And then here we extract the amplitude and we can plot it, although it's going to be plotted below,

but let's plot it anyway.

Time back and see.

And w amp.

There you go.

So here's the Amplitude Time series at forty two hertz.

OK, so now what we're going to do is create an FAA air filter that is centered at forty two hertz.

So here we see the center frequency is forty two and the filter width is seven.

And there's a comment here that says this is the one sided hertz is the one sided with of this filter.

So you can see how that's going to work here.

So here are the frequency bounds and for using FIA one.

So previously when I introduced you to the Filter Hilbert method, I use the function FIA less.

And then we specified six numbers here with FIA one, the only specify two numbers, and that is the

the lower edge and the upper edge of the band pass filter.

And that I also mentioned briefly at some point and one of the videos.

So here we have the center frequency minus this variable width and then the center frequency plus this

variable width, and then we divide by Nykvist because that's the normalization factor.

And then here we have the order.

It's not really clear why I'm picking 53 here, but let's just roll with it for now.

So let's see here.

We create the filter, Colonel, and it's from these variables.

So create the filter.

Kernell It's also have a look at this thing.

So Plott Filt Kurn and OK, it looks a little bit like a wavelet, but it doesn't seem like it really

gets all the way back to zero.

So that might already be a hint.

I'm going to have a quick look at this thing in the frequency domain.

So ABS 50, Filt Kurn and oops, I want to plot this.

Oh yeah.

I see what I did wrong.

Let's see.

So plot about not about abs.

OK, now this is actually supposed to be a plateau shape, so we can already tell that this isn't such

a great filter.

So we're probably going to want to increase the order.

But this is the code as it was given to us.

I'm actually going to just leave it like this for now and let's keep going and see how the end result

looks.

OK, so then we apply the filter kernel to the ERP.

And, uh, we got an error invalidated to our arguments must be double made, so double precision essentially

is the problem.

So you can see this is single precision.

Now, I've mentioned this earlier in the course that for many Matlab functions, particularly signal

processing functions, you might get more accurate results or sometimes it's only possible to get results

if the data are double precision.

Now, I could solve this problem on the fly by doing this.

However, this would mean that the ERP is single precision for the wavelike convolution and double precision

for the narrowband filtering.

So I think it's better, you know, to make sure that we have a really fair comparison.

I think it's good to make this double precision right at the very beginning.

And that means I'm going to rerun.

Let's see.

I need to rerun just these lines of code here.

OK, and then let's see, so then OK, so now we don't need this anymore, although it doesn't hurt

to keep it in, so then I can run this again and this will take the Hilbert transform and give us the

magnitude.

All right.

Now for some plotting, let's see how it looks.

I'm super curious and oh, it looks really disappointing.

So the blue line we've already seen before and the red line is the filter Helbert.

So the question is, do these match?

Is this a good match?

And the answer is no.

I mean you can kind of see that they want to be similar.

They're kind of going up and down together.

But clearly this is not a very good match.

So let's let's think about what are the parameters that we can change?

Well, one thing is the order.

We already discovered that this is not a very good order.

This is too low.

Let's try something else.

I'm going to go up to 300 and leaving all the other parameters the same.

I'm just going to increase the order and let's already see what that does.

Hmm.

OK, so it's looking a bit better.

At least it got smoother.

And what we can do also is plot the spectrum of the filter, Colonel.

And now you see this looks a lot better.

This looks more like a plateau shape.

So this is what you would expect for a narrowband filter.

Maybe you could increase the order a little bit further if you wanted to make this a bit more of a plateau.

I don't think this is such a bad filter, though.

So let's think about some of these other parameters that we might consider changing.

Well, we don't want to change the filter, the center frequency, of course, because that has to be

the same for the wavelet and the the FIA filter.

But how about this with let's think about this with for a moment in particular, let's think about trying

to match the width between the wavelet and the filter kernel.

So the wavelet now, we don't know what the wavelet width is in terms of frequencies.

We do have it specified here in terms of seconds.

But let's just make some plots on the fly.

So I'm going to plot the spectrum of the wavelet.

So we need to specify frequencies, vectors, that zero tax rate in the NT.

So the number the frequency resolution has to correspond to the number of points in the FFE, which

is Anquan.

So Incans.

And then I'm going to plot ABSs complex the way that we X and let's see.

So that's how that thing looks like.

And now I'm going to plot the spectrum of the filter, Colonel.

So let's see.

Hold on and then plot line space.

So this is still zero to rate, but now we have the length of the filter kernel, which is very different

from ensconce.

So the length of Filt Kurn and then we want the ABS 50 filters kearn.

OK, and let's make this a red line.

All right.

So this looks pretty interesting and also pretty insightful.

This immediately gives us a strategy for trying to make the result of wavelike convolution and filter

Hilbert more similar to each other in particular.

Let's see the wavelet full with that half maximum is I'm not going to compute this exactly, but it's

approximately approximately forty four down to thirty.

So it's about five.

There's let's let's just call that five hertz here full with RAF maximum, but the full with F maximum

here at this filter is more than 14 hertz.

So what I'm going to do is change this to let's say three.

I'll set this to three.

We'll see how that looks.

And then the goal is going to be to get these two time series to look closer to each other.

Let's see.

And yeah, OK, so run all this.

Again, and now who that looks really good, they look really close.

Obviously, they are not identical.

Let's see, I'm going to plot the spectrum here again.

So let's do plot.

And now let's make this was.

Yes, this one.

I'm going to make this a black line.

And now you see that the filter, the fire filter is much closer in the frequency domain to the Gaussian.

Now, it's still not exactly the same.

And in fact, based on, you know, this doesn't even get up to a gain of one here.

So I think we could probably work on this a little more.

The order probably needs to be higher.

Let's try recreating this kernel and then plotting again.

I will plot this.

How about M for magenta?

So this looks a little bit better.

Now you can see the fire filter is a bit wider than the Gaussian, but that's probably OK.

And oops, uh.

I see now we get actually a separate issue that the length of the data is too long.

So essentially what Matlab is doing is trying to figure out what could be the edge effect for filtering

with this kernel that 700 points long and the signal, which is 1500 points long.

So Matlab is basically estimating that the edge effect is going to be sufficiently large, sufficiently

wide that we're not really going to be able to interpret enough of this signal.

So then it's saying that the data length must be larger than twenty one hundred samples, which it isn't.

So there are ways you can get around this.

You can do a procedure called reflexion where you can flip the ERP this signal backwards and put a copy

of itself before and then another copy afterwards that will get you up to like 40, 600 points or so.

So I'm not going to get into all that.

I am instead going to leave this exercise here.

And the important point was that by tuning the parameters, we could get the result of complex Marlay

Wave convolution and filter Hilbert to be either really, really similar to each other, like what you

see here, or really, really dissimilar to each other, like what we saw at first, depending on the

parameters that you select.

All right.

And with that, so that was basically the to do thing here.

So with that, I'm now going to move on to the next exercise, exercise number five.

So wavelet convolution for all channels and visualize with the little tool called T.F. Viewer X.

OK, so we are going to do time frequency decomposition, power extraction for all channels and we're

going to visualize the results using this little tool, this Matlab gui called Taufua X, make sure

to temperately down sample the results after convolution.

This is going to be with the EEG data set.

Here are the time points that we are going to save after convolution.

So we're saving every twenty five milliseconds.

All right.

And then here's the oops.

Uh.

Hmm.

Interesting.

You already know how to decode this error and fix this error message.

Whenever you see something about Collum dimension and you're using these search and you always need

to make sure that you were using Cullom vectors here.

So let's see, rerun these guys and this looks good.

OK, so now we do time frequency analysis from two hertz to 40 hertz with thirty three steps in between

and then scanning through this stuff looks pretty accurate, I think.

Uh oops.

Right.

I just need to run that line and now let's see number of cycles.

So we're going from uh actually this is the number of cycles, this is the full width at half maximum

parameters.

So and that's ranging from half a second to two hundred milliseconds.

So the window gets smaller in time, the Gaussian gets narrower in time as we increase in frequency.

And that's giving us the trade off between temporal precision and spectral precision.

OK, now notice also this T.F. variable, this matrix that we are going to do, put the results into

it is channels by frequencies by time points, but it's the reduced time points.

It's the reduced dimensionality of the time points.

All right.

So here we have the loop over frequencies and then we create the wavelet.

So that's minus four.

And if this looks good and then we take the spectrum of the way, this also looks accurate.

And then we loop over channels, extract the Fourier transform or computer Fourier transform of the

data from this channel for all trials and all time points, and so reshape that into a vector.

So that's a super trial.

And let's see.

So this.

Also, this looks a little strange here and now I need to run through some of this, so let's set this

to be one I'm starting to get a little bit suspicious of.

Whoever wrote this code is playing tricks on us.

OK, I know that you know that I wrote this code.

So let's let's just see what's going on here.

So Data X is six four, three, eight four elements long and that corresponds to Anquan.

And now what we're doing here, it looks like we are trying to access Data X as if it were a matrix

of channels by frequencies, but that is actually not the case.

So, in fact, if I run this, it's it's going to give me well, OK, when this is one, then it will

still give me the correct answer.

But as this increases over channels, when you know channel is two, then that's no longer going to

work.

So for now, I'm actually going to delete this, but that's going to.

That gives us a hint for what we can do in the future.

That's related to one of the questions at the bottom of this code.

OK, let's see.

So data spectrum times, the wavelet spectrum, inverse F 50 and then this.

So here we are clipping the wings off of the analytic signal, the result of complex wavelet convolution

reshaping back to time by trials.

But this is funny that we're doing this division by and which actually we don't need.

So this division by the number of time points is only necessary when you're going into the frequency

domain and staying in the frequency domain.

You don't need it here because the division by points is then the forward for you transform it cancelled

by the inverse Fourier transform.

Now we could delete this and we could delete this here as well.

However, we are going to be using baseline normalized power.

So in fact, this doesn't even matter.

We can do whatever we want here.

You can multiply it by some ridiculous, arbitrary number.

And it doesn't matter because this all of these scaling factors here are multiplying all of the data,

including the baseline time window.

So they actually don't do anything after the decimal normalization.

OK, so let's see.

So we get the power time series and that is so we extract the magnitude and then square to get power.

And then this is CUMA one, which is actually averaging over the wrong dimension.

That's going to be averaging over time points.

See, the first dimension is time, and we went to average over trials, so really it should be like

this.

OK, and then maybe, you know, maybe it's a good idea to plot this Power Times series, I'm going

to type e.g. that times Palate's just to make sure we get something remotely plausible.

And I think this is plausible.

This is the lowest frequency.

So this is Frex F.I..

This is too hard.

There isn't a whole lot interesting happening in this particular data set at two hertz here you see

the edge effect up here, the edge of fact over here.

And if you like, you could zoom in here.

It'll be interesting to look at these dynamics a bit more.

So you see there is some post stimulus increase that around two thirds.

But otherwise, you know, it sort of pales in comparison to the edge effects here.

All right.

So baseline normalized power time series only from the downsampled time point.

So let's see what we want to do here.

Now, we could do payout's.

And actually what I'm going to do is first without downsampling get the DB normalized scores, the TIME

series.

So this is going to be ten times log ten of Protégé's Divided by the baseline, which is the average

power time series from the baseline time window.

And what should we call that variable?

We call that variable base IDEX.

OK.

So let's see.

So from base IDEX one to base IDEX two, and so this does not yet give us the right answer.

And we can also see that by trying to run this line of code and you will see that the left side of the

equation is 61 points and the right side of the equation is six hundred and forty points.

So what we need here is only to access the I think I call this variable t idex.

Yeah, exactly.

And now I believe this line of code will run.

OK, so let's run all of this here.

So we're looping over all the frequencies, looping over all the channels, doing all of the convolution,

extracting time, frequency, power and so on.

So let's check out the size of this T.F. variable, its channels by frequencies by time.

So now we are going to use the Taufua X function.

If you haven't used this viewer, then it's good to type help and then the viewer and just to see what

this thing is.

So let's see how we can call up this viewer.

So we want according to the help file, we want at input time, which we do, and then frequency which

we do.

Then the data and the data is a channels by frequency by time matrix and channels by frequency by time.

So that part's good and then channel locations in the format and then an optional title for the figure.

All right.

So let's have a look at this.

Let's see what this looks like.

All right, so here we get the ETF, Eurex, and what's cool about this little nice little tool is that

you can click around on the time frequency plane and then you see the topographical map for each of

these time frequency points, and then you can click around on the topographical map and then you get

a different time frequency plot here corresponding to the spectral dynamics at that selected electrode.

So this is important.

This is a nice viewer because normally, you know, it's hard to look through this three dimensional

cube of data.

You have time and frequency and space, and it's just difficult to to look all the way through it.

So here you can kind of click around in space, you can click around in time and you can click around

in frequency and you can't see the entire 3D matrix all at once.

We would need a four dimensional viewer for that.

But at least you get a lot of these different three dimensional, you know, combinations of two dimensional

slices through this three dimensional cube.

And then there's a bunch of other optional parameters that you can play around with here, which I'm

not going to pay too much attention to.

OK, question, is the double for loop really the smartest way to set this up and why?

So let's see.

So the double for Loop is referring to this thing here where we are looping over frequencies and then

looping over channels.

And what I would like to do is just spend a moment.

So obviously this this worked.

We ran.

It is fine.

There's really nothing wrong with this code.

But we I just want you to think about it and I will talk about whether it's possible to make this code

more efficient.

And now, on the one hand, this is a course about signal processing and data analysis is not, of course,

about code optimization or computer science.

But that said, it's always a good idea to try and think about your code and whether you can make it

better.

Better code is less likely to have errors.

It's easier to use and it's going to be faster to run, which means more time looking at results and

less time waiting for your computer to run through all of the numbers.

OK, so this part is optimized.

So we are computing the wavelet per frequency only once per trial.

So that would be you know, you can contrast that with something like this.

If the loop look like this, where we are looping over frequency and then looping over channels and

then creating the wavelets here, this would mean we are creating each wavelet 64 times for sixty four

channels and sixty three of those times is totally redundant.

So that would be an example of inefficient code, which is not a problem here.

I think one thing we could do remember there was you know, this was previously called as a matrix.

So I think one thing we could do to make this code more efficient would be to run the fee only once

so we can run the fee by running through all the channels at the same time.

So here this would be we reshape to channels and beachum channels by time points, by trials.

It's all going to get in here.

And then we just need to make sure that we are computing the 50 over the second dimension, which is

time trials.

And then you see we have a 64 by time trials matrix and now we have saved ourselves a little bit of

time because we don't need to recompute or call the FFE function again, which has a little bit of overhead.

OK, so that's one thing we can do and I guess that's probably it.

I've mentioned before, if you have a small number of channels and or a small data set, then you don't

even need this for loop at all.

You can do all of this stuff through vectors and matrices.

But that said, in practice, I generally always actually have a loop over channels.

OK, so that's this question.

Here's the next question.

What if you had multiple conditions?

How would you modify the code?

Well, that depends a bit on how exactly you have your code set up, but I will tell you the way that

I like to set up my code with multiple conditions so that it's maximally efficient.

So what I like to do is have all of the trials in the same structure.

So, for example, we have this EEG data structure and now this is 99 trials.

So this is you know, in fact, this is all one condition.

This whole data set just comes from one condition.

But imagine that we had three conditions in this data set.

What I would try to do if this were, you know, if I were actually doing production level data analysis

here, I would try to have all three different conditions in this same matrix.

So maybe each condition has three trials and then essentially all you need to change is here when you're

averaging.

So this would be something like a US and then we want all of the time points.

And then it would be something like, you know, condition label equals one.

So and then this could be Palate's one.

So now notice, what I'm doing here is I've run the convolution for all of the trials, including all

three conditions.

And here I'm only extracting the trials when I'm doing the averaging for the Power Time series.

I'm only averaging together not all the trials, just the trials where this variable condition label

equals one.

And then, you know, this would have four, three conditions.

It might look something like this condition one, condition two and condition three.

And then, of course, you would need to have three of these lines for three separate things.

And then this could be, you know, like this that you could put have an extra dimension at the end

of this T.F. matrix that would be for the experiment condition.

Now, this is not the only way to adapt this kind of code for multiple conditions.

But I just want to give you one idea, a bit of inspiration about the way you could do it.

OK, so that was for that exercise for number five.

And now let's go on to number six.

So here we are going to compare wavelet convolution and the mean over time with static FFE.

So this is going to be pretty interesting.

We are going to implement wavelike, perform wavelike convolution to give us a time, frequency analysis

of time, frequency result.

And then we are going to do a static FFE and basically see how the average over time of the time frequency

result compares to the static FFT.

All right.

So what we want to do here is adjust the code from the previous exercise to save these static spectrum

without baseline normalization, then implement a static FFE.

I think this is worded slightly awkwardly.

I think what we want to do is go back to this code here and save the average power time series.

So I mean, undo all of this business here and see, undo, undo, undo.

OK, so here we have the decimal normalized and downsampled version of the signal.

What I think we want to do is say, uh, let's call this variable like T.F. Raw and for this channel

and for this frequency, we want to save the average power time series.

And then the idea is that here we extract the power time series over Dynamic's over all the time points,

and then here we want to average over all of the time points and then we can compare this against the

static FFT.

OK, so this is like getting this static version of the dynamic FFT and now actually I see why we multiply

by two and divide by the number of points.

I think that's not just about confusing you so that for the the decimal normalization, I think it's

also about this.

So we can compare these values directly to the, uh, the values we'll get from the FFT in the next

section.

OK, let's see.

So we need to rerun this code.

And let's see now we're getting a little underline here.

Matlab is giving us a warning and it says that this variable is changing in size on each iteration inside

the loop.

Oh, I must have changed something else in here somewhere else.

Oh, right.

It was all this FFE thing.

Let's see.

Oh this is good.

I can leave it like this.

So leave data there and then I want to uh in this loop here.

So now here we need to extract only the channe I come all.

OK, so let's see.

And there we go, so normally it actually is a good idea to initialize variables, which is what Matlab

is warning us about in this particular case, I'm not going to initialize this because we're just running

it this once.

So we get a 64 by 33 matrix, which corresponds to channels by frequencies.

OK, so let's see now.

We are going to focus on one channel.

So Channel thirty one and then we want to do is get the power from the FFT.

So we want let's do this one piece at a time.

So we want the data and I think I'll I'll squeeze this.

So squeeze that data from channel to plot and then all the time points and all trials.

And then what we want to do is get the F.T. from here and this is going to be a default endpoint at

50.

And actually it's uh well it's good to specify this anyway.

So we want the 50 over the first dimension.

It's a little tricky because I'm squeezing it in the original EEG.

The data matrix time is the second dimension, but once I only index one channel and then squeeze it,

time is now the first dimension.

OK, so that's the FFE, and then we want power so square and, uh, then we need to, let's see, um,

multiply by two.

And then we square.

OK, so this should be a frequency's by trials matrix, which it is so frequency's by trials and then

we average over the second dimension here we create the vector of frequencies and here we make a plot.

So we have the tephra.

So the raw average power from the time frequency analysis and then FFE power and let's see whether these

are even remotely similar to each other.

And they are so far off in scale that it's not really possible to compare them.

Let's see.

Let's look at the legend.

So here we have OK, so this is looking better now.

At least we see something here.

So we have the static left and here is the average time frequency power.

Now, these do not look like they correspond.

So let's see if we can fix this here.

So one thing is that I didn't divide the output of the FFE by the number of time points.

So let's do that.

So I'm going to write in here divide by eight points and let's see if that helps at all.

And so this did get a lot smaller, but you know, what I want to do first is just scale this up a bit

just to get a sense qualitatively if these are even remotely similar.

So let's try multiplying by how about five hundred and let's see if that gives us a sensible result,

or at least we can see the two things on the scene.

OK, five thousand maybe.

How about, uh, fifteen thousand?

You can see that scaling factors in signal processing is always a tricky issue.

OK, so these are actually converging.

And this is looking good, I would say these look these to Specter look overall pretty similar.

And this is also we could play around with this a little more, but I think I'll stop it here.

This is also a bit similar to what you saw when I discussed the comparing the result of Welch's method

with the result of a single static fee.

And you'll remember that the static fee and Welches method produced overall similar results, but the

Welches method result was much smoother.

And here this is actually pretty.

So the average time frequency power is pretty equivalent to Welch's method.

So this you see here, essentially the time frequency analysis is like averaging over.

It's smoothing over what we get from this static FFE analysis.

And to some extent that is because the static fee is more sensitive to non stationary in the signal,

whereas when we do the time frequency analysis, we can isolate those non stationary and so they have

less impact on the signal.

That is the overall average signal.

The little non stationary is get averaged out much better in the time frequency power compared to one

static fee.

OK, so I think I will call it quits here.

In the interest of time, of course, I encourage you to continue exploring around with this code and

working through these exercises and improving your knowledge.

Otherwise, I hope you enjoyed this section of the course and I look forward to seeing you in the next

section.