In this video, I'm going to talk to you a little bit about some preprocessing steps that you might

possibly apply to your data.

I'm going to give you a list of possible steps of preprocessing data, and I will give you multiple

reminders that data preprocessing is often highly specific to the particular data set.

You have the particular kind of research project you're doing and the lab that you are working with.

For this reason, I often issue discussing preprocessing and data cleaning because it's hard to give

a very specific set of advice that is generally or universally appropriate.

So let me start by explaining the difference between preprocessing data and processing data processing

data you can think of as analyzing the data.

That's basically what this whole course is about.

Processing the data.

Preprocessing is everything that you have to do to the data in order to get to the point where you can

start to actually process or analyze the data.

So let me first say a few general things about preprocessing versus processing.

Preprocessing data tends to be time consuming and tedious.

It might take you for one data set.

It might take half a day.

You know, it might take you six hours to process a data set or maybe you have a larger or long data

set and maybe it takes a full day just to pre-process all the data before you even get to any analyzing.

So preprocessing tends to be consuming.

It tends to be tedious.

You know, it requires a lot of attention.

It requires some dedication and meticulousness, and it's not necessarily fun.

I enjoy analyzing data.

I enjoy processing data.

Preprocessing data is not something that I personally find to be enjoyable.

In my opinion, preprocessing data is also not really science.

It's signal processing.

It's just some stuff you have to do with your data to get to the point where you can start analyzing.

So for all of these reasons, my motto, my recommendation is when it comes to preprocessing data,

do it well, take your time, do it meticulously, don't rush.

And then you only need to do it once and then you're done.

You never need to pre-process the same data again.

Now, in practice, particularly if you are just getting started with electrophysiology data analysis

or any kind of time series analysis, it does happen in the beginning that sometimes, you know, you

don't really have the preprocessing pipeline worked out.

Exactly.

So it does sometimes happen that you need to read pre-process data multiple times, but hopefully you

will only have to do this once.

So take your time with preprocessing, be meticulous, be methodical, and then you don't ever have

to do it again.

Now, processing data or analyzing the data is it can be hypothesis driven, it can be exploratory.

I consider this to be the meat of science to me, analyzing data, collecting data from the universe,

the brain or anything else in the universe that you're studying, analyzing those data, making sense

of those data, that is the most important aspect of science, in my opinion.

Of course, other people have different perspectives on science.

Some people would say that building theories is the most important aspect of science.

I would say the most important aspect of science is analyzing data for me.

Therefore, as you might imagine, this is lots of fun.

I really, really enjoy analyzing data.

In fact, if I go too long without analyzing data like I break out in sweat and my hands start to shake

and I get nervous and anxious and I get stomach stomachache.

So analyzing data is a lot of fun and it's important to me.

I think that's where all the science is done.

And this stuff happens often multiple times, in part because you're doing a lot of different analyses,

or sometimes you might want to reanalyze existing data and so on.

So in my mind, this is the distinction between pre processing and processing data.

All right.

So with all that said, what I'm going to do now is walk you through briefly this list of possible preprocessing

steps.

And I want to highlight this again.

These are not all of the steps that you necessarily have to follow in this order.

It really depends on the kind of data that you have, what you are looking for in the data and so on.

So you should take this list as more like an inspiration for you to start from.

And then this needs to be this likely needs to be modified to your specific data.

So the first step, of course, is to import the data into Matlab, as I've mentioned before, this

entire course is focused in Matlab, and that's because I would say over 95 percent of the field of

neuroscience electrophysiology time series analysis uses matlab.

So obviously, you know, you can exchange this for Python or Juliar or C++ or, you know, HTML or

whatever environment you're using to analyze the data.

All right.

Atypical starting point is to apply a high pass filter with some relatively low cutoff.

Maybe it's point five hertz, maybe it's zero point one hertz, maybe it's one hertz, although getting

up to one Hertz might be a little bit high.

But there are often DC offsets or drifts, low drifts in electrophysiological signals, particularly

with EEG, where slow changes in sweating potentials, for example, can lead to very low frequency

activity.

And these often present as artifacts in subsequent analyses.

And then another step would be to import the channel locations.

This is important for doing topographical mapping.

If you're not familiar with topographical mapping, then don't worry.

I'm going to talk more about that in a few videos from now.

If you are measuring external channels so not only from the head, but also EOG, the electro kilogramme,

this would be electrodes placed above and below the eye or to the left and to the right of the eye or

EKG sometimes called ACG.

This would be measuring heart activity and EMG, which is the electromyography.

So measuring muscle activity or maybe you are also measuring like a force device that a subject is using.

So basically any non EEG, non brain related channels that you are also measuring might need to be re

referenced the next step.

So all this is done on continuous data, so two dimensional data.

The next step could be to epic the data around the important events.

So what are important events?

Well, this could be the trials, the repetitions of the stimulus.

Let's say it's a visual stimulus or an auditory stimulus.

And essentially the idea is that you are cutting out all of the data that you're really not interested

in and focusing on the data that you are interested in analyzing.

So up to this point, the data is going to be a two dimensional structure.

And at this point, the data become a three dimensional structure where the three dimensions are time,

channels and epochs or trials.

Once you have all of these trials cut into epochs, then it's generally a good idea to subtract a pre

stimulus baseline.

So, for example, you might compute the average voltage potential from each individual electrode,

from each channel from, let's say, minus 200 to zero milliseconds, and then subtract that average

from the rest of the time course.

So this is actually similar to the effect of a high pass filter, which basically just ensures that

all of the trials are in the same scale and then you might need to adjust the marker values.

So these would be the particular values in the data set coming from ttle pulses that indicate when specific

events happen and by adjusting the marker values.

You know, let's say you want to categorize each trial according to what the subject is, what the research

participant responded in the next trial.

So you might need to adjust the marker values.

Again, this is something that's highly, highly study specific.

After that, you want to go to trial rejection and basically look through all the trials in your data

and indicate which trials are artifacts, which trials have bad data that should definitely be excluded

from analyses because of artifacts or very unusual things happening in the data.

Now, people also differ in the field about how trial rejection should be done.

Some people argue that trial reduction should be done based on some algorithm where you are totally

blind.

So you're not looking at the data and you just apply some algorithm and reject the trials that the algorithm

says should be or that the algorithm says are unusual.

Now, I appreciate the motivation behind this argument, but on the other hand, in my experience,

I have tried many different algorithms for automatic trial rejection and they tend not to work very

well.

They tend to produce both Type one and Type two errors, which.

Means that the algorithm will reject data that I think are perfectly fine and they will also fail to

reject data that I think should be rejected.

So I am a big proponent of manual visual trial rejection.

This is also useful because I think it's really important to look at your data.

I think visualization of data is really important.

So this forces you to look at your data.

OK, and then you can mark the bad electrodes and then you have to make a decision about what to do

with particularly noisy or broken electrodes, depending on the kinds of analyses you're doing, how

you're going to be aggregating data across different individuals in the study.

Maybe you want to interpolate the bad electrodes or maybe you just remove them altogether from the data

set.

Now, EEG data collected online are typically reference to some electrodes on the scalp.

Sometimes it's actually a head electrode.

So it might be electrodes seized on the very top of the head.

Sometimes it's a reference electrode that somewhere else, maybe on one side of the head, like behind

the left ear, for example.

So then you might want to reference all of the EEG channels, for example, to average reference.

There are multiple referencing schemes and many people argue that average reference is the best, but

all of them are all referencing schemes are a little bit arbitrary at some to some extent.

The most important thing to keep in mind about referencing is that it's good to avoid using a single

reference electrode that is on one side of the head that can introduce a bias.

It's also good to make sure that your reference electrode is as clean as possible because if there are

artifacts in your reference electrode, then once you reference the data, all of the so the artifacts

from the reference electrode will leak into all the other EEG channels.

This is why it's good to do referencing only after you've done these steps, in particular marking the

bad electrodes.

So let's imagine you have one electrode out of one hundred.

Let's say you're measuring 100 electrodes from the head.

One electrode is broken.

So it has super high amplitude noise, pure noise.

It's not measuring anything biological, but it's it has activity that is really, really large amplitude.

If you leave that electrode in and then compute the average reference, then the artefact from that

bad electrode is actually going to be present in every other electrode.

So you're actually contaminating all of your data because you included this bad electrode in the average

reference.

OK, so then once you compute average reference, typically the final stage of pre processing and data

cleaning is to run independent components analysis to clean the data.

So you would run ICRA and then look through your components and identify hopefully a very small number

of components that you want to project out of the data.

And I'll close this video by reminding you that preprocessing is time consuming and tedious.

Make sure you do it well and then you only have to do it once because this whole thing might take you

anywhere from hours to a day or maybe even more than one day.

So you don't want to have to redo all of this boring stuff every time you want to reanalyze your data.