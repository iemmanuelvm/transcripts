In the problem sets for some of the previous sections, I promised at the very beginning of the first

problems that exactly how many problems I will get through in that video.

And I've always been wrong.

So I have learned my lesson.

I've learned from my mistakes.

I am not going to tell you how many of these exercises I'm going to get through in this video.

There are five in total.

I'm just going to keep talking.

I'm just going to keep going until the clock starts to get pretty high and then I will stop and switch

to the next video.

OK, so needless to say, I hope you take the opportunity to go through the code yourself and try to

get through as much of this problem on your own before watching me talk my way through it.

OK, so here is problem number one.

Exercise one, we are going to compete and all to all connectivity matrix using the V one data set.

So and then we want to do this for two frequencies, eight hertz and fifty five hertz.

All right.

So let us import the data here and now we specify these useful variables for later and is all the size

of one.

So that's kind of a funny result.

The name of the variable we can type who's as a reminder, we are working with the variable that is

called C SD and it is channels by time, points by trial.

So that's why we have these outputs in this order here.

Now, of course, if you didn't know or you forgot the name of this variable, you could clear the workspace

load in the matte file and then type who's and then see what's inside of that, this matte file here.

OK, so let's run this now.

We can specify the frequencies.

So we're going to have two frequencies, eight hertz and fifty five hertz, not a whole range of frequencies

from eight to fifty five, but only these two frequencies with different cycles.

So we'll need to create two wavelets to extract activity at eight hertz with seven cycles and fifty

five hertz with fourteen cycles.

OK, and then we create the wave time.

That's the time vector for the wavelet.

And here is the question.

Why do we remove a sample point here at the end?

And the answer, which I've discussed at some point in some previous video, is that it is useful to

have a way, a convolution kernel that has an odd number of points that makes it easy to find an exact

center of the wavelet, and that also makes it easy to cut off half the length of the wavelet in the

beginning and half the length the wavelet from the end, from the result of wavelet convolution.

And so because we have a kind of bizarre sampling rate, if you just so if you don't subtract a sample

point, you end up with an even number of time points, which is fine.

It's not a big deal.

It just means you have to change the code a little bit to get it to still work and be consistent.

OK, so that's the answer to this question.

So then we have the fifty parameters.

These are really the convolution fifty parameters, so N plus M minus one.

And this all looks good.

So I think this is all correct.

OK, now we create the wavelets, so there's two of them.

And so this loop is just going to go twice here.

We have E to the I to pay forty times the Gaussian E to the minus T squared over two S squared and here

we need to define S.

So that was the number of cycles and that is in cycles.

That was a variable name.

So ln cycles and then we want the F element.

Now this is not accurate.

This is not correct.

Remember that this s parameter, which is sigma in the formula I showed a while ago, that has to be

divided by a normalization factor.

And that normalization factor is two pi f where F would be this frequency.

So there you go.

That gives us this thing and then let's see.

OK, so then we get the Fourier transform of the more they wavelet with Nikonov points here we are normalizing

this and you know that we actually don't need that normalization factor because we are only interested

in the phase values here.

Nonetheless, it's fine to keep that in OK run convolution to extract phase values and store the phase

angle time series in a channel by frequency, by time, by trials.

Matrix nice.

This is going to be a big multidimensional matrix.

So here we initialize it so zeros and then we want channels which is an chans and then we want frequency

which is two time, which is ln points and trials which is in trials.

All right.

So let's run that line of code.

Oops.

That's zeros not zero.

Now we compute the spectrum of all the channels using the FFE Matrix input and this says check.

So I'm already highly suspicious of what is going on in this line of code.

So let's start with the innermost part of the code and we will work our way outwards.

So we are reshaping the three dimensional seized data, two channels by time, by trial.

So actually this part looks correct and then we take the FFE and that also looks correct.

However, by checking the matrix sizes, we can see or maybe you've already noticed from looking at

this line of code that this is actually the wrong FFE size.

So this is three hundred and five thousand four hundred, whereas the more they wavelets are three thousand

six hundred nine hundred and twenty four.

So what we really need to do is take the NT con point convolution.

And this is also actually there was another mistake here, which is that we computed the Fourier transform

over the channels and actually this has to be over the second dimension.

All right.

Much better now we loop over frequencies.

And what do we do here?

Well, we have to run convolution.

So that is the inverse Fourier transform of the data spectrum times, the wavelet spectrum for this

particular wavelet and then income.

So that's fine.

Although as I've mentioned before, that's also the default because this is already the right length.

And then, too, because we are taking the inverse for you, transform over the second dimension, which

is time.

OK, and then this is also a a mistake in one of the code files previously in this section, you might

remember it looked like this and we ended up with a really long vector, which is the wrong answer.

We want to extract the clip, the wings of convolution equally for all channels.

So there you go.

And then you can see this is the right size.

Three hundred and five thousand by four hundred.

So now we extract the angles.

But I think this is not still not quite right.

We need to transform this back into a three dimensional matrix.

So alright as equals reshape a spy.

So it goes back to the size of C SD.

So let's see, we didn't get an error.

That's a good sign.

And now we have channels by time, by trials.

OK, very good.

And now we are storing all the phase values from basically everything for this frequency.

OK, pretty cool.

Let's see.

I guess I didn't need to rerun this again.

All right.

So that part is good.

Now we want to compute connectivity.

So compute connectivity separately for two time windows.

We get one time window from the first stimulus window and from the second stimulus window.

So point so one hundred milliseconds to 400 milliseconds and then six hundred milliseconds to nine hundred

milliseconds.

And remember, there's always the question of whether we should compute phase synchronization over trials

or over time.

And you might remember that one.

Was it a few videos ago that we looked at it both ways.

That was the hub ness video in that for the Matlab code.

We looked at it both ways.

First computing synchronization over time and then averaging over trials and then computing synchronization

over trials and averaging over time.

And there we found that the results were really, really similar between the two methods.

So I'm going to leave it to you to test this in this case both ways.

But I for now, I'm just going to follow the instructions of first compute synchronization over trials

and then average the synchronization within those time windows.

OK, so here we define our time windows.

Here is Kamat.

So this is connectivity matrix.

So it is going to be channels by channels, by frequency, by time windows.

All right.

So let's initialize that.

And then here we are looping over channels.

Now, notice in a previous video, so also the video about happiness I computed or I set up the second

this inner for loop a little bit differently, I actually wrote Channe I plus plus one.

And then down here I just copied the results to change comma and I so that's also fine and it actually

doesn't really matter.

Well if you have a huge number of channels that can make a difference, that can, you know, you're

doing a bunch of computations in here that can cost you some time.

In this case, we are in fact re re doing several computations redundantly, but that's fine because

we only have sixteen channels.

So this is still going to go quite fast.

All right.

So let's see.

We want to compute the eulogised oilrigs phase angle differences.

So that is E to the I times and then there's going to be the phase angle difference and then the end

of the exponential.

So this needs to be that variable is called all phases.

And let me start by writing down the size of all phases.

So let's see.

This is going to be all phases from Chennai and frequency.

Uh.

Well, OK, so we don't have a loop over frequency.

So this is going to be all or both frequencies.

And then I'm going to set this up with all time points and all trials minus the same thing for Chan

J.

So let's see, let's run this and.

Oops.

OK, well, that wasn't so bad.

So that turned out to be one and change to be about three and then let's try running us again and OK,

cool.

So we get so notice this is a four dimensional matrix.

But if we look at this, you can see that it is actually it's really three dimensional.

And then there's this one singleton dimension.

Now we can ignore this or we can squeeze it out.

It doesn't really matter.

I'm going to squeeze it out.

So I'm going to write, squeeze and then run this line again.

And now we have a three dimensional matrix.

So that's frequency's by time, by trials.

So now we want to compute phase clustering so expensive for all time points over trials.

So what I am going to do now is say ABSs mean phase diffs and then we want to average over the third

dimension just like this.

So let's see what that gives us.

So now we have a matrix that is size two by 1500.

So this is actually the two frequencies and then all of the time points.

So in fact, it's interesting to plot this.

And I do this partly because I'm curious and also partly because I want to continually instil in you

the idea that it's a that it's great to plot to visualize data as you're going through and writing code.

You should always do this kind of stuff.

Anything that is possible to visualize, you should visualize.

OK, so this is interesting.

Here we have time.

This is the strength of synchronization and I believe the blue line corresponds to eight hertz.

I'm going to make a legend.

So I know that eight was the first one.

Fifty five was the second frequency.

And we do indeed see, that is pretty interesting.

So first of all, there's this huge baseline difference.

So before the stimulus, there's eight hertz.

Oscillation is really synchronous between these two channels, whereas fifty five hertz synchronization

is really low and then the fifty five hertz synchronization.

So these gamma synchronization seems to be more strongly tasc modulated and the eight hertz synchronization

of anything seems to be tasks like negatively Tarsa modulated.

It looks like it's going down after the stimulus onset.

OK, very good.

Now what we can do is average the data from the two time windows.

So notice that what we are indexing here in this connectivity matrix is Channel I, channel J and then

come.

All this corresponds to the two frequencies.

You remember the third dimension is frequencies and then we have to hear for the time windows.

So what we want is, is PC from.

Both frequencies, and then we went to IDEX one, I think this was called, yeah, so IDEX one one to

Tiddles to start IDEX one, and then the second element to it gets a little tricky.

OK, so then there's this.

So this is for both frequencies.

The first time window, the early time window.

So then we average and then we have to make sure that we are averaging over the correct dimension.

Because if I just I mean like this, in fact, I mean, get a long vector because it's now averaging

over the frequencies averaging eight hertz and fifty five hertz together, which is not what we want.

We want to get to values corresponding to the average synchronization from these two frequencies.

OK, very nice.

And then we can copy and paste and the only thing that needs to change is to.

So we need this to turn into two.

Now this is a really common mistake here that you copy and paste and you change some of the variables,

but not all of the variables.

It's a terrible mistake to make.

It can have disastrous effects, but it is unfortunately very easy to do so.

One thing that I do to minimize the chance or to check whether I made this mistake is to take advantage

of Matlab code highlighting capabilities.

So when you put the put the cursor on top of a variable, Matlab will automatically highlight all the

other appearances of that variable.

So I can use that to my advantage here by clicking on Tiddles one.

And then I see that this one is lit up, which is not supposed to happen.

It's supposed to look like this.

So these are their own variables and these are their own variables here.

All right.

Pretty cool.

So that is that and it runs and it doesn't take too long.

I guess it's still computing.

OK, I guess this actually is taking slightly longer than I anticipated, so maybe it would have been

a good idea.

And I will leave this up to you as a small code challenge to modify this code so that you're not re

computing this stuff.

Redundantly for for for the both the upper and lower of the triangles of the connectivity matrix.

OK, so now what we are going to do is look at all these things, let's look at these connectivity matrices,

and I'm just going to run all of this code and keep my fingers crossed.

OK, cool.

It all works.

So what do we see here?

This is channels and channels.

So these are all channel by channel connectivity matrices.

The top row is four eight hertz, the bottom row is four fifty five hertz.

And then the first column is for the first time period.

So a hundred milliseconds to 400 milliseconds.

This is the second stimulus period.

So six hundred milliseconds to nine hundred milliseconds.

And here we have the difference.

I call this the post minus price difference, but it's basically the second stimulus, minus the first

stimulus.

So first of all, you can just, you know, sit back and admire the beauty of brain connectivity matrices.

They're super interesting to look at.

OK, question, how do you interpret any of these things?

Now, sometimes you have to be a little bit cautious when interpreting channel connectivity matrices,

if the channel locations, the channel numbers do not, Nessa's do not correspond to physical locations.

Fortunately, in this particular case, we actually have that there is a meaningful relationship between

the numbers and the physical location.

So this is what the actual probe look like.

The geometry of this chank was.

It was a, you know, just a linear shank with 16 electrodes and they're all spaced.

I think they were spaced by 100 microns, so a tenth of a millimeter.

So this whole thing spans a one and a half millimeters.

So that means that we actually can interpret these neighbors.

So you see that there's not only on the diagonal, so the diagonal is all ones here, which we would

expect because this is standard phase synchronization.

So therefore every channel is perfectly phase synchronized with itself.

However, we also see that there's stuff on the off diagonals, so that is called R synchronization

across neighboring channels.

And what's interesting is to see that you get these these groupings here, these darker regions, these

squares.

So that's telling us that there is a really large network, a gamma network.

So fifty five hertz synchronization network across all of these channels going from Channel One to Channel

11, they form a functional group and it also looks like there's even some subgrouping in here.

So, you know, one through four looks like they are more strongly synchronized with each other.

And then there's a break at Channel five with weaker synchronization.

And then there's, again, strong synchronization here.

Now, what's really remarkable about this is this follows the anatomy of the visual cortex really well.

So these channels are in superficial layers and this channel.

So these channels here are around layer four and then we get to the deeper layers in visual cortex.

And what's also really interesting that you see so clearly here is that these electrodes down here are

starting to get into the hippocampus.

So this goes out of the cortex, out of V1 and into the hippocampus.

So you really see, just by looking at this plot, just by visualizing this, you can actually see the

detailed cortical anatomy and also the hippocampus.

And then it's also interesting to compare that to what you see here at eight hertz and eight hertz is

like the classic theta range.

That's where you see a lot of activity in the hippocampus.

So now you see the hippocampus is forming a really, really highly synchronous network at eight hertz.

And the rest of the visual cortex shows a little bit.

You know, there are some pockets of synchronization here and there.

OK, so all of that is really interesting to look at.

And then here we see post minus Priess.

So this is the difference in synchronization characteristics between the second stimulus and the first

stimulus.

And here at eight hertz, it looks like there's basically no change.

You know, I mean, we're not doing any statistical evaluation.

So I can't really say that this is no change.

But it looks like, you know, this is probably all pretty weak, small, possibly just due to sampling

variability and, you know, noise and so on.

On the other hand, when you look at this this condition comparison or time window comparison here,

you see that there's really, really strong synchronization across the different cortical layers of

V1.

So in particular, this is strong synchronization between these superficial layers of V1 and the deeper

layers of V1, and that is stronger in the second stimulus compared to the first stimulus.

So it's pretty obvious also, just from looking at these two guys, you don't necessarily need to plot

the difference map.

OK, and then it's also interesting to see that there is some pretty robust synchronization down here

in the deeper channels, so into the hippocampus.

But those are basically zero for this condition difference.

So the hippocampus connectivity is not changing during the two different stimulus conditions.

All right, so I love looking at this stuff, I love doing these kinds of analyses, looking at these

results and mapping these kinds of connectivity structures and textures onto brain anatomy.

Now, once you get to these matrices, there's a lot more you could do.

There's all sorts of decompositions and pattern analyses and machine learning techniques that you could

apply to understand the substructure that's embedded inside these kinds of matrices.

That is a bit of a more advanced topic that I'm not going to get into, certainly not in this video.

It's the kind of thing I talk more about in my dimensioned reduction and source, of course.

Anyway, I think I will call it a day for this video because the next video, the next exercise is going

to actually get into the data set.

So that's this seems like a natural place to pause.