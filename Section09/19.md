In the first lecture of this section of the course, I introduced you to the idea of multivariate verses

by various synchronization.

So multivariate synchronization is kind of what really happens in the brain where not exactly everything

is connected to everything, but there's many, many things that are connected to many, many other

things.

It is a massive, complex, multivariate system.

And yet we tend to look at by various synchronization measures because those are better well behaved,

those are more robust to noise, they're easier to interpret and so on.

But what I'm going to do in this video is introduce you a little bit to graph theory into one particular

analysis from graph theory that is approaching a multivariate metric of large scale synchronization.

And in particular, I'm going to focus on this thing called hub ness, which is related to a hub.

A hub is a node in a network that has a lot of incoming and or outgoing signaling.

I will discuss this more in a few slides.

So first, I will introduce you very briefly to the idea of representing a network, using graphs and

matrices, and that will lead to the discussion of how to quantify hubbins in networks.

Now, I should say, before we get started, that graph theory is a huge mathematical framework for

describing networks.

It's a really big topic in theoretical and in applied mathematics.

So I am hardly going to do justice to the entirety of graph theory.

But there is one specific analysis, one specific method in graph theory, and that's what I'm going

to focus on in this video.

OK, so let's say you have a network.

This is an abstracted network.

You can think of these as being electrodes or brain regions and align between two nodes represents a

connection between those two nodes.

So you can already see that this is a little bit different from electrophysiology, where, you know,

you can compute a measure of statistical synchronization between every possible pair of channels.

So it's not that there is or isn't a connection is that the connection is strong or weak.

But you can imagine, you know, maybe this reflects some statistical significance.

So we compute that there is statistically significant synchronization between electrode A and electrode

C, but not between electrode A and electrode E.

So this is a network.

It's a graph representation of a network.

We can also measure all of the connections in this network using a matrix and that would look something

like this.

So this is a connectivity matrix where you would see connections going from each of these nodes or electrodes

to each of these electrodes.

Now, this is a non directional or symmetric connectivity matrix, and that's also sensible here.

So there's no arrowheads drawn in here.

So B is connected to A, and that means that A is also connected to be.

So whatever you see on the lower diagonal or below the diagonal, you will see mirrored on the upper

diagonal, the upper triangle above the diagonal here.

So we see, for example, that D and B are not connected to each other in this matrix.

And then here we see that there is no line that goes from D to be OK.

So this is just illustrating two different ways of representing the same information.

This is a little bit more kind of visually easy to interpret for a relatively small number of nodes.

And this matrix form is actually a little bit better for working with in terms of the mathematics and

so on.

OK, so based on these connections, we can call nodes C a hub so we can say that this is a hub in the

network.

And what does that mean, that it's a hub?

Well, it's a bit of a qualitative term and that's why I'm going to start using the term hub ness in

the future.

But here we can say that Node C is a hub because it is connected to a large number of other nodes.

So it's connected.

It has four connections and a for example, only has three, and D and B have two connections.

And poor little E down here only has one connection.

But he is connected to see which is the hub.

So that's the most important one to be connected to.

So the idea of a hub is not only that, it's an important center for the transmission of information,

but it's also a really important node in the network.

So C is a really important place in this entire network.

If you would, Knock-Out E, you know, node E from this network, it may not be so bad for the overall

functioning of the network, but if you would knock out C that.

Be much worse because a lot of these nodes, these other nodes can connect to each other only indirectly

via C..

OK, so this is a simple example that introduces the idea of networks and graphs using matrices to represent

connectivity and in particular, hubs.

And so what I'm going to do now is, first of all, relax the definition of a hub from a binary definition

where we say that a node is or is not a hub to something called hub ness, where we can say each node

has some measure of happiness and the measure of happiness would be small for E kind of medium for D

and largest for C.

OK, so now let's switch back to electrophysiology and figure out how to compute this measure of happiness.

So it's a couple of steps.

It's fairly straightforward.

You start by computing all to all synchronization.

So here we have a matrix with channels on the on both axes.

And there's sixty four here because this comes from the EEG sample data set that we're working with.

So sixty four by sixty four.

And what you see represented by the colors in this matrix is the strength of the synchronization between

each pair of electrodes at 10 hertz and then the diagonal is missing here.

The diagonal is I set it to be zero because we don't want to consider the auto synchronization terms

on the diagonal.

OK, so this is the first step in computing hubbins of multichannel electrophysiology data.

So now we get all of these connectivity values, all these synchronization strengths.

And what we need to do is then determine a threshold.

Now, determining a threshold is always going to be a little bit arbitrary.

There are some subjectivity in how to determine a threshold.

So this threshold is a parameter of the analysis that you have to decide on.

What I typically do is pick a threshold to be one standard deviation above the median of all of the

synchronization values.

So what you see here, this distribution is all of the unique synchronization values.

So that's basically half of this matrix.

And you only need half.

You can use the lower triangle or you can use the upper triangle of the Matrix.

And these are all mirrored, all these values are mirrored.

So you just look at all the unique values of this matrix, make a distribution which looks like this

for this particular data set, and then come up with some threshold.

And as I mentioned, I would do the median and then plus one standard deviation, you can pick any other

threshold you want.

It's a little bit of a tradeoff.

If you pick a more stringent threshold up here, then you're going to get less information.

But the information that you get out of it is going to be more meaningful because you're going to identify

really strong hubs in the data set.

And if you pick a lower threshold, then, you know, everything looks like a big hub and it's a little

bit hard to make any finer distinctions.

OK, so now we have this threshold, which in this case is point four, seven or whatever.

So what do you do with that threshold?

Well, you go back to the Matrix, back to the connectivity matrix, and then you binaries this connectivity

matrix.

And the way that works is you go through each of these pixels and you say is the pixel value and you

go back to this one.

Is this pixel value here greater or less than zero point four seven or whatever your threshold is?

Now, in this case, that value is going to be less than zero point four seven, I'm guessing.

And so we would set this to be zero and then this is going to be zero zero zero zero.

And here we have some stronger connectivity values.

So we say this value here, the connectivity, the synchronization between Channel 20 and Channel 15

is greater than point four seven.

It's larger than our threshold.

So this value gets set to one.

OK, so I hope that makes sense.

That's how we binaries this matrix.

It's all zeros and ones.

You can think of this as being, you know, falsus and trews four below the threshold and above the

threshold.

And then the next step is to consider the assumption that a hub has a lot of connections, just like

in the graph that I showed towards the beginning of this video.

So hubs have a lot of connections to other electrodes.

So what you do is for each electrode.

So for example, electrode 26, you would sum up all of these connections.

So you say how many other Supre threshold connections are there between, uh, Channel twenty six and

all the other channels?

And that's going to give you some number and then you can divide that number for each electrode by the

total number of connections.

So for example, we have 60.

Three here, the 64 electrodes, and we exclude the synchronization between each electrode and itself.

So that means the maximum possible number of Supre threshold synchronization between any one electrode

and all the other electrodes is 63.

So each electrode can be super threshold synchronized to 63 electrodes.

Now, in practice, you can see that many of these electrodes are going to have values much smaller

than 63.

So, you know, this one looks like it might even have zero or maybe there's one for Channel thirty

nine.

I don't know where that one is.

OK, so now for every channel we have the total number of super threshold connections divided by the

total number of theoretically possible connections, which is the number of channels minus one.

And then that gives you a value.

It gives you a proportion, which we can call hubbins for each electrode, and that gives you a topographical

map.

And it happens to look like this for this data set at 10 hertz.

So these are real data.

And then what you see is that in this data set at 10 hertz, there is strong Hubner in left posterior

regions in the alphabet.

So these electrodes have a lot of connections to other electrodes on the scalp.

So these are Hub's in the Alpha Band and these are not hubs in the alphabet.

So all of these electrodes have, you know, zero or one or two or I don't know how many, but a relatively

small number of Supre threshold synchronization with other channels.

Now, what you don't see in this graph, what is hidden inside or embedded inside this graph that you

don't visualize here is which other channels this this channel is connected to.

So are these, you know, local connections that are just around here or are these long range connections

that this electrode is synchronized with a lot of prefrontal regions, that sort of thing you don't

see here?

Because we are summing over all of those data points, all of those other connections here.

So we're just interested in the total number of connections.

So there you go.

That is how we compute Hubner.

Now, this is for time, one time window and one frequency.

Of course, you can repeat this analysis for different time, windows and different frequencies.

And as I've mentioned with the, you know, several other previous kinds of connectivity analyses,

this stuff can get pretty thick, can get pretty hairy because you have so many possibilities for computing,

different ways, different measures of synchronization, different time and frequency windows, different

electrodes and so on.

So when you're doing this kind of analysis, it's important to allow yourself to be constrained by theory,

by your predictions and by the the design of the experiment.