I know I started off many videos by saying how much fun that video is going to be for you, but this

time it's really true.

I mean, well, the other times it's also really true.

This time it's also true.

This is going to be a fun video.

I am actually really looking forward to filming this video.

We are going to create connectivity hubs.

We're going to look at hub ness in EEG, scout data.

And to do this, we are going to be using all to all synchronization with the play the phase lag index.

And just as a reminder, which I've mentioned already several times before, I am using the phase like

index in this video, not because I want to specifically endorse that method over any other method for

synchronization, but because it's a decent method and it's it's pretty easy to implement without too

much fuss or too much code.

And it's also a decent method because then we don't need to worry about dealing with volume conduction,

which we would have to address if we were doing a standard phase synchronization for an all too all

synchronization analysis.

OK, so enough of that.

Here I am loading the data and then I don't do this in most of the videos, although I have discussed

it elsewhere in the course.

But in practice, when you were sitting down to analyze your data for reals, it's generally a good

idea to make sure your data are in double precision.

Data can often be reduced to single precision for saving to the disk.

I find that particularly useful for teaching because it just makes it easier for you to download and

move around these data files.

But for real data analysis, it's always good to have double precision.

OK, so we're going to do is a time frequency analysis, but only for one frequency.

And that's really just to make things a little bit simpler.

So let's start.

We are going to compute synchronization at 10 hertz from zero to five hundred milliseconds.

So this is going to be just the, you know, half second window after stimulus on set here.

We are creating the time vector for the wavelet and specifying the full with the maximum to be to 200

milliseconds.

And at 10 hertz, this corresponds to around two cycles of the frequency of interest.

So let's see.

Here are our connectivity or convolution parameters.

And here's the wavelet.

And let's see.

Then we get the Fourier spectrum of the data.

Now, notice that Data X here is eight channels by time trials matrix.

So we are getting the Fourier transform of all of the channels at the same time.

So that means that we're reshaping the data to go from being three dimensional to being two dimensional.

So a reshape it to be the number of channels by blank, which means whatever it happens to be and we

take the end can point fifty.

And importantly, we have provided a third input into the FFE function, which is the number two, and

that indicates that we should compute the FDA along the second dimension, which is time, OK, and

then we do convolution.

So now we need to do all channels and one line of code.

So that means that we have to write Data X times complex Morleigh, Wavelet X, that's supposed to be

Darkstar and then like this.

Now the thing is that the CMW X, this is a vector and data X is a matrix.

So if we try running this code then unfortunately it actually works on most recent versions of Matlab.

I think in version twenty eighteen this started working.

I say unfortunately because in my opinion I believe that a line of code like this should crash.

If you would actually write down the math for this multiplication, that would be an invalid matrix

multiplication.

This is not a valid multiplication, but matlab now since a few years and the recent versions actually

does allow this multiplication.

Now, if you talk to people at math works, they will say, you know, that's to make it a more powerful

and flexible engine.

In my opinion.

It makes it more open to to to potential mistakes.

I think when you make a math error, that should also be a coding error as well.

Nonetheless, this works.

We have to deal with the world that we live in.

And of course, Matlab is a really great product overall.

So I don't mean to complain.

OK, nonetheless, this works.

If you are using an older version of Matlab, then this will crash.

This will produce an error and then you will have to use you can do something like repp math and then

replicate this matrix or.

You can use the function because it's fun and I've introduced both of these methods previously in this

course.

OK, so then we trim the wings and then we reshape back to the size of the data, which unfortunately

is not working.

So let's see what happens.

Oh, I see already what happened.

Notice here, Variable X is a matrix its channels by frequency.

But here we try to trim the wings and what happens?

It turns into a huge vector.

And now the issue is that I'm just accessing it as if it were a vector.

I need to to say that we want to cut off the wings from all channels.

And then these are the time points that we want to preserve.

So let's try this again.

All right.

Now, you see, it is a channel by time trials matrix and then we reshape back to the size of the data.

OK, and by the way, did I just erroneously say that this was channels by frequencies?

This is still channels by time, but it's time for the result of complex convolution anyway.

I think you get the idea.

All right.

So we are primarily interested or exclusively interested in the phase angle.

So I'm just extracting here all of the phase angles here.

We are going to compute the all to all play matrix or synchronization matrix using play.

So you can see we've initialized this matrix to B channels by channels and here I have a double loop

over channels.

And now it's interesting to see that this first for loop goes Channel I from one to the number of channels,

but Channel J is actually going from Chennai plus one, so we're not looping from one through channels

and then one through channels.

We're looping one through channels and then this outer loop index plus one.

And the reason for doing that is that the matrix is symmetric about the diagonal.

So once we've computed the synchronization between I and J, that's the same thing as the synchronization

between Jay and I.

And we will see how that gets implemented in a moment.

OK, so what we do here is the phase angle differences.

So this looks like a pretty long, vicious line of code, but it's actually not so bad.

It's just a long variable names.

So we have let's look inside this inner parenthesis here, which is this one.

So let me put some spaces out here.

OK, so what you see here is this variable, this matrix, all phases.

And we are saying for Chan I from t idex one to two idex to this is the time index that goes that we

created above.

That goes from 500 to oh this is interesting, this is going to crash.

I didn't even realize this was a mistake before.

Watch what this is going to do.

This is going to say the index is invalid and that's because we are trying to index our access the zero

with element in the Matrix and Matlab starts counting at one, just like normal human beings and every

other intelligent species in the known universe, unlike Python and some other programs that start counting

at zero.

So let's see what we need to do.

So this is supposed to be actually time in milliseconds.

We need to convert this into indices so d search and eg times and then transpose here of course, because

we need column inputs.

All right.

This is much better.

So now we see that EGT times t idex corresponds to the time points in milliseconds of zero and five

hundred.

OK.

Very nice.

So we want the phase values from Chennai this time window and all trials minus all of the phase values

from Chan J and then the same time window of course and these same trials.

And then we need this other parenthesis here because we want to multiply the imaginary operator by the

phase angle difference between these two electrodes and not just the first phase angles, which would

be the case if we accidentally left out that parenthesis.

OK, so now that should work and we're going to need to squeeze here.

And that's because we have still technically a three dimensional matrix.

But the first dimension is a singleton dimension.

And of course, this matrix is now it's still time by trials, but it's one hundred and twenty nine

time points because we are sub sampling just a smaller window of time.

So then we compute play for this channel pair.

Now this line of code is incomplete.

Now we could just add.

A bunch of parentheses and make it complete like that, however, we need to think carefully about how

we are computing the average noticed there are two main functions here.

So this is the vectors defined by the phase angle differences.

Project that onto the imaginary axis, get the signs of those projections and then average now, which

directs, which dimension do we want to average over?

We want to average over trials first or do we want to average over time first?

And because of these nonlinearities in this formula, it's actually going to give us a different result.

If you first average over time and then you average over trials or vice versa, that's going to be a

different result and it's a different way of thinking about the analysis.

Now, there's no right or wrong way to do this.

You can do either one.

However, I am going to first average across trials because this is a task related design.

And I've mentioned previously when I talked about connectivity over time versus over trials, that for

a task design for trial based design, computing synchronization over trials at each time point is kind

of the standard way to go.

So you might as well do it that way as well.

OK, so let's see this is that this parenthesis here?

So I need to first average over the second dimension and then actually we should, you know, just want

to make sure just for a little bit of sanity checking here.

So now this is a vector with one hundred and twenty nine points, which corresponds to the number of

time points that we get from the subsample.

OK, and then we have mean again.

And now technically we don't need to specify which dimension it is because it's only a vector.

So it has to average over that dimension anyway.

Nonetheless, I'm still going to put comma one in here just to make it a little bit clearer, to make

it more understandable that first we are computing the average over time and then over sorry, over

trials and then over the time points.

All right.

So let's see.

So the PLI value between these two channels is pretty low.

It's around point one, OK, and then we enter this value into the matrix.

All right.

So let's run all of this code here and then we make a plot of this matrix.

So kind of nice, except that this matrix is missing the bottom part.

We don't have anything down here.

Now, the reason why there's nothing here is because change is never less than Chennai.

But we know that this matrix should actually be symmetric like this.

So therefore, when I'm going to do is write, play all 10 J comma and I equals the same exact value.

So I don't actually need to rerun this line of code.

You know, you could copy all of this code and paste it and just change this to you.

No change in this to Chennai, but we already know that this is a symmetric measure of connectivity.

So all we need to do is just copy that same value over to the other side of the Matrix.

All right, so let's see and actually let me first show it without changing the color scale.

So here you see what the entire synchronization matrix looks like for this particular frequency.

OK, and then the last line here is I set the color limit.

Notice that I'm setting the lowest limit to point to.

So that's already kind of a little bit of thresholding here.

So all the small values are basically saturated out of the map.

So technically there you just don't visualize them anymore.

OK, so what we're going to do now is define a threshold.

So what I'm doing is getting the distribution of the data.

This is going to be a vector of all the unique data values in this matrix.

And the way that I do that is by using this function, try you.

So this is for triangular upper.

This extracts the upper triangular of the matrix and the upper triangular part of the Matrix is basically

all the values above the diagonal.

Now, when you run this, that's going to give you another matrix with lots of zeros in it because all

of the elements in the lower diagonal are set to zero.

So then I embed that inside the function non zeros and that's going to return a vector and that is going

to be a vector of all of the obviously all the non-zero values in here.

OK, so then we define a threshold now, as I mentioned in the previous video.

These thresholds are always a little bit arbitrary.

We have to just pick some value and then stick with it.

So I'm going to use the median of the dist data plus one standard deviation of this data.

So it's one standard deviation above the median.

Now, there's no particular reason why this is the optimal threshold.

This is just a threshold that I have used in the past and generally just continue to use.

OK, so now we want to binaries the Matrix by testing against the threshold.

So what I'm going to do is write that the play All Thresh is equal to play all.

And actually let's even see what this value is.

So the threshold happens to be a value of point to three.

So play all greater than the threshold.

So this is going to do a boolean test at each element of this matrix and tell us whether each element

is less than or greater than the threshold.

So all the ones here mean that it's above the threshold.

Zeros are below the threshold.

OK, and then here we make some plots.

So here you see the distribution of data and you can see that these are not normally distributed data.

It's not that surprising.

There is a small or large number of weak connections and a relatively small number of powerful connections.

So, again, here is our threshold.

OK.

And then we're going to look at the sink, the threshold matrix like this.

And then finally, we will look at the topographical map of hubbins at this time window in this frequency.

So you can see the way that I'm doing that here.

I'm summing up all of the values for this for this matrix.

So that gives us this list of numbers.

And then to normalize this into proportion, I'm dividing by the number of channels and actually I see

now that this is technically, technically not correct, it should really be the number of channels

minus one, because we are never considering the connection to the individual channel or to each channel

from each channel to itself.

So, in fact, we should be dividing by 63.

Now, that said, you know, if you devote, for example, nine nine divided by sixty three and let

me write it like this and nine divided by 64, those are really, really, really similar to each other.

They're only different in, you know, after that at the third degree of precision, after the decimal

points in the thousandth is where they start to differ.

And of course this is a normalization factor.

That's a that applies equally to every single channel.

So, in fact, it doesn't really make a huge difference.

OK, and then here we see the topographical map of hubbins at 10 hertz in this window of zero to five

hundred milliseconds.

And it's pretty interesting to look at.

We see looks like there's a frontal lateral hub and a posterior lateral hub.

So it seems like there's two kind of hotspots in the topography that show a lot of synchronization to

other channels.

Now, of course, this is one data set from 99 trials.

It's only one individual.

So I'm not going to make a big deal about interpreting this.

But at least you can see what it looks like.

This shows you an example of how to compute this synchronization.

So let's see.

We have a bunch of well, two questions.

But I want to first start by showing what the difference is.

I'm going to put this in a different figure for averaging over these two different dimensions in different

ways.

So here we first averaged over trials and then averaged over time.

And what I want to do now is just illustrate to you what a difference this is going to make.

I think I don't believe I've actually looked at this data yet like this.

So this will be curious for me as well.

But I suspect that it will be a pretty noticeable difference that we will see in the final results.

Hmm, OK, I'm actually a little bit surprised that they are so similar, I would have expected them

to be a bit different.

So, in fact, this went a little bit against my expectations.

But it is also nice to see that you do a connectivity analysis in two different ways and you get more

or less the same results.

But obviously, based on what I just said, you shouldn't expect these always to be very similar.

OK, so let us answer some of these questions.

So does the threshold affect the qualitative topographical distribution?

That's pretty interesting.

So obviously, when we change the threshold, it's going to change everything.

Basically, it's going to change.

This matrix is going to change this map.

And the question is, does it really make a big difference or is it a relatively small difference?

So what I'm going to do is actually just comment about this standard deviation.

So I'm going to make the threshold be the median, not the standard deviation.

So that new threshold is one point zero point one five.

So the new threshold is going to be here, which means we are going to include a lot more connections.

So let's see.

Now, I will rerun all of this code and this is going to go into figure 12.

So I'll overwrite this figure.

But that's OK.

And let's see how this looks.

Well, that made a huge difference, although, you know, what we actually are looking for here is

the qualitative patterns.

So I think to look at this qualitatively, we need to change that, the color scaling.

So I'm going to say set get current access.

And I don't actually know what is a good color access limit here.

Hmm.

OK, so let's see.

Let's try seven maybe and point to.

All right.

So you could figure out, you know, you could play around with this and get this to look different.

But it's interesting to see already that the qualitative pattern doesn't really change.

So we see that all of the Hubner gets higher, all of these connectivity values get higher.

But the overall topographical pattern is pretty similar.

OK, and now I want to do one more test, and that's going to be two standard deviations above the median.

So now the threshold is going to be much more stringent.

We are going to have overall fewer connections identified as being above the threshold.

And now I'm going to set this again, maybe down to two and zero and OK.

Now I think you are not going to be surprised that the qualitative pattern looks similar still.

So I actually find this quite encouraging.

I really feel comfortable with results when I try different parameters and I change the analysis slightly

and the results are all converging to the overall qualitative same pattern.

When the results look really different, you look wildly different.

When you change one tiny little parameter, then you know, that makes me a little bit nervous.

I like to see convergence of results and patterns across different analysis methods, different parameter

settings and so on.

OK, let's go to this last question to the results look different for different frequencies or time

windows.

That is also a super interesting question.

But given the time that this video has gone on, I'm going to leave that question up to you to address.

But fortunately, it's really easy to look into.

All you have to do is change this parameter and or this parameter.

So good luck.

I hope you enjoy continuing to work through this analysis of Hognose and Aichi connectivity.