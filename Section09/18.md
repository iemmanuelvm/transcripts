Now we are going to put into practice the ideas and I illustrated in the previous video on Granger causality

or prediction, whatever you want to call it now, there isn't a whole lot for you to fill in in the

code for this video.

I didn't actually delete a whole lot of code or leave a lot of code missing.

And that's because this relies on not so much advanced topics, but topics like auto regressive modeling

that I haven't really introduced in a lot of depth except for very briefly in the previous video.

But I am going to walk through all of the code to make sure that you understand it, building up to

the full Granger causality analysis.

OK, so let's start.

We are going to do some univariate of aggressive modeling.

And just to illustrate to you, so I'm going to define these two auto regressive terms.

These are going to be the parameters.

And this is actually the same parameters that I showed in one of the slides.

So then we start off X with two random points.

So two points at random.

And you need some starting points because the idea is that you specify the auto regressive terms and

it has to be you know, you need some starter signal points to get things going in the beginning.

OK, so here we have a loop.

There's a couple of different ways for implementing a, uh, auto regressive model.

So what I'm doing here is I'm looping through from time point two to end minus one.

And then I specify the at the the T plus one s element and X that's going to start off by defining time.

Point three equals the uh so the tenth element and then the T minus one element.

So let's run some of this code here and then we'll make a plot and we'll see what that looks like.

And unrecognised function happiness.

OK, matlab doesn't know what happiness means.

Well I guess it's just a computer program.

Uh, let's see, this is this should be X of course.

We're trying to plot X.

OK, here you go.

So now you see what this looks like and now you have the opportunity, you can pause the video and do

it now or wait to the end of the video and come back to this to change these different parameters.

And it's pretty interesting to see how these different coefficients will affect the resulting graph.

So you can change this.

Now, this looks like it's mostly a flat line here, but in fact, if you would zoom in, you would

see that there's also quite some stuff happening back here.

So this oscillation already starts right from the beginning, but it's getting bigger and bigger and

bigger over time.

Pretty intense.

So have fun.

Play around with this.

You can see what's the difference between setting these values to be larger than one versus or of the

magnitude greater than one or the magnitude less than one.

Also, an interesting thing to explore around with.

You can try changing NP and see how that affects the plot and so on.

So I want to get to some by variant order regression.

This is also very similar to what I showed in the slides.

So I'm specifying that X should be a totally random set of numbers and in contrast, Y is going to be

a totally deterministic system.

So Y only depends on X and X itself is random.

So we can see what this looks like.

And so here you see that the only thing that I would like to point out here about the code is that I

say Y minus three.

So this minus three factor is just to shift the scale and shift the plot down a little bit.

And that allows us to better visualize.

I think it's it's easier to visualize these two signals.

If I would get rid of this minus three, then you can see that, you know, it's still the same data.

I find it just a little bit more difficult to visualize these two signals.

OK, and then the other thing to point out is that, uh, let's see if you are if you don't apply this

shift, you can have a y axis with numbers, because now these numbers actually do correspond to the

data values.

However, if you are offsetting one of the variables like this, then these values on the Y axis are

no longer realistic.

These are no longer valid.

And we can see that easily by looking at the first value of Y, which is actually plus point eight.

So really the first value of is here.

But you look at this plot and it looks like the first value of Y is minus two point two.

So that's the reason why I turn off this.

I turn off the Y tech, which basically silences the Y axis.

OK, so now I want to show a longer example of a bi variant auto regressive function.

Where I'm going to do is combine two frequencies or two steinway's at these two different frequencies

and give them different amplitudes here and then add a little bit of noise.

You can see this is random noise and I'm dividing by five, so it's just scaling it down a bit.

OK, and then it's going to be one second of simulation.

And then here what I'm doing is slightly different from what I did above.

So I'm still generating Y to be an auto regressive model of variable X.

So why is defined by previous values of X weighted previous values of X, but also Y has some error.

Term Y is an innovation term, so there is unique variance in Y that is not attributable to what it

obtained, what Y inherited from previous values of X..

So now we can see what this looks like.

And here you go, so you see the two sine waves here and you see that these two signals are clearly

highly correlated.

OK, now I'd like to move on to auto regressive model estimation now estimating auto regressive model

coefficients and therefore also the error terms is a little bit tricky.

And it's you know, the mechanics of auto regressive modeling is beyond the scope of this course.

I'm using this function.

Here are Morfe and this is from a Matlab toolbox, a free toolbox called Be Smart, which is not only

the name of a useful toolbox for auto regressive modeling and Granger causality.

It is also a good piece of advice for life in general.

OK, so what I want to do here is construct an auto regressive model, super simple model, no noise

in here.

I'm just specifying that each future point in X is the current point of X times one point one.

So I also showed an example of what this looks like.

It's just a growth model.

OK, and then I use the R a more function and I'm inputting X and then I'm inputting one here.

Second input is one and this actually refers to the number of repetitions.

So this is just one sequence.

So I just put in one.

This is the window size here, which is the length of X, so the window is the entire thing.

And then this other one here, this is the model order.

So this is an R1 one process that I am fitting to this data set.

So that's this final term.

Here is the order of the model.

OK, and then what I'm doing here.

So this is going to give two outputs, what I call X and X, and here you will see those variables printed

out.

So you can see that's just printed out here with a little bit of text.

So this tells us that the auto regressive coefficient for this order, one term is one point one.

So the model estimated that the auto regressive term is one point one, which is exactly what I specified

here.

And this is the error term.

And you can see this error term is tiny.

It's essentially zero plus some computer rounding error.

So we did a really good job here.

Now we're going to do is try this with an order, too.

So how do we adapt this code so we get a second order auto regressive model fit to the data?

You guessed it.

We just say comma two.

So it's pretty simple.

Here was comma one and here is comma two.

So now let's run this set of code and now we see the auto regressive coefficients turned out to be a

point one, five and one.

And here we get also a very small error term.

In fact, this is even a bit of a smaller error term than what we got up here, even though this was

the right answer.

Now, these error terms are both very, very close to zero.

So at this scale, it can be difficult to say whether this is really actually smaller than this term.

But it certainly does look like it's smaller now that shouldn't that also shouldn't be such a big surprise.

We can fit models equally well or sometimes even better, using different sets of parameters than what

we originally came up with.

OK, so that was four.

So here I want to do another example where the true model order is to.

So here we have so we just start off X with two random values and I'm specifying that the the future

point of X plus two equals at this point.

So this is you know, if you're thinking of this as the current time point, then this is actually two

time points ago, times one point one plus one time point ago, two times minus point three.

OK, so let's run this.

And actually I'm curious to see what this will look like.

So let's clear the figure and plot X, OK, so this is pretty interesting.

It's getting louder and louder over time.

OK, and let's see.

So here we are going to fit this with an order, one auto regressive model and we get that the first

order is or the first, the coefficient is minus one point two and the error term is one point three.

Now, the error term on its own is difficult to evaluate.

It's not really feasible just to look at this error term and say whether that's big or small, we want

to compare this to other error terms.

So here we say so.

Now I'm going to run exactly the same data, exactly the same function.

And it's just in order to.

So let's see what these parameters look like.

OK, so first of all, we can look at the error term.

Now, this is really smaller.

This is six orders of magnitude smaller than the error term over here and now.

It's also interesting to look at the coefficients here.

This is one point two and minus point two.

Now, that is.

Kind of close to this is one point one instead of one point two, and this is minus point three instead

of minus point two.

So you might be thinking that this model actually didn't do a very good job.

However, this actually isn't so bad.

And also, the auto regressive models fit data better.

The models tend to fit the data better when the data are stationary.

And you can see there's a really strong non stationary in the signal where the amplitude is increasing

over time by quite a lot.

And so this is actually something that you need to keep in mind when analyzing electrophysiology data,

when analyzing when applying Granger causality or auto regressive models in general to real data that

you should use time windows where you expect where it is reasonable to expect that the signal will be

roughly stationary.

OK, let's see.

I want to do one more example.

And this is going to be with a by various auto regression.

So this is almost exactly the same as the example that I showed earlier.

I guess that was in figure 13, which I cleared.

So this is really similar to what I showed above, except with that example.

This was just a why signal also had a unique error term and now it doesn't know.

It's purely a deterministic function of X.

So then we run this through and now we're actually going to get a slightly more complicated looking

output.

So for the X, so previously this variable X up here with these univariate auto regressive models,

these variables just had either one or two numbers corresponding to whether it was a order one or in

order to model.

So this variable X here is from this line of code with order two.

So we got two numbers here.

Now, the thing is, with this line here, I'm actually running X and Y, so this is a full by variant

auto regressive model and it's a second order auto regressive model.

So the outputs for a are actually a little bit more involved.

So we have two rows that corresponds to the two variables, X in the first row and Y in the second row.

And then we have the prediction of X based on previous values of X, and then we have the prediction

from previous values of Y onto X.

So if you think back to the formulas that I showed in the in the slides in the previous video, this

would be so the the current value of X equals weighted combinations of previous values of X, and these

would be the weights plus previous values of Y weighted values of weighted previous values of Y, and

these would be the weights for Y and then we have four variable y we predict y from previous values

of X and also from previous values of itself.

OK, so that's why it looks a little bit more complicated.

Now, you might also be expecting to find these numbers exactly repeated here, for example, in these

terms here.

Now the thing is the auto interpreting the auto regressive coefficients gets a little bit trickier here

because we can predict current values of Y from previous values of X, but we can also predict current

values of Y from previous values of itself without even having to call upon X..

So therefore is not necessarily the case that exactly these coefficients are going to be observable

in this coefficient, these auto regressive coefficient vectors.

And that's because this information here is essentially, you know, it's kind of cut up across, it's

spread out across all of these different terms.

OK, and then we have the error term.

And what we are interested in here is the errors for for X and the errors for Y.

And you can see that the errors for X are larger.

And that is not surprising because X also has a random variable added to it.

So I just I'm just adding random numbers, random noise onto X and Y doesn't have any randomness added

to it.

All right.

So you can see that we just get from UNIVARIATE to buy Varya and already things are getting a little

bit more complicated.

OK, now it's time to start looking at Granger causality, which was, of course, the whole point of

this video.

So let's see.

We are going to load in the sample data set, close all these figures and we're going to look at causality

or directed synchronization between RFQs and 01.

So a prefrontal channel and an occipital channel.

So let's load in these data.

And for now, I'm going to specify that the auto regressive model order is going to have or is going

to be 14.

So here I'm creating the auto regressive coefficients from one channel and the other channel.

This is only from one trial here just to keep things simple and this is in order.

Fourteen model.

OK, so these are the are the univariate order, regressive terms here, OK, and then the first thing

I want to do is show you that we can reconstruct with reasonably good accuracy the data based on the

auto regressive model.

So what you see here is in blue, the real data.

And in red, you see the reconstructed data from the auto regressive model and you can see that they

are quite a good match, the the blue data.

So the real data.

Let me actually zoom in here.

So the real data has more variability than the red line.

And that's basically just indicating that the data are, in fact, more complicated than a relatively

simple auto regressive model.

So here you can see how this works.

Essentially, we're looping over all of the time points in the signal.

And then there's another for loop here where we're looping over the order and essentially we are setting

each data point.

So the current data point to be weighted values of previous data points where the weights are coming

from these auto regressive coefficients.

OK, so I don't want to spend too much time on that except to show that these are models tend to work

reasonably well.

All right.

So now we are going to do Granger prediction.

So we already have the univariate auto regressive terms and in particular we have the error terms.

That's the important thing.

That's what really we really need.

So the variance of the error terms over time for X and for Y, OK, and then we get here, this is the

bi variant auto regression.

So now we have more terms for for even more error terms and also, of course, a lot more terms for

the the Matrix X Y, which is the auto regressive coefficients.

OK, and then here is our Granger causality.

You can see just like in the slides, I showed that formula that it's pretty simple.

We take the ratio of the variance of the univariate auto regressive errors to the variance of the by

variant auto regressive errors.

So it's quite a mouthful.

And then we take the log of that, the log of that ratio.

So it's quite a mouthful.

It's quite difficult to say, but but it's actually not such a difficult thing.

OK, so now this tells us that the grains are predictions of the directed synchronization from this

frontal channel to this occipital channel is point zero to and from the occipital channel to the frontal

channel is point zero four.

Now, this is one value over the entire time window, but there's a lot of stuff happening in this time

window, presumably in the brain in the signal.

There's a lot of non stationary hotties.

There could be different connectivity dynamics happening at different points of time.

So therefore, it makes sense to compute Granger causality or prediction in sliding time windows over

time.

And that's what we are going to do here.

I'm going to specify that the window size is three hundred milliseconds.

So we are going to be including for each model, a window of three hundred milliseconds, and then we

have to fit a separate auto regressive term or an auto regressive model to each of these windows.

So what I do here is get a little bit of data and I'm scoring it and that's to make sure that both channels

are in the same scale.

That's an important preprocessing step for Granger causality.

So both channels should be in the same scale or at least roughly in the same scale.

And ideally, also they should be stationary signals.

OK, so here I extract a little bit of data from these two channels from this particular time window.

So it's this time point to this time point plus the time window.

OK, and then I compute the order.

The univariate auto regressive model for Channel one, univariate auto regressive model for Channel

two and the by various auto regressive model for both of these channels together.

And then we take the log of the ratio of the auto regressive terms.

So and it's really that simple time domain.

Granger Causality is not terribly complicated.

Things get pretty complicated pretty quickly when we move up to Spectral Granger causality, as I mentioned

in the previous slide or in the previous video in the slides.

OK, so now this is interesting.

You can see that we get quite a lot of variability.

The Granger causality estimates are really varying over time.

So here we have this period where the causality in both directions is low and then we have this time

window here looks like pressed him where there's more stronger information flow from occipital to frontal.

So from the back of the head to the front of the head, and then after the stimulus onset, it looks

like that reverses and you get some information flow from the prefrontal cortex, from the front of

the head front of the brain to the back of the brain to visual cortex.

So that seems like a.

Pretty interesting result, and that's a easily interpretable, scientifically relevant result.

I just wanted to say that not only is this one subject of data, this is one data set.

This is only one trial.

So I would not make any cognitive interpretations on this.

This is too little data to make anything substantial, any kind of real actual claims.

However, in terms of signal processing, the main goal of the previous video was to explain how Granger

causality works.

And the main goal of this video was to show you how to implement it, certainly in the time domain between

two channels.

So I hope you found this interesting and insightful.

In the next video.

I'm going to start talking about graph theory and one of the metrics from graph theory related to large

scale synchronization.

So take a short break, go get a glass of orange juice or whatever, and then come back and keep watching.