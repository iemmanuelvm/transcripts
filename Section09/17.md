 Most of the connectivity methods that are applied in neuroscience and also most of the methods that I've introduced you to in this section are symmetric connectivity measures like phase synchronization. They are both directional. So it's not possible to use those methods to determine whether one electrode or one brain region is driving another region or causally influencing another region. So in this video, I'm going to introduce you to Granger Causality, which is one of the main methods used in the literature for looking at directional connectivity. Now, I'm going to start by talking about why I am not such a big fan of this term causality. That's why I put it in apology quotes here. I will explain on the next slide why I think we should all be calling it Granger prediction instead of Granger causality. Then I will talk about auto regressive modeling, which is the mathematical framework that Granger causality is based on. This is, by the way, not to be confused with aggressive modeling. That is something totally different. OK, and then I will get to discussing how Granger causality works and what are the key parameters for Granger based analyses. So let me start with a name. Here is a picture of Clive Granger. He is the namesake of the Method Granger Causality, and he's also a Nobel Prize winning econometricians. I think it's called someone who studies metrics and analyses and models in economics. So here is the thing. The thing about Granger causality is that having a positive, a significant measure of Granger causality neither implies nor requires causal interactions. So the term causality is built right into the name. However, you can have meaningful, interpretable, interesting Granger causality results that actually have nothing to do with causal interactions. So therefore, I have argued that we should be calling it Granger prediction because that is a much more accurate term. This is a prediction is a more accurate descriptor of what actually happens in the analyses. We are trying to predict what's happening in one signal based on what happened in the past, in another signal anyway. The truth is that, you know, one person can come out and say, we should call it this thing that's, you know, maybe a better term. But the entire field is still calling it Granger causality. So eventually I have capitulated and I mostly call it Granger causality as well. But you should keep in mind that Granger causality, though it is a powerful and useful data analysis method, it does not imply causality and it does not require causality. So with that in mind, let's move to the idea of Granger causality. So before getting to any math, I'm just going to speak about this operation very broadly or semantically. So the idea is, let's say we recorded electrical activity from these two electrodes, electrode A, an electrode B, and the question is, can we predict what happens here at this electrode at this time point if we know what happened here in the past in this other electrode? So can we use these data points to make some prediction, some accurate prediction about what is happening at this data point in the other electrode later in time? Now, if we think about this for a moment, we can already see that there is a there's a problem, there's a potential conflict of this. Let's imagine that electrode A an electrode B are basically the same. Maybe there was an equipment problem. Maybe there's an electrical bridge. Let's imagine that Electrode B is literally just a copy of Electrode A.. Now, if that were the case, then it would actually be kind of trivial that we could predict what happens here based on what happens here previously in time. And that's because what happens previously in time here in Electrode B is exactly the same as what happens previously in time in Electrode A.. OK, so to account for that possibility, we need to extend this question a little bit. So what we're asking is, can we predict what happens here based on what happens here? And is that better than knowing what happened here? So here we are, considering the data points previously in time, the same exact time points, but now from the same electrode. So we want to know, can we predict can we use information from the past of B to predict current values of A better than just predicting current values of A, from past values of a OK, so that is the general idea of grandeur. Causality if there is. If. The answer to this question is yes, then we say that there is a Granger causal interaction between Electrode B onto electrode A and you can already see that this can be a directed measure of synchronization, because if we turn this around and say, can we predict what happens here in Electrode B based on what happens in Electrode A here, we might not be able to predict. So the prediction might go this way and might not go this way. OK, so let's start talking about a little bit of math to start with. We need to talk about auto regressive models and we'll start with what's called univariate auto regressive models. So that is quite a mouthful of terminology. Let's see, before even looking at some equations, let's see if we can already figure out what this term means. So UNIVARIATE means one variable and auto regressive means regressing or correlating with itself. So a univariate auto regressive model means a model where we are predicting one variable from previous values of itself. So that might look like this. Now it's two equations here just to show that it's two variables. But you can see there's only X's in the top equation and there's only Y's in the second equation. So what we are saying here with this equation says is that we are trying to predict the current value. So time point T in data vector X based on some weighted combination of previous values of X. So A1, this is a coefficient that's just a number, it's a scalar times X of T minus one. So the previous time point plus a two times X of T minus two. So two time points ago and then all the way up to and, and then X of T minus. And now this might actually look a little bit familiar. This is a little bit like filtering. So auto regressive modeling is a little bit like filtering where you have a set of filter weights, a set it well here. They're called auto regressive coefficients, but they're similar, analogous to filter weights. And you set a new data point to be equal to a linear weighted combination of previous data points. And then here we have an error term. That's e that is basically anything. You know, sometimes people call this an innovation term. It's basically anything that is present, any variance in except that is not accounted for by a one X, T minus one plus A2, blah, blah, blah, all the way up to these terms. So any variance in signal X that cannot be explained by weighted combinations of previous values of X, those all go into this E term and you can see it's e, t, x, so each time point has its own error term. OK, and then we have the same thing for Y and the only reason why I'm rewriting this here for Y is so you can see that we have different coefficients, we have a different set of coefficients and a different error term. All right. So this is a univariate auto regressive model. From here we build up to a by various auto regressive model. And this means that we have the same concept, auto regressive. But now we have two variables. So it's by variants. Of course, the next step up from here would be multivariate, but I'm not going to talk about that here. So we have a bi variant auto regressive model. Now, notice, what we are trying to do here is predict current values of X from previous values of X weighted combinations of previous values of X and weighted combinations of previous values of Y. So we are adding a second variable into our attempts to predict current values of X, and noticed also that this error term is different. So here I'm using the letter E here. This is the Greek letter Epsilon, and you will see in a few moments why it's important to separate the letters here. So these are for the univariate models and the epsilons are for the by variant auto regressive models. OK, now this is a lot of terms in here, but I'm going to do now is just rewrite exactly these equations using slightly more compact and useful notations. So what this says is that the current value of T in signal Y is equal to the sum of A an X, T minus N for any equals one two K. So all of these a sub ends are are different or can be different from each other. And then you know this with this summation notation, this basically allows us to generalize to any number of K. Now this parameter K here is called the order of the auto regressive model. So if we have. A first order auto regressive model, which is also sometimes called H.R. one for auto regressive one, then essentially. Well, it's is one. Of course, you could just get rid of this design and you would have that. You are predicting current values of X based on only the previous value of X time, some scalar, OK, and then plus this innovation term. So here is the comparable formula for a bi variant auto regressive model. Again, this part is exactly the same as this part here. So we're trying to predict current values of X based on past values of X, and then we add the previous values of Y into this equation. And then again, of course, we have this error term here to account for whatever is not being accounted for by previous values of X and previous values of Y. So I'm going to get back to these equations in a few moments. But first, I want to show you a few examples of univariate and by various auto regressive time series. So here is an example of a first order auto regressive univariate time series or an R1 process. So we have that current values of X equal the previous value of X times one point one, and that's all there is to this equation, nothing else. And that gives us this geometric increase here. So each data point is the previous point times one point one. So it's growing slowly but steadily over time. Here we have an R2 process. So there are two. The order here is two. So we say that the current value of X is one point two times the previous value minus. Or maybe you want to think of this as saying plus minus point nine times two values or the data at an variable X two time points ago. And also there's nothing else to this model. And what's interesting is if you have this alternating sequence of signed coefficients like this, then you're going to get an oscillating model that looks like this. In fact, with these particular scalars here, we get a dampening oscillator. So it's an oscillator that decreases in amplitude over time. OK, so these are just two examples of auto regressive models here. I'm showing you an example of a bi variant auto regressive model. So it's two variables, X and Y. And what I did in this particular case was that X to be a random sequence that is drawn from a normal distribution with parameters zero and one corresponding to the mean and the variance. So this is just random Gaussian numbers. And then I said y to be a perfectly deterministic result of previous values of X actually following the same formula that I showed in the previous side. So it's kind of interesting to think about these two variables, because in some sense, in a kind of proximal sense, Y is a purely deterministic system. Variable Y has no variability. It built into it. It is purely deterministic of X. On the other hand, X itself is a random process. So Y actually, you know, in a kind of Destil interpretation is a random process as well or is driven by random process. OK, so these are some examples of univariate and by various auto regressive models. So now I want to talk a little bit more about these these error terms here, because they turn out to be really crucial for the quantification of Granger causality. So what we can do is plot these error terms over time because we have, you know, for every for the signal over time, every time point has its own error. So that might look like this. So this could be the time series of errors from X here, from the univariate auto regressive model. And this could be the error term from this prediction of X. So this is the bi variant auto regressive model. So now let's think about these for a few moments. What do we want from these errors, error terms if the model is really a good model to the data. If the model is a good fit to the data, then we would expect these error terms to be generally small and close to zero. So if they are close to zero, they're still presumably going to be positive and negative. Maybe this is a zero line here, there. So if we take the average of these error terms, that's basically that average is going to go to zero. But what we are interested in is the deflections away from zero. And we want to know whether those deflections are large, meaning that it's not a very good fit to the data or whether the deflections are relatively close to zero and the way that gets quantified engrained in the context. Granger causality is by looking at the variance, so we look at the variance of this error term over time and the variance of this error term and now what can we expect? Well, first of all, let's think about what would happen if X and Y, these two variables, these two signals, X and Y are totally, totally independent of each other. They have nothing whatsoever to do with each other. If that's the case, then Y has no correlation with X, and then we would expect all of these auto regressive coefficients, all these B terms to be zero. And if these terms are all zero, then of course this entire term gets knocked out, this whole thing becomes zero. And then this by variant equation and this univariate equation are identical. So if X and Y have nothing to do with each other, then we would expect that E is exactly equal to Ayda. So sorry, Epsilon. So these two error terms would be exactly identical to each other and therefore obviously there are variances will also be identical to each other. OK, and now let's consider the more interesting situation where we actually can predict current values of X based on previous values of Y. And so if that's the case, so if the past of Y tells us something about the current value, the present of X, then these B terms are going to be non-zero. And then presumably this epsilon here will be smaller. They will be closer to zero because this by variant model is fitting the data better than this univariate model. So the model fits the data better. So the errors are smaller and that's what you see depicted here. That means also that the variance of this term is going to be smaller than the variance of this term. And that is the key insight that leads us to the quantification of Granger causality. We essentially want to compare these two are the variances, this error term versus this error term, and that goes into this this fraction here. So we say the variance of E, which is the variability. So the errors of the univariate auto regressive model where we're only predicting X based on previous values of X and in the denominator, we have the error term coming from the or the variance of the error term coming from the by variant auto regressive model. So now let's think again about what happens in our two hypothetical situations. So the first situation is that Y is totally uncorrelated with X or Y does not predict X. Therefore we expect that all the E's and all the epsilons are identical. So the variances are identical. So this ratio becomes one and then we have the log of one is zero. So then our Granger causality term is zero, the value is zero. Now let's imagine the other situation where these error terms are actually smaller because we actually can take advantage of previous values of Y to make predictions, accurate predictions about current values of X. So then this ratio becomes larger because this term is large, in this term is small, and then the log is going to be positive. It's going to be some value larger than zero. And now let's think about whether this could happen the other way, whether Granger causality could be negative, which would actually mean that this ratio in here is less than one, which would mean that the variance of the auto regressive terms are actually smaller than the variance of the bi variant auto regressive terms. So coming back here, you know, let's consider the number of parameters that we have in this model here. We have K parameters in this model and here we have two parameters because Kaypro parameters for A and K parameters for the B's here. Now, it is well known in statistics. It is a well-known phenomenon in statistics that if you have a model with more parameters, it is going to explain more variance in the data. So simply by making the model more complex with more parameters, we are necessarily basically necessarily going to explain the data better, even if those parameters are not terribly useful. And that's really just related to overfitting. So what that means for us here is that this model is basically never going to fit the data better than this model. They can be the same. They can fit the data the same, which will give us a range of causality value of zero. But if this model fits the data better than this model, then you would get a grander causality to. That is negative and that's really unexpected and that can happen, so it's theoretically unexpected. It actually does happen sometimes. If you run Grayndler causality on on a lot of data sets, you will probably come across a case where the Granger causality is actually negative. And in general, that means that the models are such terrible fits to the data that this model is actually doing better than this one. And that is, if you see negative Granger causality, then something is probably, you know, maybe the data have some artifacts or they're not very clean, or maybe you need to change the model parameters. All right. So I hope that makes sense. I think Granger causality is actually a fairly straightforward concept. Now, what do you do at this point once you've computed Granger causality? Well, the idea is that you have your two time series. And because we expect sync patterns of synchronization to change over time, we expect there to be a lot of temporal dynamics. Changes in the connectivity landscape of the data sets, which you actually want to do, is compute or what is often useful is compute Granger causality, not once over an entire time series, but instead in a small window like this. So you would take only this window of data. Maybe this is, I don't know, 400 milliseconds or something and compute Granger causality inside this window. And that gives you one result. And then I'm sure you see where this is going. You slide your window over, compute Granger causality again and keep sliding the window over and over. And you end up with a whole time series of Granger causality estimates or directed synchronization. So this should look familiar. It's actually conceptually a little bit similar to convolution, except that the operation that we are implementing at each step is considerably more involved than just computing the DOT product. OK, so I've already mentioned the two important parameters for implementing Granger causality analyses. One is the size of this window, which I just mentioned. I said maybe it's four hundred milliseconds. And the other parameter is the order of the auto regressive model. And that was the character K. in the formula. So maybe I'll even go back here. So that is this K here. That is the order of the Granger causality model or the auto regressive model. So how do you know what to set these two parameters to? Well, you have some choices. There's it can be a little bit tricky to know. So let's first talk about the time window so you could have a relatively small time window like this or you could have a relatively larger time window, something like this. Now, there's advantages and disadvantages to each of these choices. If you have a long time window, that means you have more data. So the auto regressive modeling is going to be better, which means, you know, you have more data to fit the models. So the model is likely to be more sound and high signal to noise. On the other hand, you have worse temporal resolution or this should actually say precision. You have worse temporal precision because you are including incorporating data from a longer time window. So if you have, you know, transient connectivity dynamics, those might get lost or at least a bit dampened by mixing brief periods of connectivity with long periods that might not show much greater causal interactions. So then it's the flip side. For these relatively shorter time windows, you have better temporal precision. But on the other hand, it's going to be more difficult to estimate the auto regressive model. So the model estimation is not going to be as good. So what should you do? How to pick the optimal time windows? So typical values are in the range of, you know, a few hundreds of milliseconds, maybe to a few seconds. But at some point the data are going to start getting very non stationary. And you want to keep the data as stationary as possible for the other progressive modeling. You can try out different values on on test data, or you can look in the literature and see what values other people have used and use those parameters to start from. And you can also think about the time windows that you anticipate stationary to be occurring in your data sets. This is similar to one of the considerations that you use for deciding about the full with graph maximum for wavelet convolution. And again, you know, for typical cognitive electrophysiology, something like hundreds of milliseconds is a pretty decent range. OK, so now let's talk about the model order. So the model order is how much time you are incorporating here in the past. Values of the signals in trying to predict the current values of the signal so we might have this depicts an order of four. So this would be a relatively low order. And here this is a relatively higher order. And yeah, so again, these both of these options, lower orders and higher orders have advantages and disadvantages. With a higher moral order, you are more sensitive to a longer period of history. So that's a good thing. But when you have more parameters for the auto regressive model to estimate, so that can lead to poor model estimation and also longer computation time. On the other hand, if you have a smaller order, you have fewer time points, then you have better model estimation because there are fewer parameters for the model to estimate. And it also means faster computation time. But of course, the downside of this is that you are less sensitive to the history of the past. So how do you pick this order? Well, again, you can try different values on test data. You can use from the literature. You can do model comparisons like Bayes information criteria. This essentially involves running these models, estimating these models over and over and over again for various order parameters, various and KS. And then you see which which of those model orders fits the data the best. That sounds like a good idea and it can be a good idea, but it's also a little bit tricky because the optimal model order can vary over time and it can be different for different conditions or different pairs of electrodes and so on. So there can be a little you know, this sounds like a nice procedure because it's quantitative, it's statistical, but it's also a little bit tricky and it doesn't necessarily lead you to a single correct or optimal answer. So my strongest recommendation is whatever you do, however you pick your model order, pick one value and keep that value constant for all of your analyses. So you don't want to be changing the analysis parameters continuously. OK, I want to show you one slide just to give you an impression of what this looks like. So here we have the base information criteria and here I have the model order. And essentially this is showing the fit to the normalized fit to the data. So accounting for the total number of parameters as a function of the order of the auto regressive model. And then what you're looking for in this Bayes information criteria graph is the order, the order with the smallest base information criteria. So in this case, that happens to be around twenty seven milliseconds in this particular data set for this particular channel pair and this particular time window. But then what I did was rerun this model and figure out what is the optimal model order for different time windows and that is plotted here. So you can see that over time there is a different optimal order based on Bayes information criteria. So this is something that is a little bit unfortunate because if you are trying to compare a model here with a model, with an order of whatever this order is, maybe it's like four relative to a model here that has an order that's much higher. You know, it's not really a fair comparison to compare Granger causality based on this model with Granger causality that's based on this model. So therefore, my recommendation is that you can use these measures like base information criteria as a kind of rough guideline to help you choose. You know, I would say based on this, some order between 20 and 40 milliseconds, these are all reasonably good choices. And then you just fix your model order and apply it consistently throughout all of your analyses. The last thing I want to discuss in this video is what is called Spectral Granger causality. That is a pretty cool extension of Granger causality. Granger causality, as I've shown it so far, is just a broadband time domain analysis approach. So it gives you a time series of Granger Khazali estimates. But that reflects just the whatever the contents of the broadband signal are, it is also possible to do Spectral Granger causality, where you end up with a time frequency plot that looks like this. So it's taken from a paper and this shows the directed synchronization from the amygdala to the offset, minus the directed synchronization back from AFC to amygdala. So the fact that you see this red colors here tells us that between, you know, 10 hertz and 15 hertz in this time window during the task, the details of the task are not important now, but there was a directional flow of information. Preferentially from the amygdala to the orbital frontal cortex in this condition. OK, now you are probably thinking that you already know how to do Spectral Granger causality, which is to narrow band, filter the signal and then apply the Granger causality procedure that I explained in this video. Unfortunately, that is not the right way to do it. It sounds really intuitive. It sounds like it's the right thing to do, but unfortunately it's not narrowband filtering and then applying Gurinder causality is not the right thing to do because the effect of filtering is actually just to make the auto regressive model fit the data worse. And it doesn't give you the spectral specificity. And that's basically because auto regressive modeling works differently from a time frequency analysis. They are they can be used to obtain similar results, but they are not the same kind of analysis. Instead, the way that spectral greens are causality works is to take the time series of auto regressive coefficients. That's the series of of A's and B's that I showed earlier when I was showing the formula. So the scalars that are multiplying previous values of the signal. So you apply a transfer function to that time series of auto regression coefficients and that transfer function will extract the spectral content of those auto regressive coefficients. It's a little bit like a four year transform, but it's not formally exactly a year transfer. OK, so Spectral Granger causality can be a little bit hairy. I talk about it a little bit more in my book analyzing real time series data. If you would like to apply Spectral Granger causality in practice, then I recommend using a toolbox. There's one called the Multivariate Granger Causality Toolbox, and you can find it just by Googling the term. All right. So this turned out to be a pretty long video, but I wanted to be thorough and I wanted to make sure you have a really solid understanding of the idea of Granger causality, how auto regressive modeling works and how to use auto regressive modeling to compute and interpret the Granger causality resulting values.