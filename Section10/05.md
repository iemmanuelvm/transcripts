 In this video, we are going to explore a different way of computing power correlations, this time over trials based on a time frequency window. So in the previous video on power time, serious correlations, we computed correlations over time or we're going to do in this video is a different approach. So you can see. So here's like a time frequency plot. Now, this is not showing time, frequency power. This is showing time, frequency power, correlations with power in this window here. So what I did to create this plot is get the average time frequency power from this window, so the average power from all of the pixels in this time frequency window and extract one value per trial. So that gives me ninety nine values corresponding to ninety nine trials. And then for all the other power values distributed across the rest of this time, frequency plane, I computed the cross trial correlation between time, frequency power at for example, two hundred milliseconds and seven hertz and the time frequency power in this window over trials. So this is a map of connectivity, Crossrail connectivity and actually you can call this cross frequency coupling and also cross time coupling because we are seeing that changes in one frequency are related to changes in a different frequency. Of course, it's very dark here. This is just showing autocorrelation. So this window is correlated with itself. All right. I hope that makes sense in the background section, the background, the video in this section for the background knowledge, I also provide additional information about how to approach and interpret these kinds of relationships. All right. So let's get started. So we load in the sample EEG data set here. I specify the frequencies. I'm doing this in a slightly different form from how I typically do it. Normally, I have two separate variables for the minimum frequency and the maximum frequency here, I'm specifying as a two element vector, the lowest and highest frequency. There's no particular reason to prefer one method over the other. Just to give you a little bit of variety. All right. Let's see here. I specify the analysis parameters. So these are the time frequency windows for the seed region, and that's this region here, and then we pick a channel to use. So in this case, I'm using FZ, of course, after the video, I encourage you to come back to this code, try changing lots of different parameters, try changing the electrode and so forth. All right. So here, let's see in this cell, we are creating Complex Morleigh Wavelets and in particular the spectrum of the complex where the wavelets in preparation for convolution and time frequency decompositions. I want to talk about this code. I've talked about it plenty before. Let's say here is the time frequency analysis. There's also nothing particularly new or exciting in this cell except for this one line of code here where I extract power from all the trials and all the time points and all frequencies. So you can see this matrix is three dimensional. It is frequencies by time, points by trials. And that's different from the typical time frequency matrix, which is trial average. So that's just frequencies by time points. So to run this correlation analysis, we're going to need all of the individual trials here. I reshape the three dimensional data down to two dimensions so you can see it's ninety nine trials by one hundred nine, nineteen thousand time by frequency points. So let's see, the first thing I want to do is show you what the time frequency map from this electrode looks like. Looks like this FZ to this map is not showing connectivity. This is just high frequency power at each point and each time. Frequency point. Relative to the stimulus baseline, so you see there's an increase in theta, some increase in upper alpha and a bit of suppression. Now I'm going to extract the seat power and create what's called a design matrix, that's to set up a general linear model. So what I do is create this variable called seat power. You can see there's a lot of stuff here, but the meat of this line of code is here to 3D. So remember, this was frequency's by time points by trials. And here I'm extracting the frequency range and the time range over all the trials. And then I average over the frequencies and then average again over all of the time points and then squeeze out the single dimensions just to give us a normal column Vector Seapower. And then I create this design matrix. There's a column of ones corresponding to the intercept and then a column of the seed power of values. So let's see here. I show that design matrix. So this is what the design matrix looks like. If you are not so familiar with interpreting design matrices and the setting up a general linear model, then don't worry. That's not so important for this video because I'm going to show you an alternative way to do this, using just regular correlations. This shows the data matrix. You can see it's ninety nine trials and nineteen thousand two hundred time frequency points. All right, so now I'm going to implement this power cost trial, power connectivity in this cell using linear at least squares modeling. So here's what that looks like. We have the variance of the design matrix and that gets inverted and goes into the covariance between the design matrix and the data themselves. That returns a matrix, which I'm calling here X. This is the regression parameters. It's two by nineteen thousand because there are two progressors the intercept and the slope, which is the. Power in that stimulus beta band window and of course, nineteen thousand two hundred for the time frequency points. And actually what we are interested in is particularly this second column of the. Parameter set, and so that gives us this matrix of beta values, which is frequency by time points, and then here I'm just illustrating what that looks like. And then here is where I draw a box around this region. So this is the result. This is the result of the general linear model regressing the data across trials in this time window onto the data at every other time, frequency point. And basically red colors indicate that there's a correlation, positive correlation between time, frequency, power across trials and this time at this time, frequency point and in this window. OK, now you might remember from the beginning of this video, I showed this figure. So why does this figure not look like this figure? This figure looks different. In fact, this figure comes from simple correlation. So let me show you that now here I loop over frequency points and I loop over time points and I get the data at one single time frequency point. So from the 3D, I get this frequency point to this time point and all trials. That gives me this. So you can see it's basically a vector with 99 points, but then there's two singleton dimensions in there. And then here I run a standard Peerson correlation coefficient between that data at that time, frequency point and the power that gets stored in this variable kamat. And then that's what I create a contour map of in figure four, which is this figure. So the question is, why does this figure look different from this figure? Now, first of all, I would like to point out that they are not so much different. In fact, if you look here. So the color scaling is a little bit different. But you see this kind of nice extended correlation here in Theta Upper Theta and that you see here as well. Here you see some blue stuff, so some negative correlations. And here you also see it's faint, but you can still see the blue correlations here. So they are qualitatively similar, you could change the color scaling to make them look a little bit more similar, but they actually are different. And that difference has to do with normalization now in a correlation coefficient. When computing a correlation coefficient, both variables get normalized according to their own variances. And that's useful in correlation because it balanced the correlation coefficient between minus one and plus one in a general linear model. That's not the case. So the variance and the scaling in the original data also become incorporated into the resulting beta values. Now, there's an argument to be made for that being an appropriate strategy. So time, frequency points that have more variance are going to have a different beta. But it's also possible to modify the data to make this result look more like the correlation coefficient. So in particular, we can Z score the data and now you will see remember. So this is what it originally looks like. So now I'm going to run actually maybe the good I just put this in the figure five. OK, so now we have figure three was the general linear model without normalization, without normalizing the time frequency data and then figure for what's just simple correlations. And figure five was exactly the same general linear model, these squares, linear modeling but with Z scoring of the time frequency data. So now you can pretty clearly see that figure four and figure five look really, really similar to each other. And in fact, really the only difference between them is color scaling. So you could change the color scale in here to get these to look even more similar to each other. So which one is right and which one is wrong for this general and unskilled versus scaled linear model? That's a longer discussion. It's related to statistics and there's an argument to be made for both sides. But the point is that there are the same qualitative patterns that you see throughout. So it doesn't really make a huge difference. In fact, a lot of this is it's just due to scaling. Maybe we can even scale this to make this look a little bit better. Let's see. Let's go for how about three? Maybe, uh, about four. OK, well, OK, so you can see they're not identical, but they're quite similar. And by changing the colors going around a bit, you can get them to be a little bit less dissimilar from each other.