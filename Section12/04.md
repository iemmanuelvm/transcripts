 In the previous video on permutation testing in simulated time series, you saw at the end of that video that there were a lot of little individual time points that seemed to be statistically significantly different between the two conditions. But I commented that we probably don't want to believe in those individual points because they seem like Alfaro's driven by noise and the assumption that we can make, which is a very reasonable assumption for many time series data, is that we can believe in effect when it is contiguous across multiple time points. So that leads us to one method of correction for multiple comparisons, which is call it a cluster correction. So essentially what we are going to do is start from the assumption that we will believe findings. We will believe that findings are real, or at least we will, you know, reject the null hypothesis if the supressed threshold statistically significant points are in a cluster. So then the question is how to define that cluster, shall we say? You know, there needs to be at least two points next to each other or 11 time points next to each other. And that's a cluster. Obviously, these are arbitrary thresholds and they're subjective and they're not very useful. So instead, a much better way to do it is through permutation testing. And I talk about the reasoning and the mechanism of this in my YouTube channel videos. So here I'm just going to focus on the code, the implementation using simulated data. Now, most of the code for generating the Time series here, the data sets, is almost exactly the same as I used in the previous video called Permutation Testing in Simulated Time series. So if you haven't already gone through that video, then I recommend going through that one before going through this one. You will see, if you compare them, that they differ a little bit here. I've defined the full with the half máxima to be parameters that can be specified separately for the first signal and the second signal. And I hardcoded those parameters here in the previous video. So the reason why I soft-hearted them here as parameters is because after I go through this video, I encourage you to go back and play around with these parameters and see what is the effect of having different fill with the half máxima on the resulting cluster threshold and the resulting statistical significance plot. All right. So with that in mind, I'm actually not going to talk any more about the code in this cell because that generates basically the same thing as in the previous video. So I just made this one a little bit narrower. All right. Let's see. So now we go down here and then we do a permutation testing. This is also identical code as what I had in the previous video. So I'm just going to run this without saying anything about it. And now the idea for determining the cluster size under the null hypothesis is to go through each of these Permuted Time series and pretend as if these were real. So we're going to threshold each of these and then find all of the cluster. So let me walk you through how this is going to work. So let's initialize this. You'll learn about what this vector is for in a minute. Now, I initialize this permutation iteration looping index. Now, notice what I'm doing here, so I'm creating a Z score difference that initially looks like the same or really similar to computing the real difference that you saw in the previous video. However, you can see. So let me first off with this part. So this is minus the mean in the numerator and then divide it by the standard deviation. So that is part of computing Z score. However, I'm not actually using the observed condition difference. I'm using the permuted condition difference. Now, this thing, we expect to be, you know, basically zero plus some sampling variability and or noise. So this Z that the fake this is a fake Z score time series. Let me show you what this looks like. So say plot time by Z. So that looks like this. And now the thing is, I expect that all of these numbers should be zero. Right? These are all this is totally permuted. This is all null hypothesis data. Everything in this line is null hypothesis data. So it really should be all zero. And the fact that it's not zero reflects that there is noise and there's variability. Now, we've already determined that we have a threshold of one point nine six. So that means that anything, any values, any statistic values at values that are greater than one point nine six are technically, according to this threshold, statistically significant. So you can see there's quite a few points that are going to be considered statistically significant because they are above one point nine six or below minus one point nine six. So that means anything that is non-zero. So now let me plot this again. So here you see all the quote unquote significant points up here and down here and all the other points here and maybe actually with a little bit misleading time with lines. It really is more like this. This is a more accurate way to depict what's going on in this Time series now, because all of the data in here are from the null hypothesis, just null hypothesis distribution, just different ways of looking at the null hypothesis distribution data. I am going to say that all of these points are errors. These are all alpha errors. These are statistical errors. And I don't believe that these are real findings, but they occurred under the threshold that we selected. So now what I'm going to do is find all of the contiguous clusters. So this is one a cluster of one value, a cluster of one value, and this one here, you can see there's two rings and I don't know if these are exactly next to each other, but let's say they are. So maybe this is a cluster of two values to contiguous values here. So now I'm going to use this function. BW Con. This is located inside the image processing toolbox. And what I input is a logical vector of so it's going to be all zeros and ones and when I'm going to get out is a structure called that I call islands. And these are the two important fields. So this is telling us that the number of islands where each one of these is an island is one hundred and nineteen to one hundred nineteen clusters here. And this cell array pixel IDEX list tells me the indices into each of these. So let's see so we can do islands that pixel idea. Let's let's look at the hundred and first. So. So this is actually let me start actually by just looking at all of these so you can see it's a salary, most of these only contain a single element in these cells. So a single number and this is the index into this array where there is a significant value. So this tells us that the very first island, the first cluster is that index forty seven. And it's only one element, it's just one single lonely point. And then all of these are just one point. And then we have to get here all the way here up to, you know, like the twenty eighth or whatever. This is cluster before we actually get to a cluster with more than one contiguous point here. I guess it's this one here. And then here's another one with to see if there's anything more than two. I think we only get either single points or double points. So now looking at this just informally, we can look through this list and say that it is possible under the null hypothesis to find individual points or a cluster containing two points that would occur just under the null hypothesis. So therefore, it's reasonable to say that when we look in the real data, so not the fake Permuted data, but the real Z difference, if we see any clusters that have either one contiguous point or two contiguous points, we say, I reject that. That is likely to occur under the null hypothesis. All right. So that's the basic idea. So then what we do is find the lengths of all the clusters. So basically just the length of each of these elements. And I'm using the cell phone function. So a function applied to all the elements in a cell. The function that I'm applying to each element in this cell array is the length function and then this is the salary that this is applied to. So when I run this and basically this is going to be a vector of all in this case, it happens to be all ones and a couple of twos because this was all of of length one. And there were a couple of elements, a couple of clusters that had two neighbors in them. All right. And then what I do is from this list, I find the maximum number of clusters which in this case happens to be two. And then I saw that in cluster sizes. All right. So then this loops over all 1000 permutations. It takes a couple of seconds for this thing to run. And now we see that these the cluster sizes that we can expect under the null hypothesis actually maybe plot these class sizes. So the cluster sizes that we can expect under the null hypothesis are generally around two or three. There were once where there was five contiguous points and, you know, some with four, mostly with two or three. OK, so then what we do is take our P value and find the percentile. So the 90th percentile is ninety fifth because I'm using a P value threshold of point of five, the 90th percentile of this distribution. And here you can see what that distribution looks like. So here's what the distribution looks like, you can see mostly we get clusters of two or three or some four is one five and one one. Now here, we don't actually need to divide this by two because we're only doing a one tailed test. We're not looking for the smallest cluster sizes. We're only looking for the largest cluster sizes. All right. So we got a cluster size threshold of three. Now, this means that in the real data, we consider any significant cluster that has fewer than three elements in it contiguously those we don't believe, and we will throw them out. So now let's say most of this code is the same as what you saw in that previous video. So I'm just plotting it again. And then here I find all the islands in the actual data, in the observed Z value. And let's see how many there are here. Quite a few. So there's one hundred and forty three of these clusters here. So now what we do is loop through all of these. So all one hundred and forty three of these. And then I see. So I count the number of clusters and so the number of pixels in each cluster. And I say if that is less than the cluster threshold and you know, I think this probably should be less than or equal to the cluster threshold. Then I set the Z threshold in that cluster to be equal to zero and then that gets plotted again. So let me run through this. So now this already looks quite a bit nicer. So all of these individual points have disappeared and what we see is one big cluster well, looks like one big cluster here, and it looks like one big cluster here. Now, there's a bunch of doeth here, I find this a bit ugly and it's also a little bit difficult to interpret. So what I'm gonna do here is draw this again. Let me show you what this looks like and then I'll go back and explain the code. So here I draw patches that go from the Z Value Time series here down to the zero lines, a Z equals zero. And this is actually exactly the same data as what you see here, just plotted differently. I think this looks a little bit nicer. So now we can see how many objects are left, how many clusters are left. So there's nine. So from here this looks like one, two clusters, but it's actually not. And if you look carefully in here, you can see that there's nine. Let's see if we can count that. One, two, three, four, five, six, maybe seven, eight. Well, OK, so there's going to be a couple of other ones that are a little bit smaller at this scale. And essentially what I'm doing here is drawing a patch around the object here and that just dropped it. And now it's kind of interesting that you get separate patches here. Let me actually zoom in so we can have a look here. So here you can see these are separate patches here, so one, two, three, four, five patches right here. I probably missed some of these smaller ones and I was counting earlier. Now, when you look at this visually, it really does look like this is one patch. And it turns out that part of this is related to the smoothing characteristics of the noise. So how smooth the noise is. And this is a pretty interesting feature to explore in simulated data where you can actually control the smoothness or the, you know, the spectral characteristics of the noise. And that's something that you're going to have the opportunity to do in one of the projects. I think it's going to be the first project at the end of this section of the course.