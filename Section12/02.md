 If you spend some time playing around with the code in the previous video, so that's the video on permutation testing for one variable in two groups, you may have discovered that each time you run through the permutation testing, you will get a slightly different answer. So the Z value and the corresponding P value would be a little bit different for permutation testing, although of course it's identical for the parametric two sample T test. In this video, I'm going to show you the implications of that variability, that randomness, and one way to resolve that randomness, which is to use something called a meta permutation test. OK, so I will explain all these concepts as we go along. First, I'm going to generate the data. This is basically the same thing as what was in the previous video. So I just run this cell and here now this is kind of similar to what I did in the previous cell there. You saw this for loop here. So loop over permutations, then I generate fake condition labels. I compute the means of each quote unquote condition for the two different fake condition labels and then take the difference between those means as the test statistic or the statistical characteristic for this permutation test. Now, however, what I'm going to do, what I've added in this video compared to the previous video is to have this loop over many different runs of permutation. So these are I'm calling here meta permutations and this is going to run 100 times. So I'm going to run this permutation procedure 100 different times. Now, you'll notice that the actual data are the same. We're not generating new data. That's important because if you generate new data, of course, the resulting permutation tests will be different. So the question here is, if we compute the the data once and then repeat the permutation testing 100 times, what is the distribution of Z values going to look like? So here we go through this double for loop here. Here again, compute the observed means and the observed mean difference between these two conditions and how computing the set value is nearly exactly the same. Conceptually, it's exactly the same. The main thing to keep in mind here is that, well, actually, let me run through this code first. So when I give you this warning, it will make a little more sense. So previously this variable per means was a vector. So that means that the mean of that vector was a single number. Now, however, we have a matrix. It's a thousand permutations and one hundred runs. So in fact, the mean is not going to be a single number. It's going to be a vector of one hundred numbers. And that corresponds to the average of all 1000 permutations for all 100 meta permutations. OK, so basically that means that the mean is going to be a vector. The standard deviation is going to be a vector. So critically, you will need this dog's so element Y's division and not Matrix Division. OK, and then I can compute this Z value and P value. And now you see we get one hundred set values and one hundred corresponding P values. Now what I'm going to do is show you all of those null hypothesis distributions. So what I do is generate a histogram of the permutation testing results for this iteration. So for the ith iteration, which is again one hundred and then here I'm just putting them into a matrix and then plotting the matrix. So let me show you what that looks like. Here you see all the distributions. It's all the same color, but there's 50 lines that are being plotted here. Sorry. One hundred lines being plotted here. It's 50 bins. So you can see that these are not identical. Each run of the permutation testing is not identical because we are randomly generating these fake condition labels. However, they are all roughly similar. So it's not like, you know, the mean of some distributions or some meta permutation tests is out here or all the way down here. So they're all generally the same. However, they do differ. There's quite some variability across the different runs. So and then again, I'm going to plot the real difference and then that you can see here. So now the idea of Netta permutation testing is to average all of the test statistics together, so average all the Z values together. So let me show you what all these Z values look like, X, Y, Z and the P values at the same time. So here you see a histogram of all the different Z values from the different permutation tests. So from these different loops in here. So you can. That the Z values differ a little bit, not enormously, but they are a bit different. There is some variability here. Here you see the distribution of Permuted P values. Now, in this case, they all happen to be on the same side of point zero five. So that's a good sign, assuming that you're taking point of five as the alpha level, the significance threshold in all 100 of these permutation tests, you would have come to the same statistical conclusion, which is that the two conditions differ. The mean of the two conditions differ statistically significantly. However, you can imagine if the variability were a little bit higher or maybe if the means were a little bit closer to each other, or maybe if we had fewer trials, that this distribution could have been shifted more broadly. And maybe some of these values would have been above point of five, meaning that even though the data themselves hasn't changed, sometimes the result would be statistically significant and sometimes it would not be statistically significant. So the solution or a solution to this conundrum is to implement this permutation testing 100 times or some number of times, and then you can beat the Z value for each individual permutation and then finally take the average of all those Z values. And this is the Z value that you take for the real data. So it's basically the center of this distribution. Now, this is a second level statistic. So this converges to its mean value very closely. That's why I used a thousand. Iterations for the first level permutation test and 100 iterations for this permutation test and in fact, sometimes in practice are only used maybe 20 or 25 meter permutations because this second level distribution converges so quickly. And here, what I want to show you is that the average of the P values is not the same thing as the P value of the average of the Z values. So that's quite a mouthful, so let me unpack that a bit. So what I'm putting in this magenta line, you can see here, that is the average of the P values. And these are all the individual P values that I computed here based on the individual Z values. However, the thing is that the conversion from Z value to P value is not a linear distribution, not a linear transformation, it's a nonlinear transformation. So that means that averaging before versus after a transformation is not necessarily going to give you the same results and that's what you see here. So here in the blue dashed line, what you see is first I take the average of all the set values and then I convert that into a P value using this Norm CDV function. So and that's the blue line. So fortunately, these two will converge. They're generally quite close to each other, but they're not identical. And actually this is the better way to do it. This is the more appropriate way to do it. So first, you would average together all the set values. Again, these are the normalized distances of the observed data relative to the center of the null hypothesis distribution. And then you convert that into a single P value, and that would be the P value you would take for the center of this distribution.