 The goal of this project is to give you the tools to have code to understand the effects of smoothness of noise on the result of cluster correction. Now, it should make sense that you're going to get clusters of different sizes, different characteristics, depending on the smoothness of the data and also on the smoothness of the noise. The smoothness, of course, encodes the auto correlations. So when you have smoothed noise, you're going to get fewer really, really small clusters and more larger clusters. So basically, what you're going to do is integrate some of the code that you learned in this section of the course with some of the code that you learned from the first section of the course on simulating data. I suppose technically it's the second section, but the first section of the real content of the course after the introduction anyway. So you will recognize these pure signals from the past few videos on in this section. And then what you saw before in those videos was that when you average signals plus noise together and the average together 100 trials with random noise before that looked much more kind of random because that noise was totally uncorrelated. That was pure white noise. In this case, what I've done is create correlated noise, basically by having noise with low frequency characteristics. Now you can see it still looks quite noisy, but it doesn't just look like random white noise. Instead, it looks like, you know, it's kind of wiggly. These are this is low frequency structure. And now if we look at the distribution of cluster sizes generated by thresholding per minute permuted null hypothesis data, you can see that the cluster sizes are now, first of all, sort of normally distributed and they're peaking at around 38 to 40 or so. Now, that is quite different from when I used pure white noise. And then you'll remember that all the cluster sizes under the null hypothesis were basically, you know, one or two or three and there was a couple of four. And, you know, but mostly they were of clusters of size one or two. And here we see clusters of size, you know, 30 to 40. The whole range goes from actually even very, very few small clusters. Most of the clusters start at at least 10 consecutive time points. All right. And that is also going to change the smoothness of this. So here you see the again, the same results from previous videos. So this is uncorrected data. This is a cluster corrected. And this is the same cluster correction, but now with drawing pictures, instead of drawing little dots at each individual time point, now it's also interesting to note that there should be two clusters. So when you look at the ground trade data, you see that there should be two differences. One difference here and one difference here. Now, this is a large cluster and this is a relatively small cluster. It's really just, you know, a small handful of time points here. And that is let me go back and see. OK, so that's exactly at two seconds. So when you look at the uncorrected signal, you see that that difference is there before correcting for multiple comparisons using cluster based correction methods. However, when you use cluster based correction methods, this one survives. This first peak survives, but this one is now no longer significant. And that's because although it is statistically significant here, we're not correcting for multiple comparisons. The cluster threshold ended up being sufficiently large such that this cluster got obliterated. This got filtered out. And you can guess by looking at this thing that the cluster threshold is probably, you know, so it's five percent of this distribution. And so it's probably a little bit over 50 time points wide. So basically, this significant region, even though in the original ground truth data, there is a true difference here. This is too narrow relative to this. So the cluster threshold ended up being higher and this was effectively wiped out of the statistical comparison. So this would be called a type two error. There really was an effect and we failed to detect it in this data. Now, that may or may not happen in your case when you run this code. And if I would run this again multiple times, I think this might pop in and out of statistical significance anyway. I hope the goal of the project make sense. Here is the starter code to get you started. You can see I leave a lot up to your imagination. Most I give you a couple of parameters about how I simulated the noise. So frequency domain, Gaussian, these are the parameters that I used and let's see otherwise mostly just some plotting and here that I left for you. So there's quite a bit of work for you to do. So good luck. And in the next video, I will walk you through my solutions.