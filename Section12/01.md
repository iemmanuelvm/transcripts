 This video is going to be a pretty gentle introduction to a permutation testing, and I say gentle because it's going to be permutation testing for just one variable between two groups. So imagine two different groups of trials or two experiment conditions. And I think it's good to start off with a simple example, because it shows the idea, the concept of permutation testing. But it's exactly the same concept that will apply to one dimensional or two dimensional data sets or, you know, any kind of application of permutation. Testing is all based on the same concept that we are going to explore in this video in the simple case. All right. So let's see. Make sure we have a clean and clear workspace here. What I'm going to do is generate simulated data. So we're going to generate two data sets or I guess I should say one data set with two different conditions. And these are just arbitrary numbers. But you can imagine that these are times, for example, reaction times in milliseconds. So this would be half a second and a little bit under half a second. So the two conditions, they differ. So there's a true mean difference of two twenty milliseconds. But each condition will have only 100 trials and the standard deviation in each condition is 60 trials. So it's actually three, 60 milliseconds. So it's actually an interesting question whether we will be able to observe and find statistically significant support for a true mean difference of twenty milliseconds with a standard deviation of 60 mile seconds and when we are sampling only from one hundred trials. So this also means that the true difference, the observed difference, which we will compute later on in this video, might not be twenty milliseconds, it might be ten milliseconds, might be thirty eight milliseconds. But of course if we would re sample multiple times, many, many times, eventually we would converge on a difference of twenty milliseconds. OK, so I'm going to create this data set as a matrix. It's going to have two columns. The first column is testing whether the condition number is so zero or one. So it's basically just going to give us one hundred zeros and then one hundred ones. And the way that I'm setting this up is to generate index variables, to generate numbers from one to two times the number of trials. So when the two hundred. And then I test whether that's greater than the number of trials. So let me first walk you through this piece by piece. So if we just look at this part, you can see you have to run this. Now, if we just look at this code, we see the integers one through 200, not anything super interesting. And here I test whether this list of numbers is greater or each element in this list is greater than 100. And you can see that that's converted this vector into zeros and ones. And in fact, this is a boolean vector created by typing who's and you want to look at ends. And so that's the last output of Matlab gave. So it's an array of two hundred numbers and it's logical. So these are all True's or falsus. But actually later on I want to use these as numbers. I'm going to convert them into double. So that's the first column of the data set and then the second column of the dataset is artes and you can see that the code here is missing. So, of course, I encourage you to pause the video and see if you can solve this mystery here. If you need a little hint, think about generating normally distributed random numbers that have a standard deviation corresponding to this variable stuff. So 60 and a mean and average corresponding to either this variable or this variable. All right, so the solution here is we start off by saying Rand and we want in trials Cuma one. So this is going to give us a vector of normally a common vector of normally distributed random numbers. Of course, these don't have the same name or standard deviation. So let's deal first with the mean so I can say plus mean one. Now this on its own is still not what we want, but it's getting us closer now. We have normally distributed numbers with an average of 500 and now we just need to change the variance. So are the standard deviation. So the key thing to keep in mind here is that the standard deviation of this, the output of this function is one. So therefore, if you want the standard deviation to be something else, all you need to do is multiply all of these numbers by the desired standard deviation. So, in fact, we just need to multiply this by Steve Deve. All right. So that solves this. And then this one is actually nearly identical, except this is mean, too. Now you'll notice that I'm having the same standard deviation for both data sets. Of course, you could have a separate standard deviation for each data set and then you would need a separate variable here. All right. So let's run these two. Now, the first thing I want to do before getting to the statistics, the permutation testing is visualize these data distributions. Actually, I see this is different, but I should say and there means so what we're going to do here is use the hist function or you can also use histogram if you like. We want to generate a histogram with 20 Binz corresponding to all of the artists from Condition Zero, and then on this line all of the artists from condition one. And then here I'm going to plot those two histograms on the same plot. So, again, some code is missing, it's clearly some code in here, and I encourage you to pause the video and see if you can figure out how to select all the trials, because the data, this matrix data is two hundred rows, but we only want to get the rows for this line of code. We only want the rows where the condition is zero. And here we only want the rows where the condition is one. All right. Here is the solution or one possible solution. We can use a logical indexing to say data. Now we want all the columns and sorry, all the rows and the first column where those are equal to zero. And then we get the second call. So it's a bit of a weird looking line of code because you're putting the data variable directly into the data variable as an argument. That's a little bit weird, but it makes sense because here we say with this bit of code, we say data from all of the columns in the first row and where that equals zero. And that actually turns out to be the first hundred numbers are true. The next hundred numbers are false. And when you use this boolean array as logical indexing, this is going to return only one hundred numbers and it turns out to be the first one hundred numbers. So we can see again we can type whose hands and see that this is just one hundred numbers. All right. And then we get the histogram, let's make sure this code works. And then here, not surprisingly, this is nearly the same. So I'm copying and pasting code, which means you always need to be really careful to make sure that you are adjusting the code where necessary. Leaving it here as zero is a really easy mistake to make. All right. So there we go. Now, let's see, so what's the figure? Here's the weird subplot, geometry, but I'll show you what this looks like in a minute. Then so this subplot thing just says five rows and one column of subplots, and I'm going to plot into columns two through four, so that just makes it have this kind of rectangular shape. There's no particularly special reason why I did that. Just to have some variety and show you a bit about using the subplot command and kind of non-standard ways. OK, then we get the distribution's I set an output of the plot function, which gives me a handle on how this variable h this handle is actually an array because there's two lines. So it's going to be two handles. Now, I want to change the line width of both handles so I can just input the vector. But here I want to change the line color of each plot separate or each line separately. So then I have to input only the first handle and here is just the second handle. All right, and then I want to plot the means, and so we plot the X coordinates by the Y coordinates and this is going to be a blue dash line. So the X coordinates are a pair. You can see it's one one, but then it's times the average. And this is really the same thing as what I did here for here, input into the hist function and here input into the mean function. And then the same thing, of course, for condition one. And then let's see just some labels here. All right, here you go. So these are the means, these are the distributions of the two data sets of the two conditions, and then the question is, are these two distributions different or do the means of these two different distributions differ from each other? Actually, just out of curiosity, we can see what the means ended up being. Exactly. So let's see for 96, and that was specified to be 500, and then here it is, four seventy six, which is supposed to be for eighty. So it might seem like we made some systematic error. And that's why we are almost exactly four milliseconds too short relative to what we specified here. But that's really purely random. It just by chance. So now I just ran the first cell in the second cell again. And now let's see, the second one is, oh, it's also 476. OK, well, you know, sometimes the universe aligns in just the right way. All right. So here we are doing the permutation testing. So the idea of permutation testing, which I explained in the YouTube videos from the background section, is that we are going to keep the reaction times identical. You don't change the reaction times or in general the dependent variable. And instead what we're going to do is shift the or shuffle the condition labeling. So we are going to randomize which trial is condition zero and which trial is condition one. And then the way that I do that is by generating fake condition labels. So these should be conditioned labels that look just like the real condition labels, which in this case means that they have a value of zero or one. And there are several ways that you can implement the shuffling. The way I've done it is by generating normally distributed random numbers of two hundredths of two times the number of trials per condition and then test whether those are less than or greater than zero. So you can see we get this vector that's kind of randomly shuffled ones and zeros. There are several other ways to do this. And I encourage you to try and think of alternative ways to come up with the same solution. So to come up with random condition labels or to randomly shuffle the condition labels that are already there. Once we have those, we want to compute the observed difference between those two. So in this case, with the real data, the real difference between these two means is probably somewhere around 20 milliseconds. And we want to know when we shuffle the data, what is the difference in the means in the shuffled data? And that is going to be our data value, our statistic generated under the null hypothesis. And of course, we loop through many times so 1000 times. So it turns out that the way to accomplish this is basically the same as this. So I'm going to copy and paste here and now. The main difference is we don't want to select the rows that really corresponded to conditions here. So instead we want to correspond to we want the rows corresponding to the fake Kande Labs fake condition labels where that equals zero. Then of course, it's the same thing over here, but it's where the fake condition labels equals one. So let's try this for one run of studies and let's see generate these area, generate fake labels. So let's see fake I mean, one in two. Oh right. Uh, so there we go. And now let's see what this difference happened to be. So this is really close to zero. This is just under one millisecond. So you can see that when we generated data under the null hypothesis, a statistical characteristic under the null hypothesis, then we found that the difference between these two lines got much smaller. So that's a good sign. We want to know. So if these really differ, the means of these two distributions really differ, then the observed means should be considerably larger than the differences of the fake means. All right. So this gets run through 1000 times and then Nemesio, run this. So now we get this vector of perm means. And I'm going to show you what this looks like when you say plot perm means and let's go with squares. OK, so here you see over a thousand iterations, a thousand times of generating fake condition labels and here you see the observed differences. So in fact, we found that the observed difference is it's even computed down here. So this is the real difference, very real. This is the observed difference in the actual data. And you can see they differ by a little under 19 milliseconds. And now you see that sometimes purely by chance, when generating fake condition labels, we get conditioned differences that are even larger, even larger in magnitude than the observed difference of 19 milliseconds. And more than that, we even get in the other direction that goes the opposite direction. So the blue distribution is smaller than the red distribution, and that also happens on quite a few trials. Now, this is all by chance. And so what we want to do now is. Compute a Z value. So we want to know how far is this real difference of 19 milliseconds? How far away is that from the center of this null hypothesis distribution? So to do that, we are going to compute a Z value so you can see this line is missing. So I encourage you to pause the video and work this through yourself if you need a hint for computing this Z value. Here's your little reminder. So the Z value is the observed value minus the center of the null hypothesis distribution divided by the standard deviation of the null hypothesis distribution. All right, so what we need to do now is say real def minus the mean of perve means and then that's divided by standard deviation of perm means. So here is my line of code. I would like you to have a look at this line of code, maybe compare it against your code and determine whether this is correct or whether this is incorrect. So obviously this is incorrect. Otherwise, I wouldn't have asked the question. The but the next question, which is the more important question is why is this line of code wrong? Nothing is wrong here. Nothing is wrong here. It turns out what's wrong here is an order of operations. So Matlab is actually prioritizing the division over subtraction. So Matlab is in fact computing. This equation here like this, and that's not what we want, what we want is this is the denominator and this is the numerator. So we need to put in an extra set of parentheses to indicate to Matlab that this should be prioritized as the numerator. And then this is the denominator. And let's see what that happens to be here. So we have a Z value of two point four, and that means that the observed difference from in the real data, quote unquote, real, I mean, it's all simulated data, but. The observed difference is two point four times larger, whereas the further away from the center of the null hypothesis distribution. Where the null hypothesis distribution is basically our chance level data. OK, now I'm going to convert that into a P value. By evaluating it along a normal cumulative density function, now this function, Norm CDV, this is in these statistics toolbox in Matlab, so if you don't have the stats toolbox, you won't be able to compute this directly, or at least you won't be able to use this function. Let's see. And now for comparison, I'm going to compute a regular two sample t test that you would use for parametric testing. Now, it turns out that in this particular example that I've chosen a parametric test, so normal two sample test would be a totally appropriate statistical approach. So you don't need to use a non parametric permutation testing for this particular example. But of course, there are other cases where the distributions of the underlying test statistics are unknown or when the data are not normally distributed, where you will have to use a non parametric permutation testing and which is basically this method nonetheless, in this case, because we are permitted to use parametric testing on a difference of two distributions of normally distributed numbers, we can compare the permutation testing result against a parametric test result. So use the T test to function. That's also in the stats toolbox. And then here just to make the comparison a little bit more convenient, I'm going to convert the T value into a Z value because you can't really compare a T value against a Z value directly. So here I compute the T value and then here I compute the P value from the T value and then convert the P value into a Z value. And that's ultimately the thing that we can compare against this Z value. OK, so let me run through all this code again and here's where I show the distribution. So let's see. I think there's nothing for you to do here but sit back, relax and enjoy. So here you see a histogram. This is showing the mean difference for all 1000 null hypothesis tests. And you can see it's centered at zero. That shouldn't be so surprising. And the standard deviation of this distribution looks like it's somewhere around maybe eight or something. We can actually confirm this empirically. Yeah, so the standard deviation is a little bit under eight. And here is the observed difference. This is the actual difference in the data. And then the idea is that this difference is around two point four standard deviations away from the center of this null hypothesis distribution. So here you see the Z and P values for a permutation testing and the Z and P values for the parametric test. Now, in this case, they kind of align. So they both indicate statistical significance. If we would take point of five to be our significant threshold. The permutation test happened to give a larger test statistic than the parametric test, but that's not generally the case. That really just happened to be the result in this particular instance with these randomization.