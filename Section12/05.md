 I'd now like to show you an example of permutation, testing and in particular correcting for multiple comparisons across the time frequency plane using cluster correction via permutation testing in real EEG data. So this is going to be group level data. And actually, let's just start by loading this in and seeing what variables we have in this matched file. So we have a variable called Frex, a variable called Time Save. And you can probably imagine that this is a vector of frequencies. This is a vector of time points. And then we have this variable T.F., which is four dimensional. So above three dimensions. Matlab no longer shows you the actual number of elements along each dimension. So we're going to have to type size T.F. here. So this is twenty five by two. By 60 by 100. Now you can already see that 60 by 101 corresponds to frequencies by time. So this is the time frequency plane. Turns out that this dimension, the second dimension is condition. There were two conditions in this experiment and it doesn't really matter for our purposes here what those two conditions are. But the goal is going to be to compare these two conditions and see if there are features in the time frequency plane that differentiate between these two features at a P value of zero point zero five. OK, and then this 25, this actually corresponds to different individual subjects. So there were 25 individuals who participated in this experiment. And from each individual, each of twenty five individuals, we have two conditions and a time frequency map. So at this stage, we no longer have trials, the trials are actually averaged into this map. So that means we're doing a group level analysis we are looking to see across individuals across this population of twenty five people are their features and the time frequency plane that differentiate condition one from condition two. So let's see what these two conditions look like. So here I'm going to make a plot. This is going to be a contour plot of time by frequency. And you can see here it's the time frequency matrix and I'm taking the mean over. There's no additional argument here. Input. So that means it's the default of the first dimension, which is subjects. So let's see what all of this looks like. I'll run it up to here. So here you see the time frequency map for condition one and the time frequency map for connection to and you can just eyeball them and see that there's two primary features, two salient features in this time frequency. Plus one is this blue areas, this relative suppression from around 18 to 30 hertz or so. And that's pretty consistent between the two conditions. A little bit stronger suppression here. And then there's this lower frequency increase in the theta band from around two hundred to eight hundred milliseconds. And it's pretty clear that that is stronger visually in condition to compared to condition one. So we are going to determine in this video whether either of these features or any feature anywhere in this map is statistically significantly different between these two conditions. All right. So this is average data. So each of these maps is the average of data across twenty five individuals. So I'm going to show you here and figure two is one of the conditions so conditioned to this one here and now I'm going to plot it for each subject individually. And this will give you a bit of a sense of what the data look like across different individuals, a bit of a sense of the variability. So and this is interesting, you see that some features are really consistent over subjects and some features are not so consistent over subjects. So if we look at this low frequency increase that seems to be present in every individual, a little bit weaker and maybe lower frequency here, not very strong here, although you can still see it like you definitely don't see it in this one person. So this person seems to be missing this feature. All of the other individuals seem to have this features sometimes a little bit weaker and sometimes a little bit stronger. And similar story for this slightly higher frequency suppression. You see most subjects have it. This subject a little bit weak, this subject, I don't know. And this subject really not so much. OK, so there are clearly some features that are consistent, some features that are not so consistent here, for example, this might be an artifact. This high frequency thing here doesn't seem like it's there for anyone else, maybe a little bit here. OK, anyway, moving right along, so I'm going to do now is one permutation testing at the pixel level. Now some of the mechanics about how I'm doing permutation testing here are the same as what you've seen in several previous videos. The key difference is how I do the randomization and if you like, before inspecting this code to DP, you can think about how you would do the randomization, what exactly you would be shuffling, considering that this is the data and we have the time by frequency plane and then across the twenty five subjects, we want to see if condition one differs from condition to. So how would you do that. So keep in mind that the null hypothesis here is that Condition one and condition two are the same, or at least that they are not different. Now, if condition one and condition two are the same, then that means we could say condition one minus condition two and that should give us a map of all zeros. Plus, you know, some variability and sampling error and so forth, but if one minus two is zero, then we can also expect that to minus one is zero. So one minus two should equal to minus one, should equal zero. And that case, that assumption is true under the null hypothesis, that condition one in condition two are not significantly different from each other. So therefore, the way that I'm going to randomize the maps or shuffle the data here is going to compute the difference. So this is the the difference, the discrete derivative of the T.F. Matrix. This means empty here. So it's the default of a first order difference. And the second are the third input here is two, which means it's the second dimension. So I'm going to compute the difference along the second dimension here, which, of course, is condition. So now that's going to be condition one minus condition, too, and remember, if the null hypothesis is true, then two minus one is the same thing as one minus two and one minus two is also the same thing as minus one times the quantity two. Minus one. That makes sense. So essentially what I'm going to do is have this variable here called T.F. Dif. This is subjects by frequency's by time points. And then when I'm going to do inside the permutation testing. So at each iteration inside permutation testing is generate a sequence of random ones and minus one's. So what I'm doing is coming up with normally distributed random numbers, a number of subjects of those random numbers, and then taking the sign of that, and that's going to give me just a random sequence of ones and minus ones. Then another loop here where I loop over all the subjects and I say that. The difference map here, so the fake difference map is equal to the true difference for this particular individual, this subject times either one or minus one. So basically I'm flipping the sign of this subtraction for a random subset of subjects and then I compute the mean and then that gets done, you know, a thousand times or whatever the a thousand times. I'm going to start running that now. And this goes through pretty fast. But Kluster, correction takes a little bit longer, but this goes through pretty fast. So you can see that wherever or any sort of regions this time frequency plane where the null hypothesis is valid, where the null hypothesis is true, then this is basically going to go to zero, which it will be. You can imagine here these two conditions don't differ. So this is close to zero. This is close to zero. So something close to zero minus something close to zero is going to be zero here. The true difference, this minus this is going to be far away from zero. OK, so that's the idea. Now I'm going to do some statistical thresholding, pixel based statistical thresholding. So we're not correcting for multiple comparisons. I'm just going to threshold the different map at P less than point of five, not corrected. So I create a Z map, which you see now several times before. So it's the true difference minus the mean of the null hypothesis differences divided by the standard deviation of the null hypothesis differences. And then here on thresholding that map, so I say anything less than the significance threshold, which is one point ninety six that corresponds to P less than point of five, divided by two and divided by two, because we're testing both also for red and blue. And then I'm going to just make a few plots of this. Let's see what these plots look like. This is the raw difference in terms of power. So this is literally just this map minus this map. Here you see the Z map. And so this is the non threshold. It's that map. And this is the threshold at Z map. So these are all the pixels that are significant at P less than point of five, significantly different between these two maps. And now what we want to do is determine whether some of these clusters are are sufficiently small that we can expect them to appear under the null hypothesis, just given the fact that there is spatial temporal smoothing that's intrinsic to the time frequency analysis. OK, so that's what we do here. So I take each Permuted map from the permutation testing, treat that as if it were a real map, even though it's not, it's permuted, but we treat it as if it were a real map and then threshold and then find the clusters and find the length of all the clusters and then take the maximum cluster and then get the cluster size. So, again, this is all code that you've seen in the past, several videos, and then finally we're going to make a histogram and plot the observed threshold on top of that. All right. So this takes some tens of seconds, but that's OK. We're patient. So here you see the distribution of cluster sizes under the null hypothesis. And again, I think I've made the same error elsewhere. So this is actually not 10 points, but pixels. That's OK. And this is the actual threshold. So we are going to take 459 contiguously significant pixels as the significance threshold. So that means we can go through our threshold in maps. So this Z threshed map. This is this map here. Let me confirm that I've made any typos there. Yeah, that's right. So that's this map. And then we will extract all of the islands of all the contiguous regions in this map and then we loop through all of these and we say, is this bigger or is there more or fewer than four hundred and fifty nine pixels in this cluster and in this cluster? In this cluster and this cluster and so on. And if it's smaller, if the number of pixels in that cluster is smaller than the cluster threshold, then we obliterate that. We set that to zero, which means in this map it's going to turn green. OK, and then that's going to be plotted here so that you see here. And this one looks a little different because there should be another line that says Axis Square in there. There you go. That looks a bit better. OK, so this is kind of interesting. All of these differences disappeared, including this blue beta band suppression here. So this is stronger suppression here than here. Now, when you're only doing uncorrected pixel level statistics, you see that that effect appears to be there. However, when correcting for multiple comparisons, it disappears. What remains, though, is this blob here and actually this extends all the way out here. Now, I think that there's just a little too much smoothing in the plot and this happens that extend all the way up to 40, almost 40 hertz and all the way up to twelve hundred milliseconds or whatever this time is. I believe that this is not part of, you know, sort of biologically, physiologically, this is not part of the same cluster as this. But, you know, sometimes the statistics just work out the way that they work out.