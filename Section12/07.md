 So now I'm going to walk you through my solutions to Project seven one and as yet another reminder, the goal is for you to work through code, for you to think carefully and critically, write code and solve problems and learn it doesn't matter if your code looks different from mine. All right. So with that out of the way, then, let's see. The first thing we're going to do is simulate the data. A lot of this comes from code that's mostly I think a lot of this is literally copied and pasted directly from code earlier in this section of the course, for example, permutation testing and simulated time series and permutation testing for cluster correction in simulated data. So I'm not going to talk a whole lot about this code. I am going to walk you through how I created the noise, because there's a couple of tricky points in here that I want to make sure are clear. So, of course, we are not adding random noise here, white noise. We are adding color noise. And the way that I've decided to color this noise is by making frequency specific noise. So what I do is specify a frequency and a full with a half maximum and hertz. So this is so what I'm going to do is create a Gaussian in a frequency domain, Gaussian that peaks at four hertz and as a forward traffic maximum at five hertz. So let me run this code and show you what this looks like for a plot. And then Hertz by frequency Gaussian. And let's set this to get Curnock maximum change from let's go from zero to 30 hertz. Actually, it's too much about 20 hertz. So here you see a Gaussian. You can think of this like a frequency domain gain function. So it picks up four hertz. It doesn't even drop to zero at DC. So there's going to be there could be some offsets, which is totally fine. That part doesn't really matter. The key point is that this is going to generate low frequency noise. So we are going to be generating and adding noise to the data set that has a low frequency structure. All of the frequencies in noise are below 10 or 11 hertz. And they're concentrated from, you know, we can say like two to eight hertz or so is the proximate concentration of most of the spectral features of this noise. Now, what's also important here is that I'm specifying the noise, standard deviation, so that encodes how variable the noise is going to be. And that's important considering the magnitude of the signal itself. So you can imagine if you had a tiny, tiny, tiny bit of noise to the signal, then it really doesn't matter and you might as well not add noise at all. On the other hand, if you're adding so much noise that the signal is completely lost. Then there's also no point to the simulation. So we want to be able to manipulate. We want to be able to control the amplitude of the signal, which is pretty straightforward, versus the amplitude or the variance, the standard deviation of the noise. Now, if you're just generating white noise data, that's a bit easier, because if you're using the function randon, then those numbers already come out with a standard deviation of one. So then you can just multiply it by, for example, two to get a standard deviation of two. So it turns out it's not quite as simple to control the standard deviation of noise. If it's colored noise that we're already manipulating, we're already selecting part of the spectrum. So we will need to figure out a way to accomplish that. All right. So what I do now is create a loop over trials. And so I'm going to create this data, set one trial at a time. Of course, the signal is the same in every trial, but I want the noise to be trial unique. So here I generate some Fourier coefficients, these are uniformly distributed random numbers, so positive random numbers at these non-negative between zero and one, and this is going to serve as the amplitude of the four coefficients. And then this is going to serve as the random face value. So you can see it is each of the eyes. So this is going to give us our vectors on a complex plane and then by two PI and then Rande. So again, this will generate random numbers between zero and one. This gives us random fais values between zero and two pi. And here I'm basically just taking these to be the phase angle and then oilrigs them here. So this generates random vectors in the unit circle on the complex plane and here I'm giving them random amplitude. So that's going to be our random Fourier coefficients. Then the four coefficients get multiplied by the frequency domain Gaussian by this gain response. Take the inverse for you transform to get back to the time domain. And then we only care about the real part because this turns out to be a non symmetric spectrum. So the resulting time series is complex, but we only care about the real part. So that's how we generate the noise. Now the thing is the noise and let me actually just run. So let me run some of this. I can show you what this looks like so I can say, uh, plotts, let's just block the noise. So here's what the noise looks like for this trial. So this part looks fine. It's clearly low frequency noise, I think just to give you a bit of a sense of the time scale. So here it now it's being plotted in seconds on the X axis, but the Y axis, you can see that it has a range. So it's in a scale of ten to the minus three. So these values are tiny. And you can see this if we plot the signal. So it's called pure one. So now you look at the signal and all the noise, you know, you can just barely see some tiny fluctuations here. Mostly this noise is totally flat. So this is useless noise. We're basically just adding like a tiny rounding error to this. So we need to scale this up. So now if I want this noise to have a standard deviation of two, if this were just pure white noise where the standard deviation is already one, then it would be simple. The solution is simply to say times, noise, STD, and that would solve the task. However, that's still not the case. In fact, we can quantify this. I'm going to say standard deviation of the noisy times, our requested standard deviation, and that's point zero one. So three orders of magnitude smaller than what we actually want it to be. So it turns out the solution is deceptively simple. I hope you didn't come up with some really, really complicated way to solve this. The best solution is just to first divide the noise by its standard deviation, and then you can simply multiply it by the desired standard deviation. So let's run this now, and we should see that the standard deviation of the noise is exactly two, which is what we specified. So that's great. OK, so that gives us data set one for this trial and it's the signal plus noise. And then this code is almost exactly the same as copied and pasted. Really the only thing that's different is the one turns into a two here. All right. So let's see so that now I'm going to run through this whole cell. I think everything else in the cell is relatively straightforward. So it's interesting to compare this resulting time series to the signal plus noise and the average over all the trials that you saw in that previous video. And in fact, this is quite interesting. We see that the noise was sufficient to completely obliterate the effect here. In fact, I think it even goes a tiny bit in the opposite direction. So if we zoom in here, in fact, if anything, the direction reversed, it should be that signal one. The blue line should have a higher amplitude than signal two. But that even reversed over here. And that's really just by noise. That's purely just by chance. We can run it again. And it's probably going to look even different to see how it looks now. OK, so now what happened to, uh, to go in the right direction? OK, so now we do permutation testing and in fact, this cell is is copied and pasted from a previous video, so I'm not going to talk much about this and then cluster sizes under the null hypothesis. This is mostly copied and pasted from previous video. There's only one change that I had to make, and that change is that this line wasn't here. So this if is empty and that wasn't here and it was just like this. This was from the previous video on permutation testing for cluster correction and simulated data. And now if I run this cell, we can see if it happens. OK, we happen to get an error. The error message is an assignment bubble. The elements must be the same. So that's a little bit of a weird error. Usually when there's an error and there's a for loop, the first thing I do is check whether the error occurred on the first iteration. If it happens on the very first iteration, then it usually means that there's a programming mistake somewhere. But that didn't happen. It actually successfully completed 41 out of a thousand permutations. So there's nothing wrong with this loop per say. Otherwise it would have crashed on the first instance. So what's happening here is that there happened to be no islands, so in this Z fake defecting to let me actually do some plotting. So here I generate. The Z values for this particular iteration of of the permutation testing and that looks like this and say plot time, but it's said fake. So these are all real values. These are Z values. There's nothing really wrong with them. Everything looks fine. In fact, it's so fine. Everything is so OK that this worked exactly how you would expect it to work. Theoretically, this is random data from random permutations and not a single time point anywhere in this time series is supressed threshold. So when I do this, look at the threshold, basically this is one point nine six and there is not a single time point that exceeded one point nine six on either tail. So that means this ends up being all zeros once I get rid of all of the sub threshold time points. So I can even say some said the fake here and the sun is zero because all of these points are zero. So now we get the islands and of course, there are no islands, no objects, zero. This Pixel IdeaCast list, Sellery is an empty array. So now we try to extract the lengths of all the elements in this cell array, but there are no elements in this cell array. So the cluster numbers, this is just an empty matrix and then you can't take the maximum of an empty matrix. That doesn't make any sense. So therefore, I had to add this extra if statement to say if not is empty. So we can see now that this is empty, which means we actually don't want to compute this line. So I invert this boolean test, this conditional test. Now, that is not a problem because the real answer for this iteration, the real answer is that the cluster size under the null hypothesis or the, I should say, the number of of time points in the largest cluster under the null hypothesis for this iteration was zero. And because this cluster size vector is initialized as zeros, skipping this line of code here because this is empty actually means that that value for this iteration remains at zero. So everything is fine. We just need to add this little conditional here. OK, so with that, all this code works and then we're going to get a histogram of the cluster sizes under the null hypothesis. Now it's interesting to think about why we needed this statement and at the same time why this histogram is so much larger than in that other video on cluster correction and simulated data in that video. We didn't need this. There were always clusters. There was never a case where there were zero clusters. On the other hand, all the clusters were tiny. There were one pixel, two pixels, three pixels. Every once in a while you get a cluster with five pixels contiguously, you know, five, 10 points. But that was really rare. And now here we're either getting zero pixels. You can see this distribution. It's kind of bimodal, distributed, if you want to call this a mode. So there's either zero clusters or there's many sorry, zero time points in the cluster or there's many time points in the cluster. And there really isn't a lot in between. So that's pretty interesting to think about. And this is the result of the smoothing. This is what happens when you use smoother data that have a strong autocorrelation. You basically get nothing or a lot of something. OK, so now. So this is also, you know, this is more more relevant. This is more realistic. So then let's see. So then we remove the small clusters from the real threshold of data. And I believe this is also probably exactly the same code. I don't think I changed anything in this code compared to in previous videos. So and again, it's pretty interesting to see that we still get this large effect, but this small effect disappears. And that's not because it's not a real effect. And in fact, you can see it here. It's exactly the right time. So the difference is that exactly. It's centered at two seconds and then it goes a little bit, you know, for some tens of milliseconds. It goes on both sides. But here you see that that effect is really there. We know we simulated the data. We know that this effect really should be the zoom in and we'll look at this. This is a genuine effect, however, when we apply Kluster correction, the effect disappeared here and of course, it's also disappeared here. So this is something to keep in mind when you apply Kluster correction, that essentially you are selecting for larger clusters and selecting out smaller clusters, even if those are real effects in the data that's built into the assumption of cluster correction, that a real effect has to be sufficiently wide anyway. Now that you have all this code, I encourage you to go back and play around with the parameters. You can try changing the noise characteristics like the frequency of full with the maximum. So the spectral or the the smoothing characteristics of the signal and the amplitude of the noise of the variance, the noise relative to the variance of the signal which is specified up here. And you can also try changing the full with Ralph Máxima of these two signals and see if you make, for example, this one wider, if it becomes more likely to be observed significantly in the resulting test statistic.