 Now that you have a handle on the basic implementation of permutation testing, it's time to move on to something slightly more involved, which is permutation testing and entire time series instead of just a single variable. So what we're going to do in this video is create two signals and I'm going to add noise to those signals. And then we are going to see if those two signals are statistically significantly different from each other, I should say, where those two signals are statistically significantly different from each other. So we start off by defining our simulation parameters. Now, I'm setting up this signal slightly differently from how I've set up most of the other simulated time series data in this course. And that is typically I first start by specifying the sampling rate and then I will start specifying the time vector and other things here. I'm starting by specifying the total number of time points, two thousand five hundred and one. And then I create a time vector that is linearly spaced numbers between minus one and plus three and this number of points. So 25 01 points. Now the thing is at this point here, once we run these two lines of code, we don't actually know or let's say we didn't explicitly specify what the sampling rate is. However, the sampling rate is implicitly determined by the boundaries of minus one and three, assuming these are seconds and by the number of points. So whatever is the sampling rate that produces 25 or one points in between minus one and three, that is our sampling rate. So now we can compute the sampling rate empirically by looking at the difference between two successive time points. So you could say, let me actually run this code here. So now the difference between one time point and the next, you could say it's time to minus time one. So it's point zero one six seconds. You could also do something like diff time one to two. So this is the difference, the discrete derivative between the first two time points, that gives you the same thing. Here, I'm actually getting the difference, the discrete derivative from the entire time series, no surprise all of these values are exactly the same. And then I take the average of them. And now this might seem a little redundant, but this actually is useful if you're working in empirical data. Sometimes you have a time vector, but you don't explicitly have the sampling right now. The thing is, in some aquisition systems, there might be really small time skips or tiny temporal errors. So therefore, if you have a empirical time vector, it's sometimes more useful or more accurate to say mean this time instead of just the difference between the first two time points anyway. So it happens to be that with twenty five oh one time points going from minus one to plus three, that results in a sampling rate of six hundred and twenty five hertz. OK, let's see, so now we're going to specify one hundred trials, which actually means two hundred trials in total because it's going to be one hundred trials per condition. Here's some parameters for the permutation testing. We're going to do a thousand iterations and we will use an alpha threshold of point zero five. Now, skipping this for a second and looking down here, we can see what this is doing on creating these two peer signals, it's two lines of code. It's actually really one line of code, but it's split up into two lines using this ellipsis here. And you can see this is a galaxy and you'll recognize the Gaussian formula here, the difference between or the differences between this first Gaussian and the second Gaussian, which are some together, are the peak time. So this first Gaussian is going to peak at one second. This is going to peak at two seconds. And the full with that half maximum is also different between these two. And also the amplitude is different. So basically, this is going to give us a signal that is two Gaussians. One is earlier, larger and wider, and the second one is later narrower and a little bit smaller. OK, so here's the two signals and then they're going to be plotted here and all this. So here you see what these two signals look like and now the idea is that I'm going to add noise to these signals. So here's where I create a signal to talk about what this car is doing. In a second here, I add some random numbers times, the noise, amplitude, and then the idea is basically just to generate a bunch of trials that shows one trial. So going to generate one hundred trials for each of these signals and add a bunch of noise. And then we're going to test statistically whether this the amplitude of signal one differs from the amplitude of signal two. And you can see that we expect the differences to be, you know, maybe starting around point five to somewhere around one point four maybe. And then, of course, here should also be some significant differences. All right, so a quick word about how I generate this data set, because this turns out to be a time by trial data set for both of these are time by Charles Matrix. I mean, for both of these data sets, the way that I did that without using a for loop, without using a function, Redmap is through a procedure called the out of product. The outer product is a linear algebra procedure. If you're curious, of course, I encourage you to take my linear algebra course, but you don't need that. Essentially, we're just doing a trick of multiplying the signal as a column vector by a row vector. That is all ones and that's this on its own creates a matrix. And then I add another matrix of random numbers. OK, so let's see. I'm going to run this entire cell again here you see poor signals here you see the signal plus the noise, and this is the average over all of 100 trials. All right. So now we go down to permutation testing. Now, permutation testing, the mechanism of permutation testing isn't really any different here than it was in the previous videos where we had a simpler case. However, we do have to think a little bit more carefully about how we're going to set up the permutation testing, in particular with regards to shuffling the trials across the two conditions. So first, let's think about the null hypothesis. The null hypothesis here is that these two signals have equal amplitude and that predicts that null hypothesis predicts that we could take any one trial from dataset one and swap it from any other trial from data set to. And the resulting test statistic wouldn't really differ except for sampling variability and noise and so forth. That's a pretty clear, null hypothesis. The question is how to implement that null hypothesis. There's a few different ways to do it. I think one easy way, which is what I'm doing here, is to concatenate the data into one single matrix that's going to have to end trial. So in this case, two hundred trials. So I'm concatenating along the second dimension data set once a second dimension's trial stays at one end, data set two. And that means we get this matrix data set data three D. I don't actually know why I called this variable three D because it's not actually a three D matrix, but I guess this is data one and data two. So this is data three anyway. So this is a twenty five hundred by two hundred matrix and the first hundred trials are from data one and the second one hundred trials are from data two. So now what I'm going to do is generate a vector of conditioned labels and I do that here with logical indexing. So I say one through two times NP, which is. So this is one to two hundred. And then find which of these elements or test whether each of these elements is greater than 100. So let me show you what this vector looks like. You can see it's a vector, it's a boolean vector. It starts off as all zeros. And then at element one hundred and one, it switches to one. And we can confirm that this is correct by saying mean con labels. And now if exactly half of these are zero and exactly half of these are one, then the average of the entire vector should be point five, which indeed it is. These are the kinds of little coding sanity checks that it's always good for you to do. For example, one thing that one mistake that would be easy to make, let's say you did, is equal to or greater than the number of troops. I think it's equal saying goes like this. Yeah, OK, so now I said equal to or greater than the number of trials. And now if I take the average again, you can see that this is not exactly zero point five. This is biased. So we have one trial, too many. All right. So let me run this again here. I'm initializing permutation differences. Actually, let me first walk you through this for loop here. So we're going to loop over iterations in the permutation testing, generate fake condition labels, and I'll explain what this line does in a minute. Then I compute the average over time of data set data three D and this is going to be all the time points and only some of the trial. So trials where this variable fake is zero. And and then again for me, two we get we're fake equals one. So this is going to give us two time series and then I just subtract those. And this is the thing that we are using to build up our null hypothesis distribution. Now, in the previous couple of videos, we were generating null hypothesis distributions for a single variable, so all we needed there was zeros by and Perm's. But here we want to create an entire time series over each each iteration in permutation testing. So now we get a matrix that time by permutation and iterations. OK, so now that said, let me initialize this to be one, and now let me explain what I'm doing here. So I take all of the existing condition labels which remember are 100 zeros than one hundred ones. And then what I'm doing is indexing, using this function Rand Perm, which will randomly permute integers between one and the input. So this is 200. So here you can see what this looks like. These are just the numbers, one through 200 and they're kind of randomly ordered. So you can see if I run it again, I get a different random ordering. So now this vector fake cons list contains the same exact number of of zeros and ones. We can confirm this with mean fake conservation's still be point five, but you can see they no longer respect the ordering of all zeros and old ones. So this is the mechanism that I'm using to randomly swap trials from signal one with trials or I should say dataset one with data set to. This is also a bit different from how I randomized the conditions in the previous video where I just generated a random sequence of zeros and ones. So that would be more sampling with replacement and this would be sampling without replacement. All right, so to this, we generate all these this whole permutation distribution of these differences between basically this except with the null hypothesis. Now, the rest of this stuff is basically the same as in the previous video. So, again, we compute a Z score, so that's this one. So we say the observed difference, minus the mean of the permutations divided by the standard deviation of the permutations. The important here to make sure that you are computing the mean and the standard deviation over the correct dimension. So it's two, not the first dimension, which would be time. And here's the real difference. So you can see this is the mean of the data set now here. Instead of saying fake Kohn's, I'm using the actual the true condition labels. So this is the real difference from the real data set. All right. So let me compute this actually did I do. And I'll just run all of this here. OK, and now I'd like to show you what this looks like. Let's see figure and then I'd say plotts time by Z diff. So here is the this is time, of course, in the X axis. This is now Z values or statistical Z values. And the so the Y axis is basically encoding at each time point. How far away from the center of the null hypothesis distribution is the test statistic at this or is the observed condition difference at each time point. So we get a time series of statistical values and you can see it's quite noisy and I'm going to address this a little bit in the project and towards the end of this section of the course. But you can see that it respects the general outline of the condition differences, so it was a big one here and a smaller one here, and that's basically reflecting this difference here versus this little difference over here. OK, so here I compute the observed test statistic using this function norm in which I believe is in the statistics toolbox to see which norm. And so if you don't have the statistics toolbox, then no problem, you can just set the threshold to be one point ninety six. So you can actually just say one point nine six and then don't worry about the rest of this. OK, and note also that I'm dividing the P value by two, because I actually want to perform a two tailed test. So if the P value is point of five and you want to test whether signal one is greater than signal or I should say signal one is not equal to signal two, then you actually have to test both tails of the distribution, which means that you have to divide the P value by two for the two tails. OK, and then here what I'm doing is thresholding this. So I want to say any value in this Z score distribution that is smaller than the significance threshold, which is one point ninety six. So that's right about here. So any value that is smaller than here should be considered not statistically significant. And the same thing also goes for the minus because we're doing two tails here. So anything that is larger than maybe I should say this way, any Z values in this Time series that are between minus point one, one point nine six and plus one point ninety six or any value in this range, these should be wiped out. These are not to be considered statistically significant. So that's why I say absolute value of Z Thresh is less than the significance threshold and set those to be equal to zero. And I think I. Yeah, well OK. So I'll run. I'll run this so you can see what that looks like, say, plot time by thrush, so now you can see all of these non significant or sub threshold values have been turned into zero. And now I do some plotting that's going to go back and figure one. And essentially what I'm doing is drawing dots around each point that is super threshold that is beyond the threshold. So there's a couple of remarks I would like to make here. One is that Matlab is giving us a warning here. It's saying that performance can be improved by using logical indexing instead of find. And that's basically what I'm doing here. So I'm converting this into a logical variable which is going to convert this from being all zeros and non-zero values in this case. This is the actual Z value. And then if I say logical this, then all these zeros become falsies and all of these non-zero values become true. Now we can go. And this has turned into one. And you can see there's no longer any numbers in here. This is actually true and false. This is Boolean. OK, so this is a little bit more efficient. It's a little faster. So that's why Matlab is giving us this suggestion here. The other thing I would like to point out is that some of these results should look a little bit weird. Maybe you don't trust them. It seems like these shouldn't be here. And indeed, that is the case. The two time series really don't differ out here. They don't really differ out here. They really only differ around here and here. So what is going on with these points? Well, it turns out these are what's called alpha errors. These are statistical errors. They are incorrectly labeled as statistically significant. And this is really driven by a combination of noise and sampling variability or really just noise in this case, in the two data sets, plus a relatively liberal threshold. So you might not think that point of five is a liberal threshold because that's a pretty typical statistical threshold. However, we are performing this test independently on two thousand five hundred and one. Right. It was a 2005 down to 2500 data points. So we have a big multiple comparisons problem here. So this P value is actually really, really stringent. Now, you know, you could try Barnstone correction, but as I explained in my YouTube videos, Bonnefoy only correction is actually not appropriate because of the temporal autocorrelation in these signals, although the noise doesn't suffer from that. So in terms of the noise, Barnstone correction would be appropriate and valid in terms of the signal. BARNSTONE correction is actually not a valid statistical correction here. So the way to or one way to deal with this is by making an assumption that because we know these data are smooth, then we assume that any individually significant points should not be considered significant. Instead, we should only consider significant if they're in a cluster. So if there's a lot of them next to each other. So that would eliminate a lot of these individual dots and it would preserve a lot of these dots and these dots, which are really tightly clustered together.