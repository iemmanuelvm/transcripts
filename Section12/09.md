 So here are my solutions for this project and let's see, some of the code in this first cell is basically what you've seen before. As I mentioned, when introducing this project, you want to generate basically two time series, each of which having two features in them. So that means that you need four parameters. So two parameters for the signal one. So then A and B correspond to the first feature of signal one and the second feature of signal two. And then also for signal two, they end up with quite a lot of parameters. I hope these are sensible. This corresponds to the peak frequency, the peak amplitude, sorry, the peak timing, the peak amplitude, the full with the half maximum and the noise standard deviation, which I'm actually leaving as constant for both signals. So I realize that there is many, many parameters here. But this is good. This is intentional. I would like you to have this script that you can use to really explore and understand time, frequency analysis and permutation testing and cluster correction with time frequency data where you know what the ground truth is, you know what the results should be. And so being able to modify easily all these different simulation parameters will really facilitate your learning. OK, let's see. So that's basically it. So here I start creating the Gaussian window and define the exponential decay parameter. This is for the noise because we're creating pink noise, one of Reft noise here. OK, so I'll run all the code in this cell and here's where I create the two data sets. So I do this trial with loop over trials, create the first data set here and then the second data set here. And here's where I create the two components of the signal. So you can see it's some amplitude times the cosine with a random phase value. So these are non phase lock signals that we're generating and then all of this business. So this pure cosine wave is multiplied by the Gaussian sets for A and B, so these two features and then you can see they are some together here to create one signal. And then I add noise, that signal and this noise is just the one of ralfe noise. It's the same as what you saw in one of the videos on Pink Noise or one of ref noise in the first section of this course on simulating data. So let me see now. I'm going to run this cell. So you see what? So you can see what a single trial of data looks like. It's same time by actually this will be so one, eh? So here is the first part of the signal and I'll the time by seg one B so now you see the two components of the signal. Of course these get some together and now I can also show you what the noise looks like. And here we get the noise. So then we sum all of these together and that gives us our signal for one trial from data set one so all the time points and this trial. And here is what the resulting Time series looks like. Now, when you know what to look for, you can see here is one part of the signal and here's the other part of the signal and then the rest is noise. But if you didn't know what to look for, you would really just think that this is a lot of noise. And I think this looks pretty typical for real EEG or M.G. data in the sense that the noise is about the same level, the same scale as the actual signal itself. So let's see. So here everything is copied and pasted, but every time there's a one up here, it's replaced the two. So we have to stick to AMP to and so on. All these parameters have to be changed. This is, of course, a really easy mistake to make. If you forget to change one of the parameters like this, it's really common mistake. I make this sort of thing relatively frequently. OK, but of course I hope I catch all these mistakes anyway. So that creates the data. And then here this is just a little bit of plotting. So you see what the average looks like. Now, this is pretty interesting to look to look at. You actually don't see the signals. You really can't pick out the signals from the noise. You notice the amplitude is rather small compared to, for example, here where you see the amplitude actually. Then they even played it like this. So here you see the amplitude of the signals themselves is in the range of, you know, going up to four. And here for this case, it goes almost up to two and versus when I plot the data average over all the trials. Now, the amplitude is tiny. It's an order of magnitude smaller. And the reason is because this is all non phase lock signals, non phase locked. Random phases are adding random numbers between zero and two pi. All right. So now we want to do a time frequency analysis and implement. This via wavelet convolution and let's see, so I don't don't want to get too much into a discussion about this cell because this is really just copied and pasted from the previous section of this course on time frequency analysis. The main thing is to point out that I'm defining the wavelets in a number of sorry, not in the number of cycles, but as full with that half maximum to go from one cycle at the lowest frequency, up to two cycles at the highest frequency. And of course, these are parameters that you can change and see what the what impact they have on the results. So let's see the rest. This is fairly straightforward. Create the wavelet, normalize it in the frequency domain element Y's multiplication between the spectrum of the wavelet and the spectrum on the data. Take the inverse Fourier transform reshape back to time by trials. Now here you have to be careful that everything you're doing is number of trials times too. You can see that also here. So I create the time frequency matrix as frequencies by time by two times this variable and trials. Now, first of all, I need to save all the single trial data, because we're going to need that for a permutation testing. And remember, it's two times the number of trials because I've specified here that it's in trials per condition. So that means that in the entire dataset, we actually have 200 trials. OK, so let's see, where were we down here and now what's missing here? Well, not missing, but what's different here from most of the time when you're doing time frequency analysis is that here I'm not taking the average, so I'm not averaging over trials. I just get all of the single trial single time point power data. OK, and then here I'm going to make an image so you can see what the time frequency spectrum look like for these two conditions. So here you go. Condition one and condition two. Here you see the two features clearly identifiable in the time frequency plane. And you can also see the one of RF noise spectrum. So in general, so when there are no features present in the signal, you can look at the noise and you can see its brighter colors here going into more bluish colors down here. And this reflects the one over F power distribution. So as frequency is increasing, the noise amplitude, the amplitude of the noise spectrum is decreasing. OK, and then it's also interesting to go back and confirm against these parameters that what you see in these plots matches what is specified here. So, for example, data set, one has the two features at six hertz and 15 hertz. And here you see this at six hertz and this one at 15 hertz. All right. So now we are ready for the permutation testing and the mechanism of permutation testing is the same as what you've seen in previous videos. In this section of the course. You just have to keep in mind that everything is now multidimensional. All right, so then I'm going to run through this cell, this will take a little bit longer than it takes for previous videos where it's just a simple permutation testing with one variable or maybe with a time series. So whenever you're dealing with larger data sets, you're always going to have to be a little bit more patient for all of these permutations. One thing you can do to speed up this code, if it's working too slowly for you, for example, if you have hundreds and hundreds of trials or if you're also doing this over many channels, is that you can do temporal downsampling. So post analysis, temporal downsampling, as I've discussed in previous videos, in earlier sections of this course. All right, and here is where I do some plotting before Kluster correction is applied. So and this is basically what I've explained in the video, the previous video introducing this project, so it's not a whole lot of interesting stuff happening here. Well, of course, it's all super interesting, but there isn't a whole lot of new things that are happening in here that I think would be particularly challenging that you couldn't just copy from previous videos. So, again, here is the raw difference and here's the Z map difference. And it's interesting to compare these two because there are features in this plot that you really don't see in this plot. Like this thing is kind of sort of they're a little bit. And this blue thing here, you also really don't see. And a lot of these other these are subtle features which are, by the way, not in the signal. So all this stuff is just coming from the noise and that you don't see here either. Now, the key difference is that this is a segment that also incorporates the variance into the results, which is important. So here you just see the mean difference is here you also see or you see the mean difference is scaled by variances. So technically, anything that's in here is kind of also in here, but this is actually a better way to visualize the results. OK, here you see it, Thresholding, and this is the same thresholding map with outlines drawn around it just to make it a little more a little more visible. All right, so then we get to the cluster sizes for cluster based permutation testing. Now, as I mentioned, this is a little tricky because you have to do this separately for the positive clusters and for the negative clusters. Oh, yeah, that was one thing I forgot to mention. So the reason why this is red and this is blue is that you're subtracting words, that you're subtracting this map from this map. And although raw power cannot be negative, we are dealing with a condition difference. And of course, differences can be negative. So really what's happening is that this feature is higher in frequency. This feature is lower in frequency. So when you subtract the two, you're going to get red and then blue or blue and then red, depending on which one is subtracted from each other. One. OK, so you have to do the cluster correction separately for positive and negative, which means more power for commission one or more power for commission two. And to do that, you actually need to threshold these maps separately for positive values, clusters versus negative value clusters. Right. So then we get down here and extracting the again, so now it's not the 90th percentile, but it should be the ninety seven point five percentile. And so that gives us separate cluster seperate cluster thresholds for the same positive clusters and negative clusters. But really, it's the clusters that contain positive statistical values and clusters that contain negative statistical value. So you can see that the threshold is kind of similar. So they're both in the same general order of magnitude, but it's 2500 versus 2200. And that basically just reflects that this tale is a bit longer and this tail is a little bit less pronounced here. OK, so then those are finally used for thresholding the results. And again, you have to threshold the map separately for the positive clusters versus for the negative clusters. So conceptually, this project isn't so difficult relative to what you've learned so far up to this point. And of course, but there's a lot to keep track of with the positive and the negative clusters and checking for coding mistakes and all sorts of things. So it does get a little bit tricky at the end. And now we can see the final results, which looks like this. And it's pretty interesting to see that this and this. So these two features are not statistically significant. They are definitely robust. Let's see if we look at these two things. So it's pretty clear that these look like separate features of the data. And that's really happening because one signal has a higher frequency feature relative to the same feature in the other signal. And these regions are also significant. So if you look here, so this thing and this thing or you see it most clearly here, I think so before applying correction for multiple comparisons via a cluster correction, these features are super thresholds. So they survived the threshold, the significant threshold, but the clusters under the null hypothesis ended up being sufficiently large that these features were favored and these features ended up being relatively small. So that's why these ended up not being outlined here in this statistical significance plot. So these two were not statistically significant with the application of cluster correction.