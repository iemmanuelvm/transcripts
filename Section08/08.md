In the previous video, you saw the effects of different wavelet parameters, in particular the number

of cycles or the full with the maximum on the time frequency power plot from real data.

Now, as I've mentioned in the first section of this course, I'm a big fan of looking at analysis results

in real data because it gives you a taste of how things really look.

It's kind of difficult to simulate really realistic EEG or any kind of other data.

On the other hand, there's also a major advantage of simulating data because you know the ground truth

and you can directly compare the results against the simulated data, which is your ground truth.

So that's what we're going to do in this video.

This is basically a repeat of the previous video, except here we are using simulated data instead of

real data.

Now, it's going to be a little bit different, but it's the same concept.

So we're going to start by simulating data and then let's see, we're going to reanalyze the data.

So extract time, frequency information from the signal using three different wavelet parameters.

So here this is the probably the other main difference from the previous video.

I'm specifying in full with the maximum.

And this goes in seconds.

And that's mainly just to, you know, mix things up a little bit and then there'll be some plotting

and we can compare against the ground truth simulation.

All right.

So I'm simulating data as a transient oscillation.

I've discussed this in the beginning of the course.

But essentially, I'm going to create a Gaussian with a certain full with the maximum, in this case

400 hundred milliseconds and then a cosine wave.

And you can replace this with a sign, if you like, that's just to make it look symmetric, which I

think just kind of visually looks a little bit nicer.

And then the signal is simply the element Y's multiplication of the sine wave and the Gaussian.

So that you see here.

What I'm also extracting here is the analytic envelope, the amplitude envelope that you see in the

red dash line here, and that's computed as the magnitude of the Hilbert transform of the signal.

Now, in this case, this happens to work really well, just this code on its own, because this is

already a narrowband signal.

You will learn in a later video in this section of the course about Narrabeen filtering in the Hilbert

transform, that this kind of operation is valid.

It's interpretable only when the signal is already narrowband.

So this is a real data or a broadband signal.

You would want to Narrabeen filter the signal first.

Anyway, this gives us the amplitude envelope of the signal and that is going to be useful to compare

against the results of the time frequency decomposition.

All right.

So now what I'm going to do is generate a family of than wavelets from two hearts to 20 hertz in 50,

let's say, linearly spaced steps.

And I'm going to make three of these families actually, one with a four with the maximum of one hundred

milliseconds, five hundred milliseconds and two seconds.

So then the rest of the code in this cell is not super new, but these are pretty standard stuff that

you've seen before.

Here I create the complex Morleigh wavelet, and it's really important when you're creating the more

they wavelet, because now, you know, in this course, I'm kind of flexibly moving back and forth

between the two definitions of the Gaussian to use in the wavelet.

This definition, which takes the full with it maximum as a key parameter or the other version, which

takes the number of cycles as the key with parameter.

Of course, both are valid.

Both are fine.

Just make sure that you know which one you're applying.

All right.

So let's see.

And then we do convolution and extract the excess power.

But actually, I'm only doing amplitude here.

So let's you run this.

So now I'm going to plot some results.

I'm going to go through this piece by piece.

So let's see what the time frequency decomposition looks like.

So the results are quite striking, quite different from each other.

This is the type of frequency decomposition with a very narrow set of wavelets.

A I don't know, we can call this intermediate, I guess, and a wide set of wavelets.

And it's really interesting to see that the same exact signal, remember, the signal is the same and

all of these cases and with a very narrow wavelet, you get stretching in frequency, but it's very

narrow in time here.

You got something that's very narrow in frequency, but you get a lot of smoothing or smearing in time.

And in this particular case, it looks like point five seconds for what the F maximum finds a really

nice balance.

This looks like a real ball in the middle of this time.

Frequency plane.

OK, so now what we can do is plot the amplitude envelopes at the peak frequency.

So basically I'm going to extract from this time frequency plot.

I'm going to extract the time course of the.

Amplitude at 10 hertz.

So here I find which frequency corresponds to 10 hertz, so this was the parameter in Hertz to create

the signal.

This is the vector of frequencies that I used in the wavelet convolution.

And then this gives me the index into frequencies that's closest to 10 hertz.

Right.

And then I do some plotting and maybe I'll plot this first and then explain it.

OK, so what you see here are four lines.

You can see the three thin, solid lines correspond to the data from these decompositions.

So the blue line is for this one.

And then the red line is for this one.

And the yellow orange line is from this one.

So now you could look at this and the black dotted line is the ground truth.

That is the actual signal.

This dotted black line is exactly the same thing as this red dash line over here.

And you can see that here.

So I'm doing a little bit of amplitude scaling for the visualization, but that doesn't really matter.

So you could ask the question, which one is the best, which set of parameters or which parameter here

for the wavelet gets the results to match the original signal best.

So the answer is pretty clear from this plot.

It seems like it's pretty clear and that is the narrowest possible Gaussian for the wavelet.

So the blue line and the black line overlap almost perfectly.

But and there's always a but this comes at a trade off.

So now what I'm going to do, I'm going to plot over here instead of making a plot in the time domain,

I'm going to make a plot in the frequency domain.

So to generate these lines, basically, I took a slice through these plots going this way, so horizontal.

And now what I'm going to do is slice through the plots this way.

So we're going to look at time zero and I'm picking time zero, of course, because that's the center

time point of the the transient oscillation that I've generated here.

So the frequency structure at time, zero.

So it's going to slice through like this and then that we can compare to, again, the ground truth

and I get the ground truth by taking the first four eight transform the signal and then extracting the

amplitude from that.

OK, and that looks here.

Now remember, these colors are matched.

So the blue line here, which kind of look like it was the best.

Now it looks like it's the worst.

It's ugly.

It's awful.

It's so widespread.

We even have non-zero energy all the way down here to two hertz and all the way up here to 20 hertz.

And that is basically a visual representation of this smearing here.

So you can see this kind of cloud of uncertainty goes all the way down here and nearly all the way up.

I guess it actually does go all the way up, probably even a little bit past 20 hertz.

Now, in this case, in the frequency domain, it looks like.

Well, now it looks like it's pretty clear that the two second long Gaussian, which is a really, really

wide wavelet.

So two seconds, that actually is the best approximation.

That gives us the closest match to the ground truth signal.

So this is maybe a little bit difficult to reconcile because what looks good in the time domain looks

terrible in the frequency domain and what looks good in the frequency domain looks really awful in the

time domain.

Now, this is simply the nature of frequency analysis, and I would even argue that this is good if

you search around, particularly in like the engineering world and signal sort of radio signal processing,

there are methods to more accurately estimate both temporal precision and spectral precision.

However, compared to Morleigh Wavelet convolution, because it is known that Morleigh Wavelet convolution

induces smearing and you either get more smearing in the frequency plane or more smearing and the time

of plane.

However, I would actually argue that this is good.

You want this amount of smearing, particularly for biological signals, where there's going to be a

lot of variability.

There's going to be differences both within and individual differences across trials and also differences

across different individuals.

So I think having really, really great precision in both time and in frequency is actually not desirable

in biological signals.

Biological signals are inherently noisy and variable rate.

That's part of biology as part of humans, part of how the brain works and the analysis methods that

we apply to the signals should be able to match that level of ambiguity.

And that happens basically by smoothing.

So I think you can agree that the red line here, so corresponding to wavelet with point five seconds

full with the half maximum is a pretty decent balance.

It's a good but not perfect fit in the time domain, and it's a good but not perfect fit in the frequency

domain.

Now, let me also be clear that I'm not saying that a constant width of point, five seconds for with

the maximum, for all the frequencies, for, you know, every analysis that you're going to do is necessarily

the optimal parameter.

In fact, I didn't even select this parameter to be, you know, quote unquote optimal for this particular

experiment.

The more general point that I'd like to make here is that there are no optimal parameters.

What might seem like the best parameter to optimize one feature of the data is likely to be suboptimal

for another feature of the data.