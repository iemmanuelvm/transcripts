 The permutation testing framework provides two methods for appropriate multiple comparisons corrections of time, frequency data or other kinds of data that have a strongly correlated spatial temporal structure. Those two methods are cluster correction, so correcting for clusters and extreme pixel correction, which is looking at distributions of large pixel values. In this video, I'm going to talk about cluster correction. And then in the next video, I'll talk about pixel value based correction. So here we are, cluster based correction of multiple comparisons. I will talk about the assumption underlying cluster correction. That's an important assumption to keep in mind. And then I'll talk about the mechanisms of how it actually works. So let us start with the assumption. The idea of cluster correction is we start from the assumption that activity on the time frequency plane. So some feature on the time frequency plane is meaningful. It is interpretable. It's statistically significant if it is big enough, if it's in a cluster that is sufficiently large. So what is a cluster? A cluster is a contiguously significant region. Any time frequency plane now big enough. Of course, this is where the main ambiguity comes into play because what does it mean for Cluster to be big enough? How big does it have to be to be big enough that we find it meaningful? Now, we could say maybe it's some number of time points or maybe a cluster on a time frequency plane. Let me go back to showing this time frequency map. Maybe we say that a cluster is meaningful. If it is, how about two hundred and thirty seven milliseconds? So that's, you know, about this length. So this cluster is meaningful because it's more than two hundred and thirty seven milliseconds. This cluster is more than two. But this cluster here, this is not this is less than two hundred and thirty seven milliseconds. So therefore we do not believe that this thing is real. OK, obviously you can see this is problematic because this is arbitrary. Why do we pick two hundred and thirty seven time points? Why not two hundred and thirty eight time points. And we could go through the same thing for the number of frequency bins. Maybe a cluster has to spend at least five hertz. But where does that number five hertz come from? Why not four point seven three nine hertz? OK, so obviously the definition of big enough should not come from some arbitrary number of time points or arbitrary number of frequency bins. There's there's no way that some, you know, some cut off like this is going to be sufficient in a general case. So the solution, therefore, is that we use permutation testing to derive a cluster size from the null hypothesis, empirical distribution that we are creating during permutation testing. So that means that we are going to let the data tell us what the appropriate cluster size should be. So that is pretty cool because now if we find that a cluster has to be two hundred and thirty seven milliseconds long, that's not some weird, arbitrary number that we came up with. That is a threshold that was determined in a blind fashion from the permutation testing. OK, so I hope that makes sense, that assumption and the motivation for cluster correction. Let's talk now about how we actually do this. So the mechanism of cluster correction is you have to go through your iteration for your permutation testing. So you go through all of your one thousand iterations to get a set of permuted maps are shuffled time, frequency maps, and then you actually have to go back through them again. So you have to loop over all of your iterations twice, loop over the permutations twice. The first loop is where you create all of the Permuted maps and the second time you go through them, the second iteration involves getting the clusters. So what do you do here? So you go through four iteration one. This is the map, the time frequency map for the first iteration of permutation testing. Now this map was created assuming that the null hypothesis is true. So we actually don't expect any pixels in this map to be significant, to be super threshold. We expect all of the pixels here to be essentially zero plus whatever is some sampling variability. But of course, if you have this many data points, there might be hundreds or thousands of data points inside this map. And if you're running a uncorrected threshold at P equals point of five, it's very likely that you are going to get alpha errors. So here's what we assume. We assume that all of the cluster, all the pixels that are less than our. Value threshold here, so that pop up here, we assume that these are all alpha areas, we assume these are type one errors that are just appearing here by chance. So then we go through the entire map and we find the largest cluster. So in this particular iteration, that happens to be this one. This is the largest cluster. And now we say in this particular iteration, this is the largest cluster that occurred by chance. So we don't believe that this is a real cluster. This occurred by chance. So then we count the number of pixels in this cluster and then what do we do? Well, we go on to the second iteration, the second map, and then we do the same thing. So we apply our uncorrected P value threshold. Let's say it's between five and then we assume that all of these little pixels that are remaining are just statistical errors. These are flukes. So then again, we find the largest cluster in this entire map and we count the number of pixels. And then we say this is the largest pixel from this map outside of the largest cluster in this map that we expect under the null hypothesis. Obviously, you keep repeating this for all and iterations. Maybe you iterate one thousand times, you get a thousand maps, and that's going to give you a distribution of cluster sizes, 1000 cluster sizes, and then you can make a distribution that looks like this and then you pick the 90th percentile here. So then you say this is the point, zero five size of the number of voxels in clusters that we expect for the null hypothesis. And then you go back to your original data, which might look something like this, and then you go through all of these clusters in your original data, looks like here's another little cluster over here. And you say, is the number of pixels in this cluster greater then or smaller then this threshold here? And if the number of pixels here is larger than this threshold, then we keep it. And if it's smaller like this, maybe this however many pixels this is here, maybe that's all the way down here. Then we remove this from the map and we say that this is not significant. So that is the way that cluster correction is implemented in practice. Now, there's a few things that I would like to discuss about this method. First of all, it's important to keep in mind that cluster correction favors large clusters. So we are looking for large clusters. So that means potentially meaningful, potentially real effects that are relatively small in in size and in time frequency plane might be considered. You know, these might be lower than the threshold. These might be removed from your results, even if they are real, because the larger clusters will be favored. So there's one way to mitigate this a little bit, which is instead of just counting the total number of pixels in a cluster, you can weight the number of pixels according to the statistic values in that pixel. So let's imagine this is a TMAP. So these are all T values. So you can say the average of all of these T values and the average of all of these T values and the average of all of these T values rather than just the number of pixels. Now, the advantage of computing the average T value in a cluster is that you can also find small clusters like this one. Maybe small clusters that are really, really powerful will balance out with large clusters that have relatively small t values. So that's going to actually change the threshold. And you might find that this really, really big cluster, it has a lot of weak values, whereas this smaller cluster might actually be more you know, it might have more weight than this cluster because the T values in this cluster are much larger. And then, of course, keep in mind that used to take the absolute value of the T values, because what we want is a measure of the total mass of the largest cluster, and that should be independent of the sign of the statistic values inside that cluster. Another thing that I want to mention about cluster correction is why this is a good method. This is a good method because the total amount of smoothing that gets imposed on the time frequency result is a parameter that you can control, in particular the width of the wavelet or the corresponding parameters for Filter Hilbert or short time F.T.. And that's another reason why this arbitrary threshold of the number of time points, the number of frequency bins isn't a good threshold. You can arbitrarily make these clusters longer or wider so you can arbitrarily change the size of these clusters by changing your time frequency analysis parameters. However, when you're doing permutation, testing all of those parameters that you impose in your analysis, so the effect of your analysis parameters on the results, that is going to be held constant between the actual results and all of your permuted results. So if you impose more smoothing on the data, then these clusters in the real data will get larger. But these clusters in permutation testing will also get larger. So the threshold will increase in an appropriate way. So that's a really powerful feature of using the permutation testing framework for identifying cluster correction or multiple comparisons more generally. All right. So this is one method cluster correction. In the next video, I'm going to show you the other method that you can use from permutation testing, and that is extreme value based correction.