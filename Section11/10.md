 In this video, you are going to get to see extreme pixel based multiple comparisons, correction method in practice, it is going to be extremely awesome. Now, before running any of the code in this video, you need to make sure that you have run the code from the previous to Matlab videos in particular. That was the video about a permutation testing in real data. So where we compared a time frequency power between Channel six and Channel seven of the V1 data and then the last Matlab video, which I believe was two videos ago that was with Kluster. Correction. OK, and so if you run the code from those videos, then you already have all the variables that you need already in your workspace. So looking through this code here, we can see this is the main loop for this video. So we are looping again over permutations and we are extracting the one map. So the one null hypothesis map for this iteration, normalize it to Z score and then we are sorting all of the data. And I'll explain in a moment how this line of code works. And then we're going to store the largest negatives to the most extreme negative and the most extreme positive value. OK, so let's see. And first, we initialize the matrix of extreme values. You can see that's a number of permutations by two and it's two because we are going to extract the smallest value, the most negative value and the most positive value. OK, so let's see. We create this image, normalize it to Z score. And then what we're doing here is going back to this permutation. Mappa So that's funny. Well, let me first get through this line of code and then we'll discuss whether we even need to have these two lines of code. So we go back to the permutation map for this iteration. All time points, all or all frequencies, all the time points, reshape this into a vector and then we sort that vector. So let's see what that looks like, this variable temp. So I'm going to plot temp. And here's what it looks like, looks kind of neat and essentially what this is telling us is that most of these values in this map, so this is all of the time frequency values in the map, there are four thousand three hundred and seventy five of them. And that corresponds to the number of frequencies times a number of time points. We already discovered this number forty three, seventy five when discussing the number of tests that we were doing in permutation testing. So essentially what this is telling us is that most of the pixel values in this plot are in a small range, close to zero. And then there are some extreme values negative and some extreme values positive. And I'm also curious to see what this would look like as a histogram. So let's see histo histogram and we want temp. And how about one hundred bins? OK, so here you see the histogram again. It's a pretty tight distribution. Most of the values are clustered close to zero. And then we have the two tails. And it's interesting to see that the positive tail is much more pulled to the right than the left. The negative tail is to the left. In fact, the smallest value here is somewhere around minus five thousand and the largest value here is fifteen thousand. So three times as large as on this side. OK, so why why do we even have these two lines of code here? We certainly used that last time in cluster correction and there we needed that for thresholding the map. So we needed to come up with our statistical threshold. But here we're not actually thresholding the map. We don't care about any thresholds. All we want to do is get the most negative and the most positive value. So the two extremes from this map. And so, in fact, it turns out that we don't need this code at all. It is just confusing and redundant. I think that's why it's even there. OK, so now we want Maxwells for this iteration in permutation testing. We want the largest negative and largest positive from this variable temp. Now, there's two ways we could do this. We could say the minimum value of temp and then the maximum value of temp. And that's fine. That will work. Turns out that those two numbers are, uh. Yeah, well, five thousand and fourteen thousand. Almost fifteen thousand. But there's a simpler way to do this, and that's because temp is already sorted. So in fact, we don't need to explicitly find the minimum and the maximum. We already know that the minimum and maximum are the first and last points. So that makes our code, I think, a little bit easier to read like this. OK, there you go. Very nice. Now let's plot the distribution of these extreme pixel values. So, of course, this bimodal distributed. You're not going to get extreme values in the center. And actually looking at this distribution, this is now surprisingly symmetric. So there's a little you know, the distribution seems a bit taller here on the positive side. But if you look to the extremes, they seem to go to roughly the same extreme value negative and the the same value positive. Maybe that's not so surprising, given that this is all random stuff in here. I would also like to point out explicitly that this x axis here is not some normalized measure. It's not a Z score, it's not a P value. It is the raw power difference. It is in the same scale as the original data. So that's just something to keep in mind for below. OK, so now we want to find a appropriate threshold for this. So let's say the threshold is going to be zero point five or point zero five excuse me. And then we want to find the lower bound and the upper bound based on significance threshold here. So what I'm doing as I'm doing this separately for the low and the high threshold. So first I get all of these small values that corresponds to the left of this distribution. And then I take let's see, let's figure out how this code works here. So first of all, this is the same data as in Maxwells, but it's just different from the first column and then sorted. And then what I do here is compute this, score the threshold times, the number of permits, and that turns out to be 50. Now, this could be this doesn't have to be a an integer, so therefore we round it. So in this particular case, it still ends up being 50. Now, what does this mean? This tells us that the fiftieth element corresponds to zero point zero five, given the number of permutations that we have done. So a thousand permutations and then we get that index into temp and that tells us our threshold for the lower bound. So that's one point eight minus one point eight times ten to the four. And so that's going to be somewhere around here. So. This is our lower threshold now, again, we need to discuss details in statistical tests, notice that here I'm applying a P value threshold of zero point zero five and I'm applying point of five both to the negative tail and to the positive tail. So, in fact, that means that these thresholds that I am specifying here are five percent here and five percent here. So this is an effective P value of zero point zero point one, which is too liberal. So I'm going to change this to be zero point zero to five. And now we are getting two and a half percent on this side of the distribution, two and a half percent on the other side of the distribution. So I'm curious what this is going to go to. So it was one point eight and now it's minus two. So now the threshold moved from here to around here. OK, and then we repeat for the upper threshold, and this is almost the same, except here I'm doing one minus the P value. And that's because here we're looking for the top of the distribution and that threshold is also two. So in this case, they ended up two threshold are really, really similar to each other. And by the way, I also wanted to show you this code here, because this is a way to compute percentages of a distribution. So a percentile of a distribution if you do not have these statistics toolbox. So you might remember two videos ago in the cluster correction, we use the function percentile like this to identify the threshold in the cluster distribution, the null hypothesis, cluster distribution, that function percentiles in the stats toolbox. And this will give you exactly the same result if you do not have these stats toolbox. OK, so then we're just going to plot these things. So you can see where the actual thresholds are and then we are going to threshold the real data, so here we have I'm calling the Z map again. It's a little bit of a poor choice of variables, I admit, because this is not an actual Z map. This is the diff map. Everything we're doing here is in the same units of power that we got out of the wavelet convolution. OK, so we want to set all of the pixels between the two thresholds to zero. Now, that's probably not the correct way to do this because we're studying every pixel to be zero. So let's see. What we want to do is say the Z map where the Z map said map is greater than the thresh thresh low like this and set that to zero. So I haven't yet dealt with this upper threshold, but I want to walk through this one step at a time so you can see what we're doing. So we look through this Z map and let me let me open this up. So image C Z map and spin the axis around. So we want to say and let me put on a color color bar. So with this line of code says is look through this entire map, which is formerly not a Z value map, but it's the variable name is Z Map. And we want to find all the pixels that are greater than the low threshold, which is not even shown here. Actually, it's minus two times ten to the four. So all of those pixels are set to zero. So let's see what that does. Let's see what effect that has. And I'm going to run all of this plotting code here, OK? So here we have the the raw power map, the raw power difference map, the difference map with significant regions outlined in black contours. And here is the threshold in map. Now, nothing is significant and that's not surprising. The big significant regions here are positive, which means they're also above the lower threshold. OK, so let's add the conjunctive logical here, which is that Z map said map is less than fresh high. So any pixels that are simultaneously above the lower threshold and below the upper threshold and that corresponds to basically all the pixels in here with these values. Those get turned off, those get set to zero. So and then we will run this code again. And then pretty curious to see what this will look like. OK, this looks pretty cool. This result looks qualitatively similar to the result that we got from cluster correction. However, it's not exactly the same. When we did cluster correction, we found that it was still this cluster, but it was considerably bigger. You remember that it was like it was even all the way down here. I think this was included in there, but this cluster was certainly bigger. Now, the main difference is we're not actually doing any cluster ID here at all. No, this method doesn't care about clusters. Clusters are not taken into consideration. We're only considering the value at each individual pixel. OK, very interesting. I hope you enjoyed seeing extreme pixel correction put into practice. The very last thing I want to show is changing the color map to this color map called blue, white, red. So that looks something like this. So now white is let me put on a color bar here. So now white, the value of white corresponds to zero and then red corresponds to positive values and blue corresponds to negative values. Sometimes people use these plots. Are these color maps in neuroscience? I have mixed feelings about color maps. You know, some people out there in the world have very strong opinions about color maps. Personally, I don't, but that's OK. But I do think it's interesting to have this option available to you. It is a nice color map. It is not a matlab default. This function is packaged in the zip file that you downloaded for this section of the course. So in this video and also in the last Matlab video with cluster correction, I showed the same significance results in two different ways. I showed all of the power data, so all the raw different data and then this contour. And then here I blanked out everything that was non significant and I only showed the significant pixels. In the next video, I'm going to talk more about what are some ways to illustrate statistical significance in these kinds of plots. And I will give some recommendations about good ways and not so good ways to do it. See you soon.