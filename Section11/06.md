 When doing time frequency analyses, there are major problems with multiple comparisons, and that is because we are doing lots and lots and lots and lots and lots and lots of individual statistical tests on our time frequency pixels. Now, the standard approach in psychology and other related fields for addressing multiple comparisons is actually not appropriate for time frequency analysis. And I'm going to explain why. But first, I want to illustrate what exactly is the scale of the multiple comparisons problem. So let's take a time frequency analysis like this. We have imagine we have a thousand time points for the frequencies and 16 channels. It's just for one particular data set and that gives us six hundred and forty thousand tests. So six hundred and forty thousand individual little tests are correlation values or whatever kind of statistical test we are doing. That is a really, really large number. And, you know, in some sense, this is kind of a lowball estimate because I'm not even including different conditions. Let's say you have three different experimental conditions, so then you might want to do two different comparisons. So then all of this gets multiplied by two so you can easily have potentially over a million tests that you are doing. That's really a lot of multiple comparisons. Now, the standard approach in many fields for statistical adjustments of multiple comparisons is coming from this guy. Who is this guy? Well, you might not recognize him by face, but you possibly know his name from a stats course on dealing with multiple comparisons issues. So this guy is born Ferrone, I assume he was Italian. Sounds like an Italian name. Clearly, he was a very serious statistician. You can tell by the look on his face. Anyway, what is the formula for Barnstone correction? The formula for Barnstone correction is to take your threshold that you would typically use. So a P value threshold, let's say of five, standard one, and then you divide that threshold by N where N is the number of tests that you are implementing and that gives you the new P value threshold. So for example, if you have a threshold of zero point zero five and you're doing five tests, then your new P value threshold that you actually use would be point of five divided by five, which is zero point zero one. OK, so that is the bond for any correction. It is a simple and useful method. Now let us think about some problems with Bond, any correction. And I want to make clear that I'm putting problems in apology quotes here because these are not really problems with bond for any correction per say. The bond for any correction method itself is fine, but these are limitations. These are problems with bond for any correction as it applies to time, frequency analysis and not only time frequency analysis, but also other kinds of data sets where you have strong spatial temporal structure in the data. OK, so let's think about some of the problems with bond for any correction. And of course, I encourage you to pause the video and see if you can come up with some of these yourself. All right. So here we go. I have identified three issues with Bond Ferrone correction. One is that it is too stringent. Two is that it assumes independence. And three, and this is violated. That's the problem. And three is that it's based purely on NP and not on the information content in the data. So let's discuss each of these three issues in turn. So No one is that the threshold ends up being too stringent if we try to apply one Ferrone correction, appropriately correcting for all of the multiple comparisons you might get. In a simple example that I discussed a moment ago, you might have a bond Ferrone threshold of point zero five divided by six hundred and forty thousand, which is a ludicrously tiny number. Even real effects. True, absolutely. Real effects out there in the universe are unlikely to be significant using such a tiny, tiny threshold. The only effect that I would be confident would survive point zero five divided by six hundred thousand would be the difference in brain activity between living people and dead people that would probably survive, at least for most people. OK, so Barnstone is just too stringent. A second issue is that Bonfante correction assumes independence. That means that to apply Bonnefoy only correction, you assume that all of your data points are independent, uncorrelated with each other. That is obviously not the case. Wartime frequency analyses each point, you know, this time frequency point is going to be really, really strongly correlated with the neighboring time frequency point, and that is partly because brain activity is is auto correlated over time. And it's also partly because we actually impose smoothing on the data when we do time frequency analysis. So we impose both temporal smoothing and spectral smoothing. So therefore the independence assumption of bond, any correction is totally violated. It is not a valid assumption for these kinds of data sets. Finally, the third point is that Barnstone correction is based purely on nd it's not based on the information content in the data and that you see in the formula here. It doesn't actually matter what are the characteristics of the data. All that matters is NP. And the problem with that four time frequency analysis is that some of these numbers here are actually pretty arbitrary. So we could, you know, let's imagine you use Barnstone correction and then you get an effect that isn't quite significant, but you really want that effect to be there. So you need to have a less stringent threshold. So what do you do? You downsampled the time points. You do post analysis downsampling and you get down to a hundred time points instead of one thousand time points. Now, the thing is, if you go down, if you don't sample your your results to one hundred time points, this is going to be 64000 tests instead of six hundred and forty thousand tests. And that means that the P value is going to become more liberal. And you haven't actually changed anything in the results that all of the features in the resulting time frequency map are the same four thousand time points and one hundred ten points. And we could do the same thing for frequencies. We could reduce or increase the number of frequencies. That doesn't change the information content in the results. It's only changing the sheer number of tests. So that is a fairly awkward feature of Barnstone correction. And that also means that it's not really appropriate for these kinds of data sets, time, frequency, data sets. Now, I will repeat something that I've mentioned halfway through the video. This does not mean that you should completely abandon bond for any correction. That bond for any correction is wrong. And Bon Ferrone himself should have been a surfer instead of a statistician. But any correction is great and there are definitely places where Barnstone correction is appropriate. In fact, later on in this section, when I start talking about group level analyses, I will talk about a situation where Barnstone correction makes a lot of sense. However, for doing time, frequency and our statistics on time frequency analysis results, Bonnefoy only correction is a poor choice because of these three features. So we have to do some kind of correction for multiple comparisons because we have such a large number of multiple comparisons. And at the same time, Von Ferrone correction, which is the standard go to method, is not appropriate. So what do we do? What is the right way to deal with all of these multiple corrections in a way that is statistically appropriate while still sensitive enough to identify real differences? And that comes up in the next video. So I hope you are looking forward to it. I will see you soon.