 In this video and the next video, we are going to explore permutation testing in data sets. This video is going to be permutation testing in simulated data. So we're going to create a really simple data set and it will do some permutation testing. And then in the next video, we are going to apply these principles to a real empirical data set. All right. So let's begin. We want to create a data set that is essentially random numbers. And one of those distributions is going to have shifted random numbers. And then we want to determine whether data set to or the average of data set to is significantly greater than statistically, significantly greater than the average of data one. Now, we know that there is this ground truth. There is this offset that we have imposed on the data, but the data are also noisy. So just adding this offset does not guarantee that we will find a statistically significant difference between this population or this sample and this sample. OK, so we are going to create the so the first data set will have 50 numbers or trials or individuals. You know, it doesn't really matter what you think. These individual units are so 50 and 70 data points in the second data. So we actually have an imbalance. And so then we have our two distributions of data here. OK, and then what we are going to do is step one of permutation testing is pull the data. So we went from having two separate data sets to one combined data set here. So now we have all 120 data points into one vector. Now here is where we compute the true labels. So we have one and then two times ones. So, yeah. So this is just going to be a vector that is all the ones and twos corresponding to the actual condition labels of here in all data. OK, so what is the the observed condition difference. So the observed condition difference is the average of data to minus the average of data one. But I'm going to compute this from the variable all data and you will see why in a moment. OK, so we want the mean of now. It's not all data, it's all data and then just the elements in all data where true labels. Equals two minus the mean of all data. And again, all of the data points like this, but only the data values where true labels equals one. Now it's a bit arbitrary, whether you do two minus one or one minus two. I set it up this way just because I've added the mean offset to data, too. So in my mind, I'm kind of thinking about this as like the interesting experimental condition and this as the control condition. So what is the expected mean value? Well, it's printed right here. We expect that the average of the difference between the averages of these two data sets is zero point three. That is the expected mean difference based on how we simulated the data and how does that compare to the empirical mean difference. So let's find out when all of this code. Wow, that are you surprised at that? I'm a little surprised. So we actually added a mean offset of point three and yet the observed data. So this is like the true underlying difference in the population and then in our sample ended up being minus point zero three. So it's small, it's very close to zero. But if anything, it's on the wrong side of zero. Now, that is not because I made a mistake somewhere. I hope I didn't make a mistake somewhere. Yeah, it's not because I made a mistake somewhere. It is because we are doing random sampling. There are random numbers that are being generated in here. OK, so interestingly, I just ran it again. I changed nothing. And now the empirical mean difference was almost exactly what we specified. So you can see that there's some variability, even though the true underlying parameter in the population hasn't changed because we are randomly sampling from the total distribution of Gaussian distributed numbers, we are not guaranteed to get a sample that has a parameter difference that similar to the that or that matches the population simulation. OK, but you can see here it's even larger here. It's point four. The can see OK, so it turned out that was really large. It turned out that that one time that I got a value of minus point zero three, that actually ended up being pretty unusual. I'm running it over and over again and I don't really get any really tiny results anymore. OK, nonetheless, let us move on. So now we want to shuffle the data and here we're just going to shuffle the data once we're not going to worry about multiple iterations, building up a whole distribution of hypothesis values. We are working our way towards there. So let's just start by doing one shuffling. So what we want to do is leave the data untouched. We do not want to change the data. Instead, we just want to change the vector of conditions and labels. So what we want to do is just randomly shuffle which label gets to which data point. Now, the wrong thing to do is to just randomly assigned one or two to each of these data points. That's the wrong thing to do, because in the original data, we still had fifty of one condition and seventy of the other conditions. So when we shuffle the data, we want to preserve this trial in balance. So what I'm going to do is instead of just randomly coming up with a string of ones and twos, I'm going to take all the true labels and randomly permute them. So Randt Perm and then and one plus and two. In case you're unfamiliar with this perm function, I guess I can take a moment here to explain it. So rent perm and perm and then you input an integer, let's say three. And then what matlab will return is integers between one and NP and you know the number you inputted randomly sorted. So here it happened to be one, two, three. And here it's three to one and here it's one to three again and so on. OK, so this means that we are randomly permitting everything inside this true label vector. And now you see, you know, it's randomly ones and twos and importantly, there's still going to be seventy twos and fifty ones. OK, so then we want to compute the mean difference of these shuffled labels. So I'm actually going to do here is because this is almost exactly the same that we have done above. Except now instead of assigning the data points to averaging based on the true labels, this is going to be based on SHUFF labels. They shuffled labels. So here we go, shuff labels and then let's see what this ends up being. Point three. So still pretty big actually. And what was this. So the true condition difference hug. This is a really interesting situation. So we find that the true condition difference with sampling ended up being zero point one and the shuffled condition ended up being point three. So in fact, when we randomly permuted the condition labels, we ended up with a larger mean difference compared to the true shuffling or the true condition mapping. OK, and then another reminder, I've already mentioned this. Wow, this is a difficult typo here. The data have not changed, so we haven't changed the data. Only the mapping of the data to condition label has changed. OK, so question, how does the shuffle mean different compared to the observed mean difference. So was pretty interesting. The shuffled mean difference was actually larger than the observed mean difference. And if you rerun this code, how similar are the results? So we already saw that up here. I reran this multiple times. So let's run this again with new data. Oh, now it ended up being. Really big point seven. It's more than twice the actual, you know, the population mean difference. And then let's see. So I'm pretty confident that this is now going to be smaller than the observed mean difference. All right, cool. That was pretty fun. Now let's move on. So this is for one shuffling and we can't evaluate a statistic based on one random shuffling. So therefore, we are going to create a null hypothesis distribution. So we are going to go through 1000 iterations. And let's see, here is the main loop that does the permutation testing. So we loop over all the iterations and then we shuffle the permuted the true labels and then we compute the mean. And then notice here we're saving the main differences for all of the Permuted values. So let's see. This was you could scroll up and and copy and paste, but this is pretty simple here. And then I can actually just copy and paste this from before. Now, here's another case where you have to be super duper careful. You know, I could actually run it like this and I don't get any matlab errors, but I get exactly the same value for every iteration. And that's because I didn't actually apply these shuffling. This was copied from the first part with the with the real labels. Let's see. So that's shuff labels and stuff labels. OK, so let's try running this again. And now we can confirm that we get different values for each iteration, so let's see what these look like. We are going to plot a distribution of all of these permuted values. All right, so here you go. These are the permuted mean differences and this is our observed difference here, which is way out here. It ended up being point seven for this particular distribution. And interestingly, that never happened, not even once with the random shuffling. You can also see that this random shuffling is is really strongly Gaussian. Looks like almost a perfect Gaussian. Question how reliable is the distribution rerun this code multiple times to observe. So I think I'll rerun it just twice or rerun it once. So that's a total of two times. And now I'm going to. All right. So we don't want to create new data. We just want to rerun all of this permutation testing. And then essentially we want to compare what these two distributions look like. So this is total qualitative comparison. I'm not doing anything, any model fitting or testing the means or anything. But you can see qualitatively, they look very similar, but definitely not identical. These are definitely different distributions, even though they are overall quite similar. OK, and obviously they're going to be different. We are randomly coming up with the permutations. OK, so now we have our null hypothesis test and now we want to you know what I really want to do, actually, because we're going to get a P value of exactly zero for the P count method. And that is on the one hand, fine, it's legal. But I think it's also a little more interesting if we get a a non-zero P value. So I'm just going to generate a new distribution and now this is going to be point to four. So this will be interesting. The observed value is going to be around here. So now we have to of course, this is no longer a valid null hypothesis distribution because we have different data. We have to generate the null hypothesis distribution from the same data that we use to measure the true test statistic or the observed test statistic. OK, so this is pretty interesting. This is going to be a nail-biting, anxiety provoking experience. Is this statistically significant? Is this far enough away from the center of this null hypothesis distribution? I am super curious to find out. Oh, there was another question here which I ignored. So how is the shape of the distribution affected by the number of iterations? Oh, that's an interesting question. Try rerunning the code with very small and very large number of iterations. What do you think? And this is just qualitative. What do you think is a reasonable number of iterations? So let's see. I will try this. So we've done it with a thousand. Let's go to figure three and we'll try it with one hundred. And, you know, maybe I'll do about 30, because one hundred is still a pretty respectable number, let's go for 30, which is a totally non respectable number. OK, so now let's compare these to actually. Now, first, let me do in figure four and I'm going to do about thirty thousand. So we went from one thousand to my not one hundred thousand. So we have thirty one thousand and one hundred thousand. All right, and let me put this figure here. OK, so here we have and I want these all have the same. Yeah, they all have exactly the same x axis limits. They have different Y axis limits. But that's fine if you like. You could normalize all these. So it's such that the probability or this histogram distribution has a maximum of one or you can normalize it so that the total area under the curve is one. But that's really not necessary. OK, so I just want to look at these qualitatively. So actually, let's start with the thousand and ten thousand. You can see this ten thousand version does look a lot cleaner. It looks a lot nicer and smoother. And in fact, I'm sure you could make this look even smoother if you would increase the number of bins here. I'm using forty bins, but still these look overall quite similar. Maybe there's some little awkward thing here, quite similar. And then we get to the 30 condition and you can you know, you can see when you know that it should look like a Gaussian, then you can see the Gaussian in here. But if I just showed you this distribution, it might be difficult for you to imagine that it's a similar distribution as this one. So therefore, it does kind of seem like a thousand is a reasonable number. 30 does not really seem like a reasonable number. It's a little bit too small. OK, so now we are going to implement two methods for computing a P value. These are the two methods that I have discussed in the slides and the previous video. So the first method here, I'm calling P Z. So this is the Z value and then we are going to convert that into a P value. So to compute a Z value, we need the mean and the standard deviation. So that is the meaning of I already forgot what this variable is called per Furnival's. Yeah. So I mean of perm vowels and the standard deviation of perm vowels and maybe I'll run this. Well it's OK. So it's based on a huge number of of permutations but that's fine. So let's see. I'm curious what this should be around zero and which it is. It's, it's ten to the minus four and I don't know what the standard deviation is going to be. Zero point one eight. OK, so then the formula for the Z score is the observed data. And I also forget what that was called. That was called true Cundiff. So we have true Cundiff minus the perm mean divided by the perm standard deviation. So let's see. This set value is one point three that's coming from this distribution. One point three is not classically considered to be statistically significant. This might actually be marginally significant. Let's see, so we get an apparent p value of point nine now, if you have taken a stats course before then this might seem like a strange number given this distribution here, this histogram, because it doesn't look like the P value should be point nine. Now, what's actually happening is that this is the probability that, eight, that any of these values will be somewhere below this threshold here. So, in fact, what we really want is one minus this probability, and this gives us our actual P value, which is zero point zero nine. So that would be, you know, sort of textbook considered to be a marginally significant effect, not a statistically significant effect, a marginally significant effect, because the P value is less than point one. But in general, this is not a finding that you would put a lot of faith into you. Don't you know, you don't bet a lot of money on this finding being really true. And that is pretty curious because we know that the simulated data, you know, we actually imposed this mean offset. OK, let's see. So here we get a title. And so the title tells us the Z value and the P value. All right. So then so that was one of two methods that using the Z score. And then we have the other method, which is a bit more assumption free in the sense that it doesn't rely on having a distribution where the mean and the variance are well characterized like a Gaussian distribution. So here we are taking the so counting the total number of random permutations that were greater than the true difference. And then we sum up all of those and that gives us nine thousand four hundred and seventy two. So that means there are ninety nine thousand five hundred, almost ten thousand random permutations that ended up giving a condition difference that is larger than the observed difference. Now, this number is very difficult to interpret on its own, and that's why we divide by the number of iterations. So you can see that these two P values converge. They are very high. OK, so they're not very similar, actually. This one is this is another thing that we would have to think about pretty carefully, that the one minus doesn't actually belong here because we are testing for the number of permuted values that are greater than the observed test statistic. Right. So here you see that these two methods for computing a P value give really, really, really similar answers. They are they differ by 1000, basically. Now, when you have a roughly Gaussian distribution, these two methods for computing, the P value will converge. They will give you essentially the same results, not going to be numerically identical, but they will look really similar. You will find that these two results will differ. These two P value computations will differ from each other when the distribution is strongly non Gaussian. OK, so the main point of this video was to show you how to implement permutation testing in practice in code. In a simple example, in the next video, we are going to apply these methods to a real empirical data set.