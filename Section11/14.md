 This video will provide a somewhat more high level overview of group level statistical analyses, three approaches for group level analyses. This is a little bit different from most of my other videos where I try to get into the mechanisms and the math and the implementation details a little bit more so the three approaches can be broken up into two kind of families. We have approached one and approach two, which is A and B approach one for group level or level two statistical analyses is oriented towards exploratory analyses. This would be the kind of situation where maybe you don't have a real hypothesis or maybe you have a bit of an expectation, you know, which conditions you should be comparing or which electrodes or brain regions. But you're not really sure. You know, you don't have a hard constraint for which frequencies or which time points you want to constrain yourself to. So that's approach one approach to to a and to be this is more oriented towards hypothesis driven analyses. This would be the case where you have a very strong theory or apriori motivation based on some prior data, based on a theoretical framework where you have a constraint, a hard constraint that tells you which frequencies are the most relevant and which time windows are the most relevant. All right. So let's start talking about approach one. So the idea of approach one is to use permutation testing and we'll hear. Right. Cluster correction. But actually, it's any kind of correction for multiple comparisons, just like with the mass univariate approach that you already know. So you've already learned in previous videos in this section how to do mass univariate statistical testing on time frequency maps and mass univariate just means that you're doing univariate tests. So you're testing one pixel that would be UNIVARIATE. And then mass univariate means you're testing lots and lots of individual pixels. And of course, you have to apply some kind of multiple comparisons. Correction. OK, so the advantages of approach one are that you don't have to worry about assumptions underlying parametric statistics. There is a reduced chance of bias or subjectivity, and that's because you don't have to select the time frequency windows. You are testing every single pixel in this time frequency map and maybe over different channels and so on. And you apply an appropriate statistical threshold. And whatever happens to be significant is what happens to be significant. So you're not selecting any windows of time, frequency, plane apriori. So that means that there's less chance of bias or subjectivity or circular inference. And another advantage of this method is that it's also open to exploration, it's open to exploratory discovery. So coming back to this plot, imagine, you know, maybe you had some expectation about some effect being in the gamma band, but you didn't really have expectations about any other frequencies. But then you do this at mass univariate testing and correction for multiple comparisons and then you find that there's this tonic, low frequency difference. And maybe you didn't expect this to happen. It just popped out of the data. It was a data driven finding and maybe that's interpretable. And who knows, maybe this will inspire you to do further research based on this unexpected finding. So that's the kind of effect that can pop out from approach one because you are open to exploratory discovery, a disadvantage of approach. One is that this general statistical approach with permutation testing, it's really optimized for a to test comparison or a correlation coefficient. So this kind of approach is not really very amenable to the kinds of more complicated factorial designs, like if you have a a two by five factorial design where you would be running a novas and things like that. So this also means that there's potentially lower sensitivity to small but theoretically motivated effects. So let's go back here again and let's say, you know, let's let's think about this little thing right here. Let's say you actually have an hypothesis. You have a theory that predicts that there should be a 40 to 50 hertz gamma response at around one hundred and sixty milliseconds. Now, obviously, I'm making this up. I'm making up the hypothesis based on looking at the results. So, you know, it's not really appropriate in science, but this is just for educational discussion. So let's imagine before you looked at the data, before you even ran your experiment, maybe the entire reason why you ran the experiment was to interrogate this finding, was to get this finding, because that is what is theoretically predicted now. Unfortunately, it turns out that there's a lot of other things in the data going on in the data, and they have large, large clusters. So you apply a cluster correction. These are large effect sizes. And this might be theoretically meaningful. It might be relevant and interpretable, but the effect size is small and the cluster size is small. So therefore, applying approach one means that you can miss out on potentially real effects just because they are not expected and they are a little bit small. OK, so that is a potential disadvantage that these sensitivity to detecting small but theoretically motivated effects is reduced. So this is for approach one. Now let's talk about approach to such an approach to what you do is you average all of your conditions together. So all of your experiment conditions, all of your subjects average everything together. And then you look at a time frequency powerplant of all. So the grand grand total average relative to baseline, so baseline normalized. And then you look through your map and you pick some time frequency window. Maybe it looks like this, you draw a box around this window or maybe you do a test for these pixels against baseline and that allows you to define a statistically created or statistically masked region of interest here. So either way, so, you know, there's a couple of different ways that you can define your time, frequency, region of interest. And then the idea is that this window gets applied to each individual subject and each individual condition. And then what you do is averaged together the power values or, you know, phase clustering or connectivity. Whatever is the feature of the data that you're working with, you average all the data together in this time frequency window. So all of these pixels in here get averaged together to create a single number. And then that allows you to build up a table, a data table that might look something like this to have each row corresponds to a subject or an individual research participant, and each column corresponds to a different condition. So this would be activities, average activity from the Delta Band, which is what this region is showing from Condition one and Delta Band activity from condition two and better and better and so on for, you know, however many regions of interest you have, hopefully not too many, but potentially could be several. Now I'm going to discuss in a later video, might be the next video, some strategies for selecting time, frequency, windows that are appropriate and also sensitive and meaningful. OK, but so this is the idea. So let me just expand on this a little bit. So you take your group and condition average. So this is the time frequency powerplant averaged over all. You know, let's say you have 30 subjects and averaged over all of your five different conditions, however many conditions you have. So then the interpretation of this map is this map is not really telling you about the different conditions per say. This is really telling you about, like TASC engagement, General TASC engagement. So relative to the baseline, we see that this task elicits activity at, you know, whatever electrode this is coming from, illicit activity in the theta band and in the end a beta band suppression. So these are the task relevant regions. So then we take this Arawa, this region of interest and apply that window back to each individual subject. So here you have the grand average and here you have 12 individual subjects. And you can see that this this time frequency window is applied to each individual subject. And then, of course, you would have this for each of the different conditions as well. OK, so that's the idea that you extract the average time frequency data from one or possibly more time frequency windows or regions of interest. The advantage of approach to A is that it is amenable to factorial design. So these are the kinds of experimental designs that are often done in psychology and sociology and cognitive neuroscience, where you're not just comparing condition A, condition B, you might be comparing several levels of condition A and more levels of condition B, maybe you have some within subjects factors and some between subjects, factors and so on. You can see that this is a fairly hypothesis driven approach, and that's because we are putting all of it, pulling all the data together and then coming up with some region of interest. Now, you can also specify a focused region of interest based on some theory or prior data. So let's say you predict that they that the most important feature in this experiment in the brain is going to be theta between 200 and 600 hertz and between, say, two hundred and six hundred milliseconds and between four and eight hertz or whatever. These boundaries are so highly amenable to specific hypothesis driven statistical analyses. And therefore, we also have minimal multiple comparisons issues. So, for example, going back to this side here, we are only comparing two or sorry, we're only running tests in two windows of interest here. So now if we wanted to correct or multiple comparisons, you could actually use BARNSTONE Correction. You don't need to worry about cluster correction or extreme pixel. You don't need to worry about permutation testing. You can do. Barnstone correction for these two comparisons here. So then at a P value of five, a nominal P value point of five, the effective corrected P value would be zero point zero two five because you have point zero five divided by or correcting for two comparisons. So for this reason approach to has increased sensitivity for detecting potentially subtle effects. And that's because you don't have to correct for every single time frequency pixel. You don't have to correct for all the electrodes. You don't have to correct for every possible test that could in theory be done with your data. Instead, you just dive right in, you go right for the jugular and you just test the specific regions of interest that you have hypotheses for. So maximal sensitivity to potentially small effects. And then the main disadvantage of approach to A are the disadvantages. The two disadvantages is that there is a potential for bias when selecting the time frequency window. And a second disadvantage is that you can you are open to missing important but unpredicted results. So that's because, you know, your theory says that this is the important region, but maybe in the brain, this is actually the most important thing you can see. Basically, every almost every subject shows this beta band depression. I guess these two subjects don't, although this subject doesn't really show much of anything. Not even sure the subject was really fully alive. But anyway, so maybe this is actually the really important, really discriminative feature of this dataset and you are completely ignoring it because your hypothesis tells you to focus on this region alone. So that is a potential disadvantage of approach to a any potential bias here refers to selecting these time frequency windows, these regions of interest. Again, as I mentioned a moment ago, I'm going to talk about this later. I'm going to have a whole video just about how to set up your analyses appropriately. So you are avoiding circular inference, which is also called double dipping. OK, so that is approach to a now let's talk about approach to B, that's actually really similar, as you might imagine from the label. The idea of approach to B is that you draw a larger window. So this is still the average overall experiment conditions and the average overall subjects. So this is like the whole all task related dynamics relative to baseline. And then you define a larger window than you would have defined for approach to. And then within this really large window, you pick smaller windows that can be applied to each individual subject. So you see that this larger group window is constraining the data, constraining the search space. And then you go for subject one and subject to and subject three, and you can move this little smaller window around to maximize the fit to make this be the best location for each subject's individual data. So it's a bit constrained and it's also a bit flexible. Now, one thing you can do with approach to be that's not possible with approach to a is record, not only the average power or whatever feature of the data you're working with, but also the peak frequency and the peak time. So we can see that the peak frequency differs for different subjects. And it also differs. The peak time also differs for different subjects. Now, maybe this is not interesting variance, but maybe it is interesting variance. For example, it is well known in brain oscillations research that. Rain freeze, oscillation frequencies slow down as you get older, so it could be that this is a younger participant. This is older and this is an older participant. There are also hypotheses floating around in the literature about the relationship between dominant oscillation frequency and information processing speed, for example, how quickly you can process visual information and so on. So this might be pretty interesting, important information. OK, so here you see another example of this procedure. So I've defined a group level time, frequency, region of interest, which is fairly large. And then for each individual subject, you see that group window here in grey and then these specific plot or the specific boundaries for each individual's region of interest is optimized for that particular individuals theta bursts so it can move around slightly for different individuals. OK, so then the idea of approach to B is you extract time frequency data from one or more windows, but it's optimized for each subject and constrained at the group level, just like with approach to a. This approach is amenable to factorial designs and it's hypothesis driven, which is generally a good thing in science. You have also like to a minimal multiple comparisons, issues and an advantage over to a and therefore also over approach. One is that you can also extract the peak time and frequency per subject in case that's meaningful individual variability. OK, and then the disadvantages are the same as with approach to a there's a potential for bias when selecting the time, frequency, regions of interest and you can miss important but unpredicted results. Now, there's also the possibility that some of these subject specific windows will be driven by noise at the single subject level. And I guess you don't really you don't really see that here because these are fairly clean data. But I guess you can imagine this thing up here. This is like a really transient burst at a high frequency. I suspect this is noise. I suspect if you would go back and look very carefully at this subject's data that produced this time frequency powerplant, I would guess you would find one trial where there's really a high frequency noise up here. OK, but imagine that this little noise burst wasn't here. I imagine it was inside this window. Maybe it was all the way down here. So this is like the truth data. This is where you really should be. But if this little noise bursts were down here, then this window could algorithmically identify this little, you know, this little blip of of transient high frequency noise as being the most important feature. And then that could introduce some unsystematic errors into your statistical analyses. So there you go to families of approaches for doing group level statistical analyses. Of course, it's possible to combine these two and or to apply both of them. That's something that I do very often in my own research. I will start off with the hypothesis driven regions of interest. And then once I've done these statistical analysis, I come back to the same time frequency maps and I apply approach one to do these exploratory analyses. I like this approach, this combined approach, because first of all, I'm a big fan of methodological convergence, so I like to see the same results popping out, even though I'm applying different data analysis or statistical procedures. Secondly, I also think it's interesting to be open to exploratory discovery, data driven blind discovery, even if I have very specific hypotheses that are guiding my analysis.