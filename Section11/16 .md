 In this video, I'm going to discuss a statistical issue called Circular Inference, which is also sometimes called double dipping. Now, first of all, circular inference is very different from circular statistics. Circular statistics refers to doing statistics on the circle, on a complex plane. So this is basically like what it's like and phase clustering and phase lag index. These are circular statistics. What I am talking about here is called circular inference. So I'm going to explain what circular inference is. I will define it and I'll illustrate with a simple example why it is a problem. And then we will talk about a few examples of what might or might not be circular inference. So the definition of circular inference is selecting data in a way that biases the test statistic and therefore also biases your interpretation towards obtaining a particular result. So if you select data in a way that makes a particular statistical result more likely or more likely to be significant or maybe more likely to go in one direction or another direction, then that is circular inference. Let me give you an example. So I ran an experiment here with my data methods. I generated 20 normal random numbers. And now by normal, I don't mean that these numbers were like psychologically healthy and they watch TV and stuff. I mean, these are normally distributed random numbers. Gaussian distributed random numbers. And then so that was step one. Step two was I selected from this sample of 20 numbers. I selected the ten largest numbers, whatever they were, I just took the ten largest numbers. And then I took these ten numbers and ran a test against zero. So I did a statistical test to see if these 10 numbers were statistically significantly different from zero and then steps one through three. Well, so then I recorded the result of that test and then steps one through three. I repeated one hundred times. That was step four. OK, so the question is, how many of these 100 times do you think I got a significant result here for step three? How many times were this set of random numbers significantly different from zero? The answer is about 90 percent of the time. So it turns out I ran this experiment multiple times. So this is an average over several of these step for us. So the question is, are you surprised at this result or are you surprised that around 90 percent of the time the T values were significant at some point or five, meaning that 90 percent of the time that I generated 20 normal numbers randomly, you know, uniformly. So Gaussian normally distributed random numbers, which have a mean of zero and a variance of one around both sides of zero, 90 percent of the time the T test was significant, meaning that these numbers were significantly different from zero. I guess you are not at all surprised and you are not surprised because of step two. Now, if I didn't tell you about step two and I gave you this result, you would think something was weird. Something isn't right. But because of step two, you are not surprised. In fact, maybe you would have even thought that it was higher than 90 percent. So the problem here is that I selected the I selected from this sample numbers in a way that biased the test. So I was more likely to get a bunch of positive numbers, in fact, that it's fairly likely that 10 of these numbers were are all ten of these numbers were positive because by chance, about half of these should be positive and half should be negative. And to be clear, the problem here is not that I selected data because I could have selected I mean, I'm already selecting numbers here, but I could have selected ten of these numbers at random or maybe I could have selected from from this sample in a different way. Maybe, you know, if I ordered the numbers and then I selected all of the even indices. So, you know, the second and fourth and sixth and so on numbers. So the problem is not the selection per say. The problem is how I did the selection and I did the selection in a way that biased the test that I was running. And then, you know, out of curiosity, I also ran this without step two, so I didn't do step two. And then I found that about five percent of the T values were significant. So approximately five percent of the time, generating 20 random numbers ended up being significantly different from zero. So that's also pretty interesting and quite remarkable because I was testing it with a P value of five. So that's in fact exactly the prediction. Anyway, I think you get the idea so. Here is an example in real data of selecting data in a way that biases the results. So here's what I did. These are my statistical methods. I generated this time frequency power plot. And then step one was I looked I use my eyeballs and my brain's visual system. And I looked for the biggest, brightest, reddish blob in this entire time frequency plane. And that was here. And then I drew a box around this and then I averaged all the pixels inside this blob. Now, so far, step one and step two is exactly what I said, approach to an approach to a should be in the last video or two videos ago. I forget when it was. However, in this case, what I did was step three. And here I ran a test against zero. So I looked for the biggest blob and I averaged all the values together. And then I ran a test against zero. Now, this is a biased test. To be clear, we are not guaranteed to get a statistically significantly significant effect, but we are very likely to. Even if this was purely random data, we are likely to get a significant result here at step three because of the way that I am selecting the data. I hope you can see that this is essentially the same procedure. It's the same mistake that I made in this fake experiment where I'm generating random numbers and picking the largest ones to test against zero. Now, I also want to be clear about this example, because the problem here, the statistical problem here is not step three. The problem is not that I'm running a test against zero. The problem is that I'm selecting data in a way that biases the result. So there's another problem here, which is really just about the P value threshold. So an appropriate way, if I wanted to know what on this map, if anything, is significantly different from zero. So let's say step three was the main goal, then an appropriate way to do this would be to do permutation testing. And I test every single pixel on this time frequency plane and, you know, correct for multiple comparisons using one method or another that is appropriate for multiple comparisons. Correction, then I can still do step three. So the problem here is not in step one. I'm allowed to look for big red blobs. The problem is not in step three. Running a test against zero is great as long as you do it appropriately. The problem is that I am selecting data in a way that biases this result. OK, so here is two strategies for avoiding circular inference strategy. One is to test everything and correct appropriately. That is what I just described in the previous slide. So you can do a permutation test that every single time frequency pixel correct for multiple comparisons and then you can test against zero at each pixel. And then we have strategy two, which is to select data and test orthogonal. So strategy one you already know about. I've already talked quite a bit about permutation testing. So now I want to spend a few moments talking about this strategy where you are selecting data, but you are selecting data in an appropriate, statistically appropriate way that avoids circular inference. So the idea is that you want to select your data in a way that is orthogonal to or uncorrelated with in a way that is unbiased for your the statistical test that you actually want to perform. So, for example, here we have a map and now let's say this is not a condition difference, this is a condition average. So this is condition A plus condition B, divided by two. So now I'm going to select data, I'm going to look at this map and select these two time frequency windows, these two regions of interest purely based on visual inspection. So this is data selection. However, what I am selecting for is the average of both conditions. And I don't know from this plot whether A and B are actually different or whether they are the same in these two regions of interest. So then you can extract all the data from this region and extract all the data from this region. And here I'm showing them plotted in a bar graph. And of course, you could run through a statistical test, you could do tests or from and padrone correct. For the two regions of interest, you could do an ANOVA if you want to look for an interaction, whatever is the appropriate test. Now, here's the thing. This method is totally valid because the way that we are selecting the data based on the condition average is orthogonal to the statistical test that I'm actually interested in, which is the condition difference. And now you have to be a little bit careful, because if I were to test whether these these data here in R1 were significantly different from zero. That is biased, that is circular inference, and again, we are not guaranteed to get a statistically significant difference from zero, but we are likely, even if these were random, smooth data, we might get a significant difference because of the way that I selected the data. So selecting the data isn't the problem. The problem is whether you are performing statistical tests and inferential statistical test based on the way that you selected the data or based on some other way that is orthogonal to how you selected the data. So by testing for differences between conditions, that's totally valid. It's totally legit. OK, here is another example I have now. This is a time frequency map of condition A, minus condition B, and then I look at this map. I look everywhere in this map. I look all over the place and I say, hey, this is a pretty interesting thing happening over here. Here's an interesting blob in this time frequency difference map. I'm going to put a time frequency window exactly. Covering this blob. And then I plot the these bars for the average power from this time frequency window for condition A and for condition B. All right. So now, based on this different plot, I generate this region of interest. I plot these data and then I test statistically and I find in a T test that B, you know, this condition is significantly greater than this condition. Now, this is circular inference. This is biased. This is inappropriate. However, you know, this stuff starts to get a little bit tricky because the interpretation of the significance here is biased by the data selection. However, the visualization here is not necessarily biased and in fact visualized. So selecting data and visualizing it like this can actually be a really good idea and can really help you understand and interpret and report your data. And that's because these three different scenarios are all equally likely given this time frequency difference map. However, these three scenarios all would require a very qualitatively different interpretations. So here we see that condition. B has basically no change in power, whereas Condition A has a lot of activity here. We see increases in activity in both conditions, but is more for a condition a and here finally we see the opposite, where in fact condition A doesn't show any change in power and condition B has a negative power. So a suppression of power. Now, all three of these cases are equally likely based on this scenario here. So to summarize, this method is tricky. These statistical significances are biased. So that is circular inference. However, visualizing the data like this is informative because the pattern of differences, which condition goes up and which condition goes down or do they both go up or down? That is not trivial and that is definitely not circular. So I hope you see that avoiding circular inference is not so difficult. It can be a little bit tricky. You have to think carefully. And I recommend always going back to this thought experiment here and imagine running through this thought experiment with your selection procedure. So think about generating random numbers and then thinking about selecting from those random numbers the way that you propose to select from your actual data. And then how many times do you expect this test to be significant? If it's more than point zero five, then you have a circular inference issue and you need to rethink your strategy.