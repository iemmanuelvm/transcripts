 Let us continue our discussion of statistics, inferential statistics, and in particular, I'm going to talk about parametric statistics, the assumptions of parametric statistics, and we will have a discussion about whether those assumptions are actually met in the kinds of analyses, the kinds of data sets that you will be acquiring with electrophysiology. All right. So this is really all about what I mentioned in the previous video with evaluating the statistical significance of your test statistic value, which, as I mentioned, was really just a signal to noise ratio value. So you have the signal to noise ratio value, the test statistic that is computed from the actual data that you have measured. So that is the observed value. Now, how do you know if that's big enough? Well, it's big enough if it is unlikely to occur and effect size that's of that size is unlikely to occur given a distribution of null hypothesis values. So the question is, where does this distribution come from? How do we get this distribution to compare against our observed value? Well, essentially, we have two ways of generating this distribution. We can generate it based on a mathematical formula. So that would be an analytic solution. And then you rely on several assumptions about the underlying population, or we can generate that distribution based on null hypothesis, empirical distributions. And this generally comes from permutation based approaches. So this is called parametric because we have parameters. We are computing parameters based on analytic formulas that we have. And here this is referred to as non parametric because we don't have specific parameters that we are evaluating based on a formula. Instead, we are generating these distributions empirically based on the actual data that we have and simulating situations that could arise under the null hypothesis. Now, I'm going to talk much more about how you actually go about generating this kind of empirical null hypothesis distribution. That's the entire purpose of the next video. For now, I want to talk about some of the assumptions that allow you to validly use a particular formula to evaluate against a observed test statistic. Okay, so the question is, what are the assumptions underlying parametric statistics? Now, if you already have taken a stats course, then you should know at least some of these assumptions. And so I encourage you to pause the video and take a moment to think about or write down the assumptions that you can remember about parametric statistics. So they are that the data are sampled randomly from a population. So there's no bias in the data sampling measure and that that population from which the data are randomly drawn has known parameter distributions. And here the parameters refer to the key descriptive statistics. So like the mean, the variance, the shape of the spectrum and so on. Furthermore, we assume or parametric statistics generally assume that variance across repeated measurements is homogeneous. I will explain in a moment what this means if you're not totally clear on what this statement means. And finally, there's an assumption that measurement error and sample variability are independent across different measurements. So the errors, the error terms, the variability is independent across different individuals that you are measuring. So that could be different neurons, for example, or different individuals and so on. OK, so let's walk through each of these assumptions in turn and discuss them a little bit. So do spectral data. So we are thinking in particular about time frequency data or synchronization data. Do they meet these assumptions? So first of all, let's start with this. Data are randomly sampled from a population? Well, you know, the population of humans, for example, if you're studying humans, there's seven plus billion humans all around the world. And are you sampling randomly from that total distribution of humans? Now, in some experiments, people do try to sample as as randomly as possible from the population. But in many other experiments, people are researchers are not sampling randomly from the population. They are sampling non randomly from, for example, a sample of educated college students. Now, that is not a problem, per say, but it is important to keep in mind when you want to generalize your findings and to understand what are the groups of individuals that you can. Generalise, too, based on the data and how you have sampled from the data. OK, and then we can also think about not just sampling from humans, but sampling neurons in the brain. So is the way that we sample neurons random from the population. Now, this depends a little bit on the method that you were using, but for electrophysiology, the answer is largely no. We are not sampling randomly from neurons in the brain. And that's because a lot of neurons in the brain are small. They are a bit shy, a bit quiet, meaning they have a relatively low firing rate. And so they are difficult to identify electrophysiological. So we are sampling neurons not randomly from a population of neurons, but in a biased way, according to some types of neurons that exhibit certain functional characteristics or anatomical or morphological characteristics. Again, that on its own is not a problem per say, but it does mean that we need to be careful about how we can generalize findings to an entire population and what assumptions we really should be making about the population versus the data sets that we are sampling from. OK, so another assumption is that the population has known parameter distributions. Now it turns out that that is sometimes the case and sometimes not the case. So there are analytic solutions. There are known null hypothesis distributions for some features of electrophysiology data, but there are other features of electrophysiology data, other kinds of analyses that we can do where we just don't know what the actual parameter distributions are in the population under the null hypothesis. So I forgot to come to a conclusion about this first one, but essentially this first assumption is sometimes met and sometimes not. And this also this assumption that the population has known parameter distributions is sometimes met and sometimes not. It depends on the analysis. And then some distributions can actually be transformed into known distributions. For example, raw power is not normally distributed, but decimal normalized power. So if you normalize your power, time course is relative to a pre stimulus baseline, for example, then the decimal normalized power will approach a Gaussian normal distribution under the null hypothesis. OK, so now let's talk about this assumption. Variance across repeated measurements is homogeneous. What does that mean? That means that if you take repeated measurements of some system like the brain, for example, that the variance across different measurements should be the same or homogeneous. And unfortunately, that's also not the case in electrophysiology data. So we see that here you see the ERP. So the average over trials and oops, sorry, that's not true. These are in blue. You see the single trial traces. This is actually not the IRP here. This is the standard deviation. This orange line is the standard deviation over all of these different trials. Now the assumption is that there's an assumption of parametric statistics is that the variability across repeated measurements over time in this case is homogeneous. So that should mean that the standard deviation is constant over time, but is not. You see that some time points have a higher variance and some time points have less variance. So the variance across repeated measurements is not necessarily homogeneous. Sometimes it can be and sometimes is not going to be. So this is showing variance over time in the time to name. And what you see here is a similar kind of analysis, but this is in the time frequency domain. So here I am plotting the average power. This is from the V1 data set. You've seen this kind of plot several times before. And what I'm plotting here is the standard deviation of power over different trials at each time, frequency point. So you see that when there's large power, there's also a lot of variability over trials. And there's also other regions where there is a large amount of variability over trials without necessarily having too much power at the average level. Again, this violates the assumption of homogeneity of variance across repeated measurements. OK, and then finally, we can discuss this assumption that the measurement error and sampling variability are independent. So what this means is that if you measure two different individuals from the population, they should be independent of each other. Their measurement error, their sampling variability should be independent. But when we do large scale measurements of the brain, we. Find that there is a lot of dependency, there's a lot of autocorrelation, spatial autocorrelation and temporal autocorrelation and spectral autocorrelation in the data sets that we are acquiring. This is just an example showing structure, showing correlational structure in multichannel data sets. And that structure is consistent over space. So there's a correlation over space and there's also correlation over time in multiple dimensions. So the sampling variability across different measurements is definitely not independent or, you know, often is not independent, particularly when doing large scale recordings. OK, so overall, the conclusions of the last several slides are do spectral data meet assumptions that are required by parametric statistics? And the conclusion? The answer is sometimes yes and sometimes no. It depends a lot on the experiment, on how you're measuring the data, on the kind of data you're measuring, the kind of analyses that you're applying. So things get pretty hairy pretty quickly when it comes to parametric statistics and the assumptions underlying parametric statistics. Now, I would like to take a moment to discuss non parametric statistics and the assumptions underlying non parametric statistics, in particular permutation testing. Now, whenever I am teaching this live and I ask the students to tell me about the assumptions of non parametric statistics, I tend to get a bunch of confused and concerned looks and sometimes people think I'm making a joke. So it turns out non parametric statistics arguably don't really have very strong assumptions. That's kind of the the big advantage of non parametric statistics over parametric statistics. However, I do like to say that these are assumptions because these are really important considerations to keep in mind when doing non parametric statistics. So one is there exists a null hypothesis. Now, this sounds kind of trivial. It may sound even weirdly stupid just to say there exists a null hypothesis. However, you will see, particularly for complex data sets or more complex designs, it's not always so easy to define exactly what the null hypothesis is now. Sometimes, like in your undergraduate statistics course, the null hypothesis is really easy. You know, there's some there's a medication study. And so the null hypothesis is that the medication has no effect. But you will see later on in this section that it can actually sometimes be tricky in real science and real when you get down to doing real data analysis, not just in statistics, textbooks, sometimes it is a little bit difficult to think about what exactly the null hypothesis is and how to translate that null hypothesis into a concrete situation that we can construct to get a null hypothesis, an empirical null hypothesis distribution. OK, enough for that. So and then another quote unquote assumption is that you have data, you have sufficient data from which to create a situation that would arise under the null hypothesis. And this is kind of you know, this statement is just a way to make sure that you are thinking carefully about the amount of data that you need to make claims to make statistical claims based on your data sets. So, for example, if you only have, you know, let's say you only collect data from three trials, it's a bit of an extreme example. But if you have data from three trials, you're not going to be able to create a situation that could arise under the null hypothesis. You're not going to be able to do permutation testing with three trials. So it is important to think about these two considerations, even if they are not actual formal assumptions underlying non parametric and in particular permutation based testing. All right. So with that discussion out of the way, I'm sure you are just burning with curiosity of how to create a null hypothesis distribution like this based on permutation testing that is covered in the next video. So keep watching.