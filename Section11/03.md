 The main goal of this video is to explain to you the mechanism of permutation testing and the statistical inference in the permutation testing framework. I'm going to start by providing a couple of motivations for using permutation based statistics. So the two primary advantages of permutation based statistics over parametric statistics are that you don't need to worry about assumptions that are necessary for parametric statistics. And I discussed those assumptions in the previous video. Secondly, a major advantage of permutation testing in electrophysiology data is the ability to deal appropriately with multiple comparisons. In particular, when you're multiple comparisons have strong autocorrelation structure. Now, if you're not sure what multiple comparisons refers to or why permutation testing is so useful for multiple comparisons, and don't worry, I'm going to have a whole video. In fact, I'm going to have several videos that are addressing exactly this topic. So you can just pin this in the back of your mind and we will discuss it in depth later. So I discussed in the first video in this section that when you are evaluating your statistical parameter, that is the signal to noise ratio, you want to know whether it's statistically significant and you determine whether it is statistically significant based on where that value is, the magnitude of that value relative to a distribution of test statistic values expected under the null hypothesis. So how do you get that null hypothesis distribution? Well, if you're doing parametric statistics, you computed analytically based on a formula that is derived, making some assumptions about the data, the underlying distributions, the population and so on. But if you are doing permutation based tests, then that distribution is not based on a formula. It is not an analytic solution. Instead, we call this an empirical null hypothesis distribution. It's a distribution that you are obtaining yourself from the data that you have actually collected by simulating situations that could have arisen under the null hypothesis with your data. OK, so how do you create this distribution? That's the main goal of the next several minutes. Let's start by thinking about our imaginary data set. So this is our data set. It is seven measurements. We have seven measurements in this data set and two conditions. We have the blue condition and the orange condition. And I'm explicitly showing that there's different number of trials or data points in these two different conditions. And that's to show you that this method of permutation testing works even if you have an unbalanced design. Now, of course, for other reasons, it's preferable to have a balanced design. So when possible, you should strive to have your your experiments such that you have roughly the same number of conditions in roughly the same number of trials. And all of your conditions, roughly the same number of individuals and all of your groups and so on. But if you don't have an exact matching number, then it's OK. All right. So here we have seven measured data points and two conditions. Now, the first step in permutation testing is to pool all of these data together. And what that means practically here is that we are going to strip the color from each of these data points. So now these data points are all pulled together into one common distribution. There is no such thing as conditions anymore. I have stripped the condition labels away from the data points, OK, and then what we do, which you might you might already be able to guess based on the label permutation testing, is we randomly reassign these conditions to different data points. So it might look something like this. We still have four blue trials and we still have three orange trials so that the number of conditions and the number of elements in each condition doesn't change. What does change is that we are randomly remapping the condition label to the data point. So for one particular shuffling, one particular random iteration, it might look something like this. OK, and then what do you do now? I'm going to move this thing to the left so we have some space. So you take your shuffled data and then you compute your test statistic. And whatever the test statistic is, maybe it's a test or a correlation coefficient. Whatever it is that you're doing, you generate a statistical value. So this is kind of supposed to illustrate something like. A a test, we have the difference of the means or the sums and divided by the standard deviations. OK, so you generate a test statistic. Now, here's the thing. What do you expect the value of this to test test statistic to be right? You expect that this value will be zero because you've totally randomly assigned each data point to be in condition orange or condition blue. So this the you know, the average of these guys should be about the same as the average of these guys because this is just total random shuffling. So you would expect that this value is going to be zero. Now, in reality, that, you know, reality often doesn't match expectations, unfortunately. The reality is that this is probably not going to be exactly zero because we are randomly shuffling, so just by chance, we are likely to get larger values in the orange condition and smaller values in the blue condition. That can happen just randomly. So then this would actually be larger than zero. But that's OK. So we take this test statistic value and we say we build up a distribution like this. So here is our test statistic value. We might have expected it to be zero. Let's imagine zero is here in the middle and it happened to be a little bit larger. So here I have count. So far I've only done one shuffling and one test. So that gives me one test statistic value. And this is a test statistic value that I could expect under the null hypothesis. So if the null hypothesis is true, then I expect that these condition labels are meaningless. So they are just randomly assigned to the different data points. OK, so this is one test statistic value. What do you think we do next? What do you think is the next step in permutation testing? You guessed it correctly. I am sure I have a lot of confidence in you. We go back to the shuffled data and we reshuffle the labels again. We come up with a new random labeling or assignment of labels to data points, and that's going to give us new condition mapping. And then we generate another test statistic value and we get another value to build up in this distribution. Then, of course, you repeat this many, many times. It's typical to do like a thousand or two thousand random shuffling to build up a distribution that looks something like this. Now, the shape of this distribution is not necessarily going to be Gaussian. That's just what I'm drawing here. But this will build up a distribution over hundreds to a few thousand different random shuffling of the condition to Datapoint label. All right. So then you have this null hypothesis distribution. This is our empirical null hypothesis distribution. And then what you do is you go back to the original data. This is the original data with the true labels assigned to their original value. So this is without any shuffling. And then this is what's called the observed test statistic. This is the actual true test statistic that you really measured in the data without doing any shuffling. And then you see where this test statistic lies in your null hypothesis distribution. And then the idea is that if this is far enough away from the center of the distribution, then it is statistically significant. And then you can safely interpret this difference between the conditions. And alternatively, if your test statistic ends up being, you know, somewhere around here close to the center, then you would say, well, we are pretty likely to observe a test statistic value like this in the null hypothesis data where we did random shuffling. So therefore we cannot interpret that test statistic as being statistically significant. OK, so how do you know if you are far enough away from this null hypothesis distribution? To do that, you need to generate a P value. And there are two methods. There are two ways of computing a P value in null hypothesis, empirical null hypothesis testing. So one of those methods is to create a Z value. And the way that you create a Z value is actually just the standard Z value formula. You take your observed data here minus the mean of this distribution. This is for the expectation value. So this is the expected value of the null hypothesis. Typically that would be the mean. And then you divide by the standard deviation, which is essentially like the width of this distribution. So this set value gives you a normalized distance in standard deviation units of your observed statistic value away from the center of the district, the null hypothesis distribution. So then you have the Z value and then you can easily convert that into a P value using you know, decades ago they would have used a lookup table, but now we just use Matlab. And so, for example, a Z value of one point six four corresponds to point of five P value point of five, a Z value of two point three corresponds to a P value of one and so on. The larger the Z value, larger in magnitude. If it's positive it's here. If it's negative, it will be down on this tail. So the larger the magnitude of the Z value, the smaller the P value. Now one thing to keep in mind is that this formula is appropriate only for null hypothesis distributions that are approximately and distributed. It doesn't have to be a. Perfect exact mathematical Gaussian distribution, but this formula is valid only for distributions, null hypothesis distributions that are roughly Gaussian distributed. Why is that assumption important? That assumption is important because this said score only makes sense. This is only a sensible formula if the distribution not here, this null hypothesis distribution is itself roughly Gaussian. And that's because the mean of a distribution and the standard deviation of a distribution are sensible metrics only for roughly Gaussian distributions. Otherwise, you know, these are not necessarily the mean is not necessarily a useful measure of central tendency for all distributions. And I've already discussed this concept when I talked about one over Heff and Scalfari Dynamics and so on. OK, so that is the key assumption for this method of obtaining a P value. There's a second way to get a P value if your data do not conform to this rough Gaussian distribution or if you just prefer to use the other formula and that I call P underscore C and the C here is for count. And the idea here is that you are simply counting the number of null hypothesis tests, test statistics that were larger or or could be smaller if you're on the negative tail, but more extreme than your observed test statistic value. So you simply sum up all of the null, empirical null hypothesis test statistics that were greater than your observed value. So it's essentially the number of test statistics that lie in here above the observed statistic, and then you just divide by the total number of null hypothesis tests. So, for example, let's take an easy example. Let's say you did 100 permutations, so you did 100 shuffling and you made this distribution from one hundred different random shuffling of the data labels. And then you find that there were four four of those tests out of one hundred four of those tests were actually larger than the observed test statistic. So then you would say this numerator numerator would end up being for the denominator is 100. So your P value in this case is zero point zero four. So that's how this works. This method is useful because it is totally robust to the shape of the distribution. It doesn't matter if your null hypothesis distribution is Gaussian or, you know, it kind of power law or whatever f distributed or any kind of distribution. This is going to be valid. You'd have to be careful or mindful of which tail you are testing. But I guess you always have to be mindful of that. You will see, by the way, when I start talking about cluster based multiple comparisons, correction, you will see examples of distributions that are not normally distributed. There are distributions that are roughly power distributed. So you will see a use case for this later on in the course. So a few slides ago, I said that it's a good idea to do, let's say, a thousand or two thousand of these iterations to build up this empirical null hypothesis distribution. But now in this simple example that I just gave, I said there were 100, which is a lot smaller than one thousand one hundred iterations. Is one hundred iterations enough? How do you know how many iterations is enough to build up this null hypothesis distribution? Well, that's kind of difficult to determine, apriori. And because we are doing random shuffling, we are relying on just, you know, random number generations. And that means that this this result, this null hypothesis distribution is actually going to be different every time you rerun your permutation testing. And that's what I'm illustrating here in this slide. So here you see the Z value. So that is this value here for a real dataset. I took an example, real data set, did permutation testing and computed these add value. And then I repeated the whole procedure for a varying number of iterations. So starting with I guess this is one hundred iterations. And you can see that each time I run this test over and over again, the the end result, the statistical value is changing slightly. I mean, it's not changing wildly. It's not all over. The place doesn't get negative. For example, it's a pretty restricted range, but it is still different every time you rerun the statistic. And depending on the nature of the data and the strength of the effect, that can be a little bit awkward. If you have a really, really large effect, then it doesn't really matter. You're going to get a huge Z value no matter what. And if you really have no effect, if there's a null effect in your data and what you're testing, then this set value is basically always going to be very. Close to zero, regardless of the number of iterations or how many times you rerun permutation testing, however, you can have a uncomfortable situation where you're not really sure if your affect is statistically significant according to your P value threshold, because each time you rerun the permutation testing, you get a slightly different set of let's imagine. So these Z values would not be traditionally considered statistically significant. But let's imagine that this value here was one point sixty five that corresponds to around a P value of five. That means that your test is going to be it can be labeled non statistically significant or it can be labeled statistically significant. And that can change just by rerunning the permutation testing. So that is a pretty awkward feature of this kind of permutation testing. So what do you do? How can we deal with this situation? Well, if you are concerned that something like this is happening, you can perform what's called a meta permutation test. So the idea of a meta permutation test is that if you look so, you can already see that even though this function never converges to a single Z value, you can see that it is fluctuating around a Z value. So it seems like there is one true Z value and these different random permutations are kind of, you know, building up a distribution around that true Z value. And that's what I'm showing here. So this is literally just the histogram of all of these data points. And you can see that there is variance here, but it would quickly converge to the center value. So the central tendency of this distribution, this distribution for all these different permutations is, you know, whatever this number happens to be, I guess it's one point to eight maybe. So this leads to the idea of doing a meta permutation test. So the idea of a meta permutation test is that you repeat your permutation testing multiple times and then you average the Z value over all of those individual permutation tests and that will quickly converge that kind of level to analysis. So you're averaging over the permutations that will quickly converge to some central value here, the maximum likelihood value, and presumably that reflects the true underlying Z value. OK, very good. Very nice. I hope that this is clear. I hope that this all makes sense, this procedure. Now, the thing is that this procedure that I just explained, this is for one distribution. You know, this is for one set of numbers with two conditions. Now, when you're talking about time, frequency analysis, that entire procedure that I just spent 10 or 15 minutes explaining is giving you a a P value for one pixel. So we have this entire time frequency map and all of this stuff that I just explained is for one single pixel. So each pixel in this time frequency map gets its own null hypothesis distribution. Now, first of all, that is really cool. That's a really major, significant advantage of this kind of permutation testing, because you can already see in this map that the characteristics of the data are going to change over time and over frequency. So the characteristics of the data are different in this time, frequency range and they are different in this time frequency range. And that's different from the characteristics in this time, frequency range. Now, if we were doing standard parametric tests, we would have to assume that the data distributions, the characteristics of the data are identical here and here and here and here and here. Every pixel has an identical distribution, but now we are building up this null hypothesis distribution. Empirically, we do not need to worry about the fact that the distribution characteristics, the data characteristics will change at different times, frequency window. So that is already a pretty cool advantage of permutation testing. But it does mean that you have to do the permutation testing separately for each individual time frequency pixel. Now, fortunately, we're not doing this all by hand, so it's actually not so bad. Matlab does it all for us and we can do the whole thing map wise. We don't need to have a double for loop over time points and over frequency bins. All right. But so you get the idea. So we have a permutation based null hypothesis distribution for each individual time point. Now, one thing that is sometimes tricky to think about with permutation testing is what is exactly going to be permuted? What is the feature of the data that you will permit? Sometimes this is obvious, like the example that I showed in most of this video where you just. You have conditioned labels and you randomly swapped the conditioned labels, but there are other times and you will see some examples later in this section where it's not necessarily so trivial to know what feature of the data needs to be changed. So the first thing to think about is the one key factor that your null hypothesis predicts doesn't matter. So ideally, you want to change exactly one feature of the data and nothing else. So in our example that I was discussing earlier, we change the condition labels, why we didn't change anything else about the data. So try to understand the one key feature of the data that the null hypothesis predicts doesn't matter. And then that's what you change. Now, as I mentioned, sometimes is easy and sometimes this is a little bit more challenging and you don't want to change anything else. This is a, you know, permutation testing is also a bit like running a scientific experiment. If you change too many factors at the same time, you're not going to know what your results are attributable to. So you want to change only one factor at a time. OK, and so I want to give one example of this. Let's say you have a time series. Maybe this is a power time series. It looks like this. So power is going down and then we have a burst of power and so on. Now, let's say you want to permute this time series because you have a prediction. You know, your null hypothesis or your experiment is that, you know, let's say something happens at this time. This is time zero. And there's a stimulus that happens exactly right here. And this is the brain response. So you want to know, is this peak here, these activities here, are they statistically significant or is this something we could have expected by chance? Because you can see that, you know, back here before time zero, we didn't expect anything to happen and we still got a little burst of power. So maybe this is just random that we happen to get a burst of power right here. So what can you do? Well, one thing you can do is just totally shuffle the time points so you could just completely randomly reassign all of these time points. But the thing is, if you do that, you're going to get basically something that looks like noise. And this is not a fake example. I literally took this time series, which has, you know, a lot of there's a lot of smoothness in this Time series. I took all these data points and I just totally randomly shuffle them in time and that produced this plot. Now, the thing is, this is not a good permutation. This is not a good way to shuffle because there is a temporal structure, there is low frequency structure in this Time series, and we are destroying that here. So we are actually manipulating too much. We are changing too much of the characteristics of the data. Now let's think about our null hypothesis. The null hypothesis does not say that we should be rent. You know, that time points are totally randomly distributed with no Nabor autocorrelation structure. So this is actually not an appropriate way to do the shuffling. If you do it like this, you're going to get an overly liberal test statistic value. So this is not an appropriate way to shuffle the data. But the null hypothesis does predict that that this burst could happen anywhere. It could happen after time zero after Stromness onset, or it could happen before. So it really could happen anywhere. Now, what we want to do is instead of shuffling the data time points randomly, what we can do is randomly reassign the time zero point. So we could say, let's imagine that time zero isn't here. Let's imagine that time zero is over here. So, you know, a couple of hundred milliseconds later, just by chance, we're just going to put time zero randomly somewhere else. And then what do we do? We treat this as a cut point and we take this part of the TIME series and put it before this part of the Time series. So that would look like this. So now you see this is the part of the Time series after this orange line and this is the part of the Times series before the Orange Line, and I've just concatenated them together. Now, notice that the important temporal structure of the data that the spectral structure of the data is almost entirely unchanged. There is one awkward little point here where we get an edge. So there's one, you know, little weird thing happening here. But the rest of the time series has the same temporal structural characteristics that it did before we did the permutation testing. So this would be an appropriate way to shuffle a time series. OK, and so this is just you know, I just meant this as one illustration of how you have to think carefully about what is the feature of the data that you are going to permit. And the idea, again, is that you want to manipulate exactly one feature of the data without changing anything else in the data, unless it's absolutely necessary, like this tiny little thing here, which is fine.