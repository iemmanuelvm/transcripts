 I have come to the conclusion over the years that there are two types of researchers, there is the type of researcher that really, truly likes statistics, and there is the second group of researchers that claims to like statistics. But deep down in their heart, they really don't like it, maybe because they find statistics annoying, maybe because they find it arbitrary, maybe because they find it confusing, or maybe because they find it a necessary evil that just kind of slows down the progress in science. So I will let you guess which category I fall into. Nonetheless, here we are having a whole section about statistics. Now, statistics is a huge topic in biology, physics, engineering, signal processing and so on. Statistics pervades basically every aspect of quantitative science. So it's way too big of a topic to cover in depth for all of statistics. What I am going to do in this section of the course is mainly focus on non parametric and permutation based statistics, in particular permutation based statistics. I'm not really going to be discussing in a lot of detail things like ANOVA tests, F tests, multiple regressions, these kinds of things. If you have already some statistical background, then that's great. Then you will find it fairly straightforward and easy to get through this section. If you have no background or minimal background, formal background in statistics, then don't worry, you don't need a formal background in statistics, but just be aware that there are many kinds of statistical procedures that are important, that are widely used like a novas and regressions that I am not going to be discussing in any detail in this course. All right. So I want to say some general things about statistics in this video, and that's going to kind of paint the background for the rest of this section. OK, first of all, I would like to discuss briefly whether we even need statistics and why we might need statistics. Now, when I teach this course live, this usually ends up in a longish discussion of about ten minutes or so. But because there's quite limited interaction here in this medium, I'm just going to present a few points and make a few discussion points, and I will leave it to you to continue critically thinking about this issue. First of all, it's important to make a very clear distinction between inferential statistics and descriptive statistics. It is really unfortunate that both of these totally different categories of numerical approaches have the same name. They both fall under the name statistics. So when we talk about descriptive statistics, that is characteristics of sets of numbers, things like the mean, the variance, the spectrum of time, frequency, characteristics. These are all descriptive statistics. And this section we are talking about inferential statistics and that refers to P values, values, T values, chi square values, correlation coefficients and so on. Basically, the kinds of tests, statistical tests that you use to make a determination of whether a particular finding is statistically significant. So these kinds of inferential statistics, so do we need statistics? Do we need inferential statistics? In some cases, yes, we do, because there is sampling variability. There is noise, there's issues of generalizability. We want to know which results are safe to interpret and which results we should not interpret. And that is essentially what we do with P values. Ultimately, with inferential statistics, we get a P value. If the P value is smaller than some threshold, which is typically point zero five, then we are safe. We are allowed to interpret that result. And if the P value is larger than the threshold, then we do not interpret those results or we interpret them very cautiously. OK, so that is one case where we need statistics. There are in fact other situations where inferential statistics are not really necessary or they are not appropriate. But you will often find that the field demands it. So other people in the field will demand that you compute and report inferential statistics, even if they're not necessarily important in every single case. OK, but so for both of these reasons, it's important to understand statistics. So I am going to make the claim that all inferential statistics, every inferential statistic that you ever perform or ever encounter all boils down to the same fundamental quantity, which is. A ratio, so if we think of representing our data using histograms or probability densities, something that looks like this, then we have two groups here and you can have more groups, of course. But in this case, I'm illustrating just two groups. And then essentially all inferential statistics boil down to a difference of central tendencies of these distributions. Maybe that's the mean of the distributions. Maybe it's the median of the distributions, but somehow a difference of these centers or central tendencies, and that is divided by or normalized by something about the width of those distributions. It might be the pooled with or the standard deviations of these distributions, something like this. Now, this we can further reduce to understand it a little bit better. And essentially this ratio is a signal to noise ratio or S and R for signal to noise ratio. So I think that's pretty interesting to think about. Every statistic that you come across can basically be interpreted as a ratio of signal to noise. So that is pretty interesting because if you want statistically significant effects, you need to maximize this ratio. So how do you maximize this ratio? Well, you can do that either by getting the signal to be as big as possible or by getting the noise to be as small as possible or both, of course. But that is interesting to think about, because in some experiments with some kinds of methodologies, you have more or less control over the signal versus the noise. For example, let's imagine you are doing research in a rare patient population like I don't know, let's say you're interested in studying Parkinson's patients who also have a diagnosis of schizophrenia. Now, I don't know how many people in the world have both Parkinson's disease and schizophrenia, but I guess it's a fairly small number of patients and therefore you're not going to be able to acquire a huge amount of data. Furthermore, there might be a lot of variability across these different patients because the disease will have different severities and so on. So in other words, the noise is going to be large and you're not really going to be able to have much control over that. So therefore, you need to design your experiment knowing that the noise term is going to be large. So therefore you have to limit yourself to experiments where the signal is really, really big. In contrast, imagine you are doing research on, let's say, slices of brain tissue. So in in vitro experiments and let's say the way that it's set up, you can collect a huge amount of data, you can collect a ton of data. Basically, you can get as much data as you want. And because it's an in vitro experiment, you can also have a lot of control over the various environmental variables. So that means you can control that the noise is going to be really, really, really small. So therefore you can afford to design and experiment with a really small effect size because the noise is going to be even smaller. OK, so that's just something important to think about when designing your research and thinking about your analyses. OK, so this would be like the T-test or the F test or the correlation coefficient and so on. So how do we know whether this ratio is safe to interpret? Well, it's safe to interpret when this ratio is big enough. And what does big enough mean? Well, to know whether that signal to noise ratio value is big enough to interpret, we have to figure out some way of defining a distribution of signal-to-noise values or test statistic values that we expect under the null hypothesis. The null hypothesis is generally the hypothesis that there is no effect and it's often indicated as H with a zero in the subscript, so h not the null hypothesis. So there are different ways of estimating what the the distribution of signal-to-noise ratios would be under the null hypothesis. And this could be done using parametric methods or non parametric methods, permutation based methods, for example. And I'm going to talk about that in a later video. So the idea is that you have your observed s and our value that I discussed in the previous slide and you compare it to this distribution of null hypothesis s and our values. And then, of course, you come up with some threshold and maybe that threshold is like five percent overall. So it would be 2.5 percent of the distribution on this side, 2.5 percent of the distribution on this side. And then if this is greater than that threshold, then you would call this the. Medically significant. OK, so I hope this sounds familiar to you, and essentially what I'm trying to do is kind of paint all of inferential statistics under one very broad landscape. And if it's not totally clear if this is new to you and you still have some questions about how exactly do you find the threshold and what does this distribution mean, then? Don't worry. I'm going to talk about that in later videos. There is one more concept that I would like to introduce in this video, and that is what's referred as levels in statistical analyses. A level in a statistical analysis refers to the grouping at which you are collecting data. So, for example, we often have a level one that's like the lowest level. That's the level associated with the most fundamental aspect of the data acquisition, and that might be trials. So trials would be the smallest units of meaningful data that you are collecting. So by trials I'm referring to, you know, it could be stimulus repetitions or items and a memory task. So this would be level one, a lot of within subject's analyses. So statistical analysis done in each individual data set is done is called level one analyses. And then we get to level two. And that would be your subjects or research participants or individual animals. So this would be, for example, you know, you might have, let's say, 500 trials in the experiment and then you have 20 subjects who are each doing 500 trials. So this is called level two. And then we can have level three, maybe if you have multiple groups. So, for example, let's imagine you have 40 subjects in total and 20 of them are in Group A and 20 of them are in Group B and 20 in Group A got some medication treatment and the 20 in Group B got a placebo control treatment. So that would be two groups. In this case, that would be a level three analysis. And then, you know, you can keep going higher. Level four might be culture. So you want to see whether the effect of this medication on how the individuals are doing. This task depends on whether they are from an Asian culture or a Western culture. OK, so I'm making some of this stuff up now. It's not necessarily always the case that level one is trials, level two is individuals and so on. Furthermore, obviously not every experiment has four levels in the hierarchy of statistical analyses. I'm showing you this as a typical example. Most people in most kind of standard experiments would have two levels. So you have trials within an individual and then you have multiple individuals, OK? And then most of the inferential statistics that we are concerning ourselves with is at the level two. So this would be the subject level. So we want to test whether the findings that we observe are consistent across a group of individuals. And the idea is that if the findings are consistent across a group of 20 subjects, then there probably or they might be also observed in a larger sample of, you know, millions of people. Again, I don't want you to take these mappings of level to terms here too seriously. This is just a typical examples of what you might come across in many experiments. But obviously they're going to be exceptions where the levels have different meaning. And maybe at some point in the future we will meet some friendly, intelligent space aliens and we will have level five. We will be comparing across different cultures and different planets. Anyway, this diagram is made as a trapezoid on purpose, and this shows that the recommended NP is typically larger for the lower levels. So that's why in typical research experiments, you might have, let's say, a thousand trials per subject each. Each subject does a thousand trials, but then you only have 20 or 30 subjects and maybe three groups and two cultures. Again, this goes for the typical experiments. There's always going to be exceptions where you might have relatively few trials and a lot of subjects. It really all depends on how much variability you expect in these different levels. We typically expect certainly for if there's some kind of cognitive or behavioral component, it's reasonable to expect that there's going to be a lot of variability at the level of individual trials. So therefore, we want to have a lot of data here because the variability will be high and then we expect typically less. Variability at the level of individuals and even less variability at the level of groups and cultures. OK, so with that as a guide, I'm now going to move on and start discussing parametric versus non parametric statistics, which is essentially related to how you estimate a distribution of S and our values expected under the null hypothesis. See you in the next video.